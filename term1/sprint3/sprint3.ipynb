{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】仮定関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の数式で表される線形回帰の仮定関数を実装してください。メソッドの雛形を用意してあります。  \n",
    "\n",
    "$$\n",
    "h_\\theta(x) =  \\theta_0 x_0 + \\theta_1 x_1 + ... + \\theta_j x_j + ... +\\theta_n x_n.   (x_0 = 1)\\\\\n",
    "$$  \n",
    "\n",
    "$x$: 特徴量ベクトル  \n",
    "$\\theta$: パラメータベクトル  \n",
    "$n$: 特徴量の数  \n",
    "$x_j$: j番目の特徴量  \n",
    "$\\theta_j$:j番目のパラメータ（重み）  \n",
    "特徴量の数$n$は任意の値に対応できる実装にしてください。  \n",
    "なお、ベクトル形式で表すと以下のようになります。  \n",
    "\n",
    "$$\n",
    "h_\\theta(x) = \\theta^T \\cdot x.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のメソッドをScratchLinearRegressionクラス内に実装\n",
    "```python\n",
    "def _linear_hypothesis(self, X):\n",
    "    \"\"\"\n",
    "    線形の仮定関数を計算する\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "      次の形のndarray, shape (n_samples, 1)\n",
    "      線形の仮定関数による推定結果\n",
    "\n",
    "    \"\"\"\n",
    "    return np.dot(X, self.coef_)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】最急降下法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最急降下法により学習させる実装を行なってください。以下の式で表されるパラメータの更新式のメソッド_gradient_descentを追加し、  \n",
    "fitメソッドから呼び出すようにしてください。\n",
    "\n",
    "$$\n",
    "\\theta_j := \\theta_j - \\alpha \\frac{1}{m} \\sum_{i=1}^{m}[(h_\\theta(x^{(i)}) - y^{(i)} )x_{j}^{(i)}]\n",
    "$$\n",
    "\n",
    "$\\alpha$: 学習率  \n",
    "$i$ : サンプルのインデックス  \n",
    "$j$: 特徴量のインデックス"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のメソッドをScratchLinearRegressionクラス内に実装\n",
    "```python\n",
    "def _gradient_descent(self, X, errors):\n",
    "    \"\"\"\n",
    "    最急降下法により重みとバイアスの更新を行う\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "      訓練データ\n",
    "    errors : 次の形のndarray, shape (n_samples,)\n",
    "      目的変数から予測値を引いたもの\n",
    "    \"\"\"    \n",
    "    self.coef_ -= self.lr * np.dot(errors.T, X) / X.shape[0]        \n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "推定する仕組みを実装してください。  \n",
    "ScratchLinearRegressionクラスの雛形に含まれるpredictメソッドに書き加えてください。  \n",
    "仮定関数 $h_\\theta(x)$ の出力が推定結果です。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のメソッドをScratchLinearRegressionクラス内に実装\n",
    "```python\n",
    "def predict(self, X):\n",
    "    \"\"\"\n",
    "    線形回帰を使い推定する。\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 次の形のndarray, shape (n_samples, n_features)\n",
    "        サンプル\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "        次の形のndarray, shape (n_samples, 1)\n",
    "        線形回帰による推定結果\n",
    "    \"\"\"\n",
    "    return self._linear_hypothesis(X)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】平均二乗誤差"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "線形回帰の指標値として用いられる平均二乗誤差（mean square error, MSE）の関数を作成してください。  \n",
    "平均二乗誤差関数は回帰問題全般で使える関数のため、ScratchLinearRegressionクラスのメソッドではなく、別の関数として作成してください。  \n",
    "雛形を用意してあります。  \n",
    "平均二乗誤差は以下の数式で表されます。\n",
    "$$\n",
    "L(\\theta)=  \\frac{1 }{ m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "$m$ : 入力されるデータの数  \n",
    "$h_\\theta()$ : 仮定関数  \n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル  \n",
    "$y^{(i)}$ : i番目のサンプルの正解値  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:08:54.002060Z",
     "start_time": "2020-07-06T22:08:53.996050Z"
    }
   },
   "outputs": [],
   "source": [
    "def MSE(y_pred, y):\n",
    "    \"\"\"\n",
    "    平均二乗誤差の計算\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    y_pred : 次の形のndarray, shape (n_samples,)\n",
    "      推定した値\n",
    "    y : 次の形のndarray, shape (n_samples,)\n",
    "      正解値\n",
    "\n",
    "    Returns\n",
    "    ----------\n",
    "    mse : numpy.float\n",
    "      平均二乗誤差\n",
    "    \"\"\"\n",
    "    mse = ((y_pred - y)**2).sum() / y.shape[0]\n",
    "    return mse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】目的関数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下の数式で表される線形回帰の 目的関数（損失関数） を実装してください。  \n",
    "そして、これをself.loss, self.val_lossに記録するようにしてください。  \n",
    "目的関数（損失関数） $J(\\theta)$ は次の式です。\n",
    "$$\n",
    "J(\\theta)=  \\frac{1 }{ 2m}  \\sum_{i=1}^{m} (h_\\theta(x^{(i)})-y^{(i)})^2.\n",
    "$$\n",
    "$m$ : 入力されるデータの数  \n",
    "$h_\\theta()$ : 仮定関数  \n",
    "$x^{(i)}$ : i番目のサンプルの特徴量ベクトル  \n",
    "$y^{(i)}$ : i番目のサンプルの正解値"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下のコードをScratchLinearRegressionクラスのfitメソッド内に実装\n",
    "```python\n",
    "self.loss[i] = MSE(y_train_pred, y_train) / 2\n",
    "self.val_loss[i] = MSE(y_test_pred, y_test) / 2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】学習と推定"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "機械学習スクラッチ入門のSprintで用意したHouse Pricesコンペティションのデータに対してスクラッチ実装の学習と推定を行なってください。  \n",
    "scikit-learnによる実装と比べ、正しく動いているかを確認してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:08:56.911953Z",
     "start_time": "2020-07-06T22:08:56.899541Z"
    }
   },
   "outputs": [],
   "source": [
    "class ScratchLinearRegression():\n",
    "    \"\"\"\n",
    "    線形回帰のスクラッチ実装\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    num_iter : int\n",
    "      イテレーション数\n",
    "    lr : float\n",
    "      学習率\n",
    "    no_bias : bool\n",
    "      バイアス項を入れない場合はTrue\n",
    "    verbose : bool\n",
    "      学習過程を出力する場合はTrue\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    self.coef_ : 次の形のndarray, shape (n_features,)\n",
    "      パラメータ\n",
    "    self.loss : 次の形のndarray, shape (self.iter,)\n",
    "      訓練データに対する損失の記録\n",
    "    self.val_loss : 次の形のndarray, shape (self.iter,)\n",
    "      検証データに対する損失の記録\n",
    "\n",
    "    \"\"\"\n",
    "    def __init__(self, num_iter, lr, random_state=1, no_bias=False, verbose=True):\n",
    "        # ハイパーパラメータを属性として記録\n",
    "        self.iter = num_iter\n",
    "        self.lr = lr\n",
    "        self.random_state = random_state\n",
    "        self.no_bias = no_bias\n",
    "        self.verbose = verbose\n",
    "        # 損失を記録する配列を用意\n",
    "        self.loss = np.zeros(self.iter)\n",
    "        self.val_loss = np.zeros(self.iter)\n",
    "        \n",
    "    def _linear_hypothesis(self, X):\n",
    "        \"\"\"\n",
    "        線形の仮定関数を計算する\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "          次の形のndarray, shape (n_samples, 1)\n",
    "          線形の仮定関数による推定結果\n",
    "\n",
    "        \"\"\"\n",
    "        return np.dot(X, self.coef_)\n",
    "        \n",
    "    def _gradient_descent(self, X, errors):\n",
    "        \"\"\"\n",
    "        最急降下法により重みとバイアスの更新を行う\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "          訓練データ\n",
    "        errors : 次の形のndarray, shape (n_samples,)\n",
    "          目的変数から予測値を引いたもの\n",
    "        \"\"\"    \n",
    "        self.coef_ -= self.lr * np.dot(errors.T, X) / X.shape[0]        \n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"\n",
    "        線形回帰を学習する。検証データが入力された場合はそれに対する損失と精度もイテレーションごとに計算する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            訓練データの特徴量\n",
    "        y : 次の形のndarray, shape (n_samples, )\n",
    "            訓練データの正解値\n",
    "        X_val : 次の形のndarray, shape (n_samples, n_features)\n",
    "            検証データの特徴量\n",
    "        y_val : 次の形のndarray, shape (n_samples, )\n",
    "            検証データの正解値\n",
    "        \"\"\"\n",
    "                \n",
    "        if self.no_bias == False:\n",
    "            X = np.insert(X,0,1,axis=1)\n",
    "            if X_val is not None:\n",
    "                X_val = np.insert(X_val,0,1,axis=1)\n",
    "                \n",
    "        # 重みの生成\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.coef_ = rgen.normal(loc=0.0, scale=0.01, size=(X.shape[1]))\n",
    "        \n",
    "        for i in range(self.iter):\n",
    "            # 仮定関数を求める\n",
    "            output = self._linear_hypothesis(X)\n",
    "            # 誤差を求める\n",
    "            errors = output - y\n",
    "            # 最急降下法による重み、バイアスの更新\n",
    "            self._gradient_descent(X, errors)\n",
    "            # 目的関数を求めて、self.lossに格納\n",
    "            self.loss[i] = MSE(output, y) / 2\n",
    "            \n",
    "            # 検証データについて、目的関数を求める\n",
    "            if (X_val is not None) and (y_val is not None):\n",
    "                output_val = self._linear_hypothesis(X_val)\n",
    "                self.val_loss[i] = MSE(output_val, y_val) / 2\n",
    "                    \n",
    "            if self.verbose==True:\n",
    "                print(\"num_iter : {}, loss = {}\".format(i, self.loss[i]))\n",
    "                pass\n",
    "                \n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        線形回帰を使い推定する。\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        X : 次の形のndarray, shape (n_samples, n_features)\n",
    "            サンプル\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "            次の形のndarray, shape (n_samples, 1)\n",
    "            線形回帰による推定結果\n",
    "        \"\"\"\n",
    "        if self.no_bias == False:\n",
    "            X = np.insert(X,0,1,axis=1)\n",
    "        return self._linear_hypothesis(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:08:59.664011Z",
     "start_time": "2020-07-06T22:08:59.632806Z"
    }
   },
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "df = pd.read_csv('train.csv')\n",
    "X1 = df.drop('SalePrice', axis=1)\n",
    "X1 = X1[['GrLivArea', 'YearBuilt']].values\n",
    "y1 = df['SalePrice'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:09:00.143974Z",
     "start_time": "2020-07-06T22:09:00.140222Z"
    }
   },
   "outputs": [],
   "source": [
    "# 標準化\n",
    "X1 = (X1 - X1.mean())/ X1.std()\n",
    "# 対数変換\n",
    "y1 = np.log(y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:09:00.631071Z",
     "start_time": "2020-07-06T22:09:00.626021Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X1, y1, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:11:32.892102Z",
     "start_time": "2020-07-06T22:11:31.995812Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_iter : 0, loss = 72.30585734169878\n",
      "num_iter : 1, loss = 70.16113474910307\n",
      "num_iter : 2, loss = 68.09050223442897\n",
      "num_iter : 3, loss = 66.0911323258728\n",
      "num_iter : 4, loss = 64.16031136575234\n",
      "num_iter : 5, loss = 62.29543481116348\n",
      "num_iter : 6, loss = 60.49400273090468\n",
      "num_iter : 7, loss = 58.753615490429894\n",
      "num_iter : 8, loss = 57.07196961693875\n",
      "num_iter : 9, loss = 55.446853837044415\n",
      "num_iter : 10, loss = 53.87614527977682\n",
      "num_iter : 11, loss = 52.3578058379846\n",
      "num_iter : 12, loss = 50.889878681490025\n",
      "num_iter : 13, loss = 49.4704849156311\n",
      "num_iter : 14, loss = 48.09782037909263\n",
      "num_iter : 15, loss = 46.770152575184525\n",
      "num_iter : 16, loss = 45.4858177309711\n",
      "num_iter : 17, loss = 44.243217978890776\n",
      "num_iter : 18, loss = 43.040818655730604\n",
      "num_iter : 19, loss = 41.87714571403625\n",
      "num_iter : 20, loss = 40.75078324124473\n",
      "num_iter : 21, loss = 39.66037108202539\n",
      "num_iter : 22, loss = 38.60460255950436\n",
      "num_iter : 23, loss = 37.58222229122945\n",
      "num_iter : 24, loss = 36.59202409590691\n",
      "num_iter : 25, loss = 35.6328489871076\n",
      "num_iter : 26, loss = 34.703583250300866\n",
      "num_iter : 27, loss = 33.803156599726485\n",
      "num_iter : 28, loss = 32.93054041176248\n",
      "num_iter : 29, loss = 32.08474603158647\n",
      "num_iter : 30, loss = 31.26482315006319\n",
      "num_iter : 31, loss = 30.469858247919383\n",
      "num_iter : 32, loss = 29.698973104390884\n",
      "num_iter : 33, loss = 28.95132336764491\n",
      "num_iter : 34, loss = 28.22609718439395\n",
      "num_iter : 35, loss = 27.522513886225926\n",
      "num_iter : 36, loss = 26.839822730279685\n",
      "num_iter : 37, loss = 26.177301691993858\n",
      "num_iter : 38, loss = 25.534256307753076\n",
      "num_iter : 39, loss = 24.91001856534655\n",
      "num_iter : 40, loss = 24.303945840241752\n",
      "num_iter : 41, loss = 23.715419875759544\n",
      "num_iter : 42, loss = 23.14384580531776\n",
      "num_iter : 43, loss = 22.588651214986857\n",
      "num_iter : 44, loss = 22.049285244675154\n",
      "num_iter : 45, loss = 21.52521772633173\n",
      "num_iter : 46, loss = 21.015938357622627\n",
      "num_iter : 47, loss = 20.520955909600982\n",
      "num_iter : 48, loss = 20.039797466953445\n",
      "num_iter : 49, loss = 19.57200769946508\n",
      "num_iter : 50, loss = 19.11714816340158\n",
      "num_iter : 51, loss = 18.674796631562458\n",
      "num_iter : 52, loss = 18.2445464508108\n",
      "num_iter : 53, loss = 17.8260059259357\n",
      "num_iter : 54, loss = 17.418797728750942\n",
      "num_iter : 55, loss = 17.022558331379873\n",
      "num_iter : 56, loss = 16.63693746272019\n",
      "num_iter : 57, loss = 16.261597587124577\n",
      "num_iter : 58, loss = 15.8962134043735\n",
      "num_iter : 59, loss = 15.540471370055295\n",
      "num_iter : 60, loss = 15.19406923550556\n",
      "num_iter : 61, loss = 14.856715606493472\n",
      "num_iter : 62, loss = 14.52812951987669\n",
      "num_iter : 63, loss = 14.208040037479023\n",
      "num_iter : 64, loss = 13.896185856476356\n",
      "num_iter : 65, loss = 13.592314935606092\n",
      "num_iter : 66, loss = 13.296184136544177\n",
      "num_iter : 67, loss = 13.007558879821115\n",
      "num_iter : 68, loss = 12.72621281467471\n",
      "num_iter : 69, loss = 12.45192750226245\n",
      "num_iter : 70, loss = 12.18449211168062\n",
      "num_iter : 71, loss = 11.923703128260293\n",
      "num_iter : 72, loss = 11.669364073632494\n",
      "num_iter : 73, loss = 11.421285237076152\n",
      "num_iter : 74, loss = 11.179283417682614\n",
      "num_iter : 75, loss = 10.943181676890138\n",
      "num_iter : 76, loss = 10.712809100960289\n",
      "num_iter : 77, loss = 10.488000572986179\n",
      "num_iter : 78, loss = 10.268596554039439\n",
      "num_iter : 79, loss = 10.054442873079449\n",
      "num_iter : 80, loss = 9.845390525263793\n",
      "num_iter : 81, loss = 9.641295478314209\n",
      "num_iter : 82, loss = 9.442018486606573\n",
      "num_iter : 83, loss = 9.24742491266733\n",
      "num_iter : 84, loss = 9.05738455577202\n",
      "num_iter : 85, loss = 8.871771487354247\n",
      "num_iter : 86, loss = 8.690463892945553\n",
      "num_iter : 87, loss = 8.513343920378347\n",
      "num_iter : 88, loss = 8.340297533995127\n",
      "num_iter : 89, loss = 8.17121437461802\n",
      "num_iter : 90, loss = 8.005987625042822\n",
      "num_iter : 91, loss = 7.8445138808315376\n",
      "num_iter : 92, loss = 7.686693026186862\n",
      "num_iter : 93, loss = 7.5324281147010215\n",
      "num_iter : 94, loss = 7.381625254780025\n",
      "num_iter : 95, loss = 7.234193499552614\n",
      "num_iter : 96, loss = 7.090044741081207\n",
      "num_iter : 97, loss = 6.9490936086995925\n",
      "num_iter : 98, loss = 6.811257371309532\n",
      "num_iter : 99, loss = 6.676455843475279\n",
      "num_iter : 100, loss = 6.544611295161803\n",
      "num_iter : 101, loss = 6.415648364968804\n",
      "num_iter : 102, loss = 6.289493976718825\n",
      "num_iter : 103, loss = 6.166077259263559\n",
      "num_iter : 104, loss = 6.045329469378114\n",
      "num_iter : 105, loss = 5.927183917618422\n",
      "num_iter : 106, loss = 5.811575897022054\n",
      "num_iter : 107, loss = 5.698442614537749\n",
      "num_iter : 108, loss = 5.587723125073634\n",
      "num_iter : 109, loss = 5.4793582680587125\n",
      "num_iter : 110, loss = 5.373290606416514\n",
      "num_iter : 111, loss = 5.269464367853977\n",
      "num_iter : 112, loss = 5.167825388372658\n",
      "num_iter : 113, loss = 5.068321057913178\n",
      "num_iter : 114, loss = 4.970900268047467\n",
      "num_iter : 115, loss = 4.875513361636922\n",
      "num_iter : 116, loss = 4.782112084377945\n",
      "num_iter : 117, loss = 4.690649538159575\n",
      "num_iter : 118, loss = 4.601080136160984\n",
      "num_iter : 119, loss = 4.51335955961963\n",
      "num_iter : 120, loss = 4.427444716203659\n",
      "num_iter : 121, loss = 4.343293699924905\n",
      "num_iter : 122, loss = 4.260865752531424\n",
      "num_iter : 123, loss = 4.180121226320999\n",
      "num_iter : 124, loss = 4.101021548319485\n",
      "num_iter : 125, loss = 4.023529185770134\n",
      "num_iter : 126, loss = 3.947607612882232\n",
      "num_iter : 127, loss = 3.8732212787895306\n",
      "num_iter : 128, loss = 3.800335576670938\n",
      "num_iter : 129, loss = 3.7289168139879187\n",
      "num_iter : 130, loss = 3.658932183794841\n",
      "num_iter : 131, loss = 3.590349737080389\n",
      "num_iter : 132, loss = 3.5231383560997696\n",
      "num_iter : 133, loss = 3.457267728659143\n",
      "num_iter : 134, loss = 3.3927083233152486\n",
      "num_iter : 135, loss = 3.329431365454715\n",
      "num_iter : 136, loss = 3.2674088142189643\n",
      "num_iter : 137, loss = 3.2066133402420256\n",
      "num_iter : 138, loss = 3.1470183041698934\n",
      "num_iter : 139, loss = 3.088597735931303\n",
      "num_iter : 140, loss = 3.031326314731079\n",
      "num_iter : 141, loss = 2.975179349738303\n",
      "num_iter : 142, loss = 2.920132761442729\n",
      "num_iter : 143, loss = 2.866163063653901\n",
      "num_iter : 144, loss = 2.8132473461184913\n",
      "num_iter : 145, loss = 2.761363257732332\n",
      "num_iter : 146, loss = 2.710488990324587\n",
      "num_iter : 147, loss = 2.6606032629923835\n",
      "num_iter : 148, loss = 2.611685306965135\n",
      "num_iter : 149, loss = 2.563714850978572\n",
      "num_iter : 150, loss = 2.5166721071393288\n",
      "num_iter : 151, loss = 2.47053775726169\n",
      "num_iter : 152, loss = 2.425292939658835\n",
      "num_iter : 153, loss = 2.380919236371616\n",
      "num_iter : 154, loss = 2.3373986608185913\n",
      "num_iter : 155, loss = 2.2947136458516693\n",
      "num_iter : 156, loss = 2.2528470322023693\n",
      "num_iter : 157, loss = 2.21178205730424\n",
      "num_iter : 158, loss = 2.1715023444776262\n",
      "num_iter : 159, loss = 2.131991892463456\n",
      "num_iter : 160, loss = 2.093235065293279\n",
      "num_iter : 161, loss = 2.0552165824832764\n",
      "num_iter : 162, loss = 2.017921509540468\n",
      "num_iter : 163, loss = 1.9813352487697553\n",
      "num_iter : 164, loss = 1.9454435303709658\n",
      "num_iter : 165, loss = 1.9102324038153808\n",
      "num_iter : 166, loss = 1.8756882294917627\n",
      "num_iter : 167, loss = 1.8417976706121666\n",
      "num_iter : 168, loss = 1.8085476853682994\n",
      "num_iter : 169, loss = 1.7759255193294772\n",
      "num_iter : 170, loss = 1.7439186980736323\n",
      "num_iter : 171, loss = 1.7125150200431034\n",
      "num_iter : 172, loss = 1.681702549617318\n",
      "num_iter : 173, loss = 1.6514696103947073\n",
      "num_iter : 174, loss = 1.6218047786765815\n",
      "num_iter : 175, loss = 1.5926968771458772\n",
      "num_iter : 176, loss = 1.5641349687340451\n",
      "num_iter : 177, loss = 1.5361083506695252\n",
      "num_iter : 178, loss = 1.5086065487015874\n",
      "num_iter : 179, loss = 1.4816193114934828\n",
      "num_iter : 180, loss = 1.4551366051791237\n",
      "num_iter : 181, loss = 1.429148608077719\n",
      "num_iter : 182, loss = 1.4036457055609854\n",
      "num_iter : 183, loss = 1.3786184850677907\n",
      "num_iter : 184, loss = 1.3540577312612585\n",
      "num_iter : 185, loss = 1.3299544213235421\n",
      "num_iter : 186, loss = 1.3062997203836806\n",
      "num_iter : 187, loss = 1.2830849770741029\n",
      "num_iter : 188, loss = 1.2603017192115107\n",
      "num_iter : 189, loss = 1.2379416495980482\n",
      "num_iter : 190, loss = 1.2159966419388024\n",
      "num_iter : 191, loss = 1.1944587368718134\n",
      "num_iter : 192, loss = 1.1733201381069496\n",
      "num_iter : 193, loss = 1.1525732086701046\n",
      "num_iter : 194, loss = 1.1322104672493247\n",
      "num_iter : 195, loss = 1.1122245846395808\n",
      "num_iter : 196, loss = 1.092608380283031\n",
      "num_iter : 197, loss = 1.0733548189017394\n",
      "num_iter : 198, loss = 1.0544570072199138\n",
      "num_iter : 199, loss = 1.0359081907728254\n",
      "num_iter : 200, loss = 1.0177017507997228\n",
      "num_iter : 201, loss = 0.9998312012180696\n",
      "num_iter : 202, loss = 0.9822901856766048\n",
      "num_iter : 203, loss = 0.9650724746847844\n",
      "num_iter : 204, loss = 0.9481719628162296\n",
      "num_iter : 205, loss = 0.93158266598393\n",
      "num_iter : 206, loss = 0.9152987187850148\n",
      "num_iter : 207, loss = 0.8993143719129539\n",
      "num_iter : 208, loss = 0.8836239896351845\n",
      "num_iter : 209, loss = 0.8682220473341666\n",
      "num_iter : 210, loss = 0.8531031291099863\n",
      "num_iter : 211, loss = 0.8382619254426735\n",
      "num_iter : 212, loss = 0.8236932309124505\n",
      "num_iter : 213, loss = 0.8093919419762203\n",
      "num_iter : 214, loss = 0.7953530547986403\n",
      "num_iter : 215, loss = 0.7815716631361798\n",
      "num_iter : 216, loss = 0.7680429562726376\n",
      "num_iter : 217, loss = 0.7547622170046195\n",
      "num_iter : 218, loss = 0.7417248196755523\n",
      "num_iter : 219, loss = 0.7289262282568296\n",
      "num_iter : 220, loss = 0.7163619944747748\n",
      "num_iter : 221, loss = 0.7040277559820936\n",
      "num_iter : 222, loss = 0.691919234572589\n",
      "num_iter : 223, loss = 0.6800322344379164\n",
      "num_iter : 224, loss = 0.668362640465203\n",
      "num_iter : 225, loss = 0.6569064165744115\n",
      "num_iter : 226, loss = 0.6456596040943358\n",
      "num_iter : 227, loss = 0.6346183201761841\n",
      "num_iter : 228, loss = 0.6237787562437158\n",
      "num_iter : 229, loss = 0.6131371764789391\n",
      "num_iter : 230, loss = 0.6026899163424102\n",
      "num_iter : 231, loss = 0.5924333811272053\n",
      "num_iter : 232, loss = 0.5823640445456642\n",
      "num_iter : 233, loss = 0.5724784473480319\n",
      "num_iter : 234, loss = 0.5627731959721597\n",
      "num_iter : 235, loss = 0.5532449612234372\n",
      "num_iter : 236, loss = 0.5438904769841773\n",
      "num_iter : 237, loss = 0.5347065389516767\n",
      "num_iter : 238, loss = 0.5256900034042096\n",
      "num_iter : 239, loss = 0.5168377859942351\n",
      "num_iter : 240, loss = 0.508146860568126\n",
      "num_iter : 241, loss = 0.49961425801172576\n",
      "num_iter : 242, loss = 0.49123706512109805\n",
      "num_iter : 243, loss = 0.48301242349780854\n",
      "num_iter : 244, loss = 0.474937528468145\n",
      "num_iter : 245, loss = 0.46700962802565854\n",
      "num_iter : 246, loss = 0.4592260217964525\n",
      "num_iter : 247, loss = 0.4515840600266543\n",
      "num_iter : 248, loss = 0.4440811425915223\n",
      "num_iter : 249, loss = 0.4367147180256552\n",
      "num_iter : 250, loss = 0.42948228257378984\n",
      "num_iter : 251, loss = 0.4223813792616865\n",
      "num_iter : 252, loss = 0.415409596986612\n",
      "num_iter : 253, loss = 0.4085645696269512\n",
      "num_iter : 254, loss = 0.4018439751704923\n",
      "num_iter : 255, loss = 0.3952455348609284\n",
      "num_iter : 256, loss = 0.3887670123621523\n",
      "num_iter : 257, loss = 0.3824062129399216\n",
      "num_iter : 258, loss = 0.3761609826604805\n",
      "num_iter : 259, loss = 0.37002920760574565\n",
      "num_iter : 260, loss = 0.3640088131046674\n",
      "num_iter : 261, loss = 0.3580977629803857\n",
      "num_iter : 262, loss = 0.3522940588128272\n",
      "num_iter : 263, loss = 0.3465957392163713\n",
      "num_iter : 264, loss = 0.3410008791322532\n",
      "num_iter : 265, loss = 0.33550758913536133\n",
      "num_iter : 266, loss = 0.3301140147550987\n",
      "num_iter : 267, loss = 0.3248183358099923\n",
      "num_iter : 268, loss = 0.3196187657557424\n",
      "num_iter : 269, loss = 0.31451355104640716\n",
      "num_iter : 270, loss = 0.3095009705084263\n",
      "num_iter : 271, loss = 0.30457933472719984\n",
      "num_iter : 272, loss = 0.2997469854459409\n",
      "num_iter : 273, loss = 0.29500229497653496\n",
      "num_iter : 274, loss = 0.29034366562213687\n",
      "num_iter : 275, loss = 0.2857695291112454\n",
      "num_iter : 276, loss = 0.2812783460430107\n",
      "num_iter : 277, loss = 0.2768686053435251\n",
      "num_iter : 278, loss = 0.27253882373285737\n",
      "num_iter : 279, loss = 0.2682875452026017\n",
      "num_iter : 280, loss = 0.2641133405037119\n",
      "num_iter : 281, loss = 0.26001480664439686\n",
      "num_iter : 282, loss = 0.2559905663978693\n",
      "num_iter : 283, loss = 0.25203926781972885\n",
      "num_iter : 284, loss = 0.24815958377477673\n",
      "num_iter : 285, loss = 0.24435021147306843\n",
      "num_iter : 286, loss = 0.24060987201499473\n",
      "num_iter : 287, loss = 0.23693730994521559\n",
      "num_iter : 288, loss = 0.23333129281525244\n",
      "num_iter : 289, loss = 0.22979061075455537\n",
      "num_iter : 290, loss = 0.22631407604987455\n",
      "num_iter : 291, loss = 0.22290052273276123\n",
      "num_iter : 292, loss = 0.21954880617502215\n",
      "num_iter : 293, loss = 0.216257802691973\n",
      "num_iter : 294, loss = 0.21302640915332255\n",
      "num_iter : 295, loss = 0.20985354260153213\n",
      "num_iter : 296, loss = 0.20673813987749676\n",
      "num_iter : 297, loss = 0.20367915725340113\n",
      "num_iter : 298, loss = 0.20067557007259387\n",
      "num_iter : 299, loss = 0.1977263723963509\n",
      "num_iter : 300, loss = 0.19483057665737452\n",
      "num_iter : 301, loss = 0.1919872133199004\n",
      "num_iter : 302, loss = 0.18919533054627527\n",
      "num_iter : 303, loss = 0.1864539938698749\n",
      "num_iter : 304, loss = 0.18376228587423274\n",
      "num_iter : 305, loss = 0.18111930587825903\n",
      "num_iter : 306, loss = 0.17852416962742299\n",
      "num_iter : 307, loss = 0.17597600899078308\n",
      "num_iter : 308, loss = 0.17347397166374506\n",
      "num_iter : 309, loss = 0.1710172208764358\n",
      "num_iter : 310, loss = 0.16860493510758182\n",
      "num_iter : 311, loss = 0.1662363078037796\n",
      "num_iter : 312, loss = 0.16391054710405525\n",
      "num_iter : 313, loss = 0.16162687556960587\n",
      "num_iter : 314, loss = 0.15938452991862193\n",
      "num_iter : 315, loss = 0.15718276076609106\n",
      "num_iter : 316, loss = 0.15502083236848374\n",
      "num_iter : 317, loss = 0.15289802237322642\n",
      "num_iter : 318, loss = 0.15081362157286657\n",
      "num_iter : 319, loss = 0.14876693366384106\n",
      "num_iter : 320, loss = 0.1467572750097538\n",
      "num_iter : 321, loss = 0.1447839744090801\n",
      "num_iter : 322, loss = 0.14284637286720592\n",
      "num_iter : 323, loss = 0.14094382337272307\n",
      "num_iter : 324, loss = 0.13907569067789222\n",
      "num_iter : 325, loss = 0.1372413510832007\n",
      "num_iter : 326, loss = 0.13544019222592682\n",
      "num_iter : 327, loss = 0.13367161287264087\n",
      "num_iter : 328, loss = 0.1319350227155648\n",
      "num_iter : 329, loss = 0.1302298421727116\n",
      "num_iter : 330, loss = 0.12855550219174183\n",
      "num_iter : 331, loss = 0.1269114440574538\n",
      "num_iter : 332, loss = 0.1252971192028485\n",
      "num_iter : 333, loss = 0.12371198902369436\n",
      "num_iter : 334, loss = 0.12215552469652706\n",
      "num_iter : 335, loss = 0.12062720700001857\n",
      "num_iter : 336, loss = 0.11912652613965191\n",
      "num_iter : 337, loss = 0.11765298157563787\n",
      "num_iter : 338, loss = 0.11620608185401105\n",
      "num_iter : 339, loss = 0.11478534444084819\n",
      "num_iter : 340, loss = 0.11339029555954483\n",
      "num_iter : 341, loss = 0.11202047003109741\n",
      "num_iter : 342, loss = 0.11067541111732931\n",
      "num_iter : 343, loss = 0.10935467036700783\n",
      "num_iter : 344, loss = 0.10805780746479872\n",
      "num_iter : 345, loss = 0.10678439008299838\n",
      "num_iter : 346, loss = 0.10553399373600124\n",
      "num_iter : 347, loss = 0.10430620163744059\n",
      "num_iter : 348, loss = 0.10310060455996088\n",
      "num_iter : 349, loss = 0.10191680069756621\n",
      "num_iter : 350, loss = 0.10075439553050114\n",
      "num_iter : 351, loss = 0.09961300169261049\n",
      "num_iter : 352, loss = 0.09849223884113904\n",
      "num_iter : 353, loss = 0.09739173352891983\n",
      "num_iter : 354, loss = 0.09631111907890678\n",
      "num_iter : 355, loss = 0.09525003546101138\n",
      "num_iter : 356, loss = 0.09420812917119771\n",
      "num_iter : 357, loss = 0.09318505311279231\n",
      "num_iter : 358, loss = 0.0921804664799717\n",
      "num_iter : 359, loss = 0.09119403464338163\n",
      "num_iter : 360, loss = 0.09022542903785387\n",
      "num_iter : 361, loss = 0.08927432705217814\n",
      "num_iter : 362, loss = 0.08834041192089023\n",
      "num_iter : 363, loss = 0.08742337261804213\n",
      "num_iter : 364, loss = 0.0865229037529144\n",
      "num_iter : 365, loss = 0.08563870546763518\n",
      "num_iter : 366, loss = 0.08477048333667286\n",
      "num_iter : 367, loss = 0.08391794826816434\n",
      "num_iter : 368, loss = 0.08308081640704633\n",
      "num_iter : 369, loss = 0.08225880903995691\n",
      "num_iter : 370, loss = 0.0814516525018744\n",
      "num_iter : 371, loss = 0.08065907808445955\n",
      "num_iter : 372, loss = 0.07988082194607189\n",
      "num_iter : 373, loss = 0.07911662502342869\n",
      "num_iter : 374, loss = 0.07836623294487435\n",
      "num_iter : 375, loss = 0.07762939594523391\n",
      "num_iter : 376, loss = 0.07690586878221858\n",
      "num_iter : 377, loss = 0.07619541065435542\n",
      "num_iter : 378, loss = 0.07549778512041355\n",
      "num_iter : 379, loss = 0.0748127600202984\n",
      "num_iter : 380, loss = 0.07414010739738951\n",
      "num_iter : 381, loss = 0.0734796034222911\n",
      "num_iter : 382, loss = 0.07283102831797234\n",
      "num_iter : 383, loss = 0.07219416628627166\n",
      "num_iter : 384, loss = 0.07156880543573982\n",
      "num_iter : 385, loss = 0.07095473771079518\n",
      "num_iter : 386, loss = 0.07035175882216987\n",
      "num_iter : 387, loss = 0.06975966817862164\n",
      "num_iter : 388, loss = 0.06917826881988834\n",
      "num_iter : 389, loss = 0.06860736735086242\n",
      "num_iter : 390, loss = 0.0680467738769634\n",
      "num_iter : 391, loss = 0.06749630194068411\n",
      "num_iter : 392, loss = 0.06695576845929427\n",
      "num_iter : 393, loss = 0.06642499366367301\n",
      "num_iter : 394, loss = 0.06590380103825697\n",
      "num_iter : 395, loss = 0.06539201726207786\n",
      "num_iter : 396, loss = 0.06488947215087279\n",
      "num_iter : 397, loss = 0.06439599860024633\n",
      "num_iter : 398, loss = 0.06391143252986599\n",
      "num_iter : 399, loss = 0.06343561282867134\n",
      "num_iter : 400, loss = 0.06296838130107797\n",
      "num_iter : 401, loss = 0.06250958261416012\n",
      "num_iter : 402, loss = 0.0620590642457915\n",
      "num_iter : 403, loss = 0.06161667643372864\n",
      "num_iter : 404, loss = 0.061182272125617845\n",
      "num_iter : 405, loss = 0.060755706929911474\n",
      "num_iter : 406, loss = 0.06033683906767409\n",
      "num_iter : 407, loss = 0.05992552932526483\n",
      "num_iter : 408, loss = 0.05952164100787654\n",
      "num_iter : 409, loss = 0.05912503989392088\n",
      "num_iter : 410, loss = 0.05873559419023939\n",
      "num_iter : 411, loss = 0.05835317448812788\n",
      "num_iter : 412, loss = 0.05797765372015919\n",
      "num_iter : 413, loss = 0.05760890711778857\n",
      "num_iter : 414, loss = 0.057246812169729155\n",
      "num_iter : 415, loss = 0.05689124858108169\n",
      "num_iter : 416, loss = 0.05654209823320614\n",
      "num_iter : 417, loss = 0.05619924514432127\n",
      "num_iter : 418, loss = 0.05586257543081824\n",
      "num_iter : 419, loss = 0.05553197726927594\n",
      "num_iter : 420, loss = 0.055207340859166024\n",
      "num_iter : 421, loss = 0.054888558386232425\n",
      "num_iter : 422, loss = 0.05457552398653708\n",
      "num_iter : 423, loss = 0.054268133711155624\n",
      "num_iter : 424, loss = 0.05396628549151387\n",
      "num_iter : 425, loss = 0.053669879105352876\n",
      "num_iter : 426, loss = 0.053378816143310845\n",
      "num_iter : 427, loss = 0.053092999976109924\n",
      "num_iter : 428, loss = 0.052812335722339294\n",
      "num_iter : 429, loss = 0.05253673021682067\n",
      "num_iter : 430, loss = 0.05226609197954805\n",
      "num_iter : 431, loss = 0.05200033118518945\n",
      "num_iter : 432, loss = 0.05173935963314241\n",
      "num_iter : 433, loss = 0.05148309071813085\n",
      "num_iter : 434, loss = 0.051231439401335455\n",
      "num_iter : 435, loss = 0.05098432218204704\n",
      "num_iter : 436, loss = 0.050741657069833336\n",
      "num_iter : 437, loss = 0.05050336355720996\n",
      "num_iter : 438, loss = 0.05026936259280616\n",
      "num_iter : 439, loss = 0.05003957655501718\n",
      "num_iter : 440, loss = 0.049813929226133834\n",
      "num_iter : 441, loss = 0.04959234576693987\n",
      "num_iter : 442, loss = 0.049374752691770545\n",
      "num_iter : 443, loss = 0.04916107784402233\n",
      "num_iter : 444, loss = 0.048951250372105806\n",
      "num_iter : 445, loss = 0.04874520070583456\n",
      "num_iter : 446, loss = 0.04854286053324158\n",
      "num_iter : 447, loss = 0.04834416277781544\n",
      "num_iter : 448, loss = 0.04814904157614825\n",
      "num_iter : 449, loss = 0.04795743225598913\n",
      "num_iter : 450, loss = 0.04776927131469429\n",
      "num_iter : 451, loss = 0.04758449639806752\n",
      "num_iter : 452, loss = 0.047403046279584346\n",
      "num_iter : 453, loss = 0.04722486083999114\n",
      "num_iter : 454, loss = 0.04704988104727478\n",
      "num_iter : 455, loss = 0.04687804893699403\n",
      "num_iter : 456, loss = 0.046709307592967764\n",
      "num_iter : 457, loss = 0.04654360112831154\n",
      "num_iter : 458, loss = 0.046380874666819095\n",
      "num_iter : 459, loss = 0.046221074324679555\n",
      "num_iter : 460, loss = 0.04606414719252575\n",
      "num_iter : 461, loss = 0.045910041317808264\n",
      "num_iter : 462, loss = 0.04575870568748767\n",
      "num_iter : 463, loss = 0.04561009021104027\n",
      "num_iter : 464, loss = 0.045464145703771386\n",
      "num_iter : 465, loss = 0.04532082387043097\n",
      "num_iter : 466, loss = 0.04518007728912531\n",
      "num_iter : 467, loss = 0.04504185939551981\n",
      "num_iter : 468, loss = 0.044906124467328355\n",
      "num_iter : 469, loss = 0.04477282760908254\n",
      "num_iter : 470, loss = 0.04464192473717712\n",
      "num_iter : 471, loss = 0.04451337256518548\n",
      "num_iter : 472, loss = 0.0443871285894418\n",
      "num_iter : 473, loss = 0.04426315107488269\n",
      "num_iter : 474, loss = 0.044141399041146424\n",
      "num_iter : 475, loss = 0.04402183224892283\n",
      "num_iter : 476, loss = 0.0439044111865505\n",
      "num_iter : 477, loss = 0.04378909705685667\n",
      "num_iter : 478, loss = 0.043675851764234205\n",
      "num_iter : 479, loss = 0.043564637901954216\n",
      "num_iter : 480, loss = 0.043455418739706285\n",
      "num_iter : 481, loss = 0.043348158211365555\n",
      "num_iter : 482, loss = 0.04324282090298039\n",
      "num_iter : 483, loss = 0.043139372040977654\n",
      "num_iter : 484, loss = 0.04303777748058122\n",
      "num_iter : 485, loss = 0.042938003694440126\n",
      "num_iter : 486, loss = 0.04284001776146225\n",
      "num_iter : 487, loss = 0.04274378735585042\n",
      "num_iter : 488, loss = 0.04264928073633649\n",
      "num_iter : 489, loss = 0.04255646673561005\n",
      "num_iter : 490, loss = 0.04246531474993914\n",
      "num_iter : 491, loss = 0.042375794728977505\n",
      "num_iter : 492, loss = 0.042287877165757486\n",
      "num_iter : 493, loss = 0.04220153308686327\n",
      "num_iter : 494, loss = 0.04211673404278164\n",
      "num_iter : 495, loss = 0.04203345209842799\n",
      "num_iter : 496, loss = 0.04195165982384316\n",
      "num_iter : 497, loss = 0.04187133028505892\n",
      "num_iter : 498, loss = 0.04179243703512843\n",
      "num_iter : 499, loss = 0.041714954105318794\n",
      "num_iter : 500, loss = 0.04163885599646341\n",
      "num_iter : 501, loss = 0.04156411767047049\n",
      "num_iter : 502, loss = 0.04149071454198498\n",
      "num_iter : 503, loss = 0.04141862247020155\n",
      "num_iter : 504, loss = 0.04134781775082572\n",
      "num_iter : 505, loss = 0.041278277108179946\n",
      "num_iter : 506, loss = 0.04120997768745294\n",
      "num_iter : 507, loss = 0.04114289704708922\n",
      "num_iter : 508, loss = 0.041077013151315955\n",
      "num_iter : 509, loss = 0.041012304362804904\n",
      "num_iter : 510, loss = 0.04094874943546775\n",
      "num_iter : 511, loss = 0.040886327507381005\n",
      "num_iter : 512, loss = 0.040825018093839045\n",
      "num_iter : 513, loss = 0.04076480108053333\n",
      "num_iter : 514, loss = 0.04070565671685425\n",
      "num_iter : 515, loss = 0.04064756560931446\n",
      "num_iter : 516, loss = 0.04059050871509101\n",
      "num_iter : 517, loss = 0.04053446733568479\n",
      "num_iter : 518, loss = 0.040479423110693684\n",
      "num_iter : 519, loss = 0.04042535801169933\n",
      "num_iter : 520, loss = 0.04037225433626392\n",
      "num_iter : 521, loss = 0.04032009470203539\n",
      "num_iter : 522, loss = 0.040268862040959745\n",
      "num_iter : 523, loss = 0.04021853959359776\n",
      "num_iter : 524, loss = 0.04016911090354418\n",
      "num_iter : 525, loss = 0.04012055981194873\n",
      "num_iter : 526, loss = 0.040072870452134896\n",
      "num_iter : 527, loss = 0.0400260272443176\n",
      "num_iter : 528, loss = 0.03998001489041471\n",
      "num_iter : 529, loss = 0.03993481836895394\n",
      "num_iter : 530, loss = 0.039890422930070585\n",
      "num_iter : 531, loss = 0.03984681409059681\n",
      "num_iter : 532, loss = 0.039803977629238756\n",
      "num_iter : 533, loss = 0.03976189958184147\n",
      "num_iter : 534, loss = 0.03972056623673957\n",
      "num_iter : 535, loss = 0.039679964130191314\n",
      "num_iter : 536, loss = 0.03964008004189589\n",
      "num_iter : 537, loss = 0.03960090099059152\n",
      "num_iter : 538, loss = 0.039562414229733125\n",
      "num_iter : 539, loss = 0.03952460724324832\n",
      "num_iter : 540, loss = 0.03948746774136989\n",
      "num_iter : 541, loss = 0.0394509836565438\n",
      "num_iter : 542, loss = 0.03941514313941138\n",
      "num_iter : 543, loss = 0.03937993455486372\n",
      "num_iter : 544, loss = 0.03934534647816782\n",
      "num_iter : 545, loss = 0.03931136769116256\n",
      "num_iter : 546, loss = 0.039277987178523585\n",
      "num_iter : 547, loss = 0.03924519412409563\n",
      "num_iter : 548, loss = 0.039212977907291176\n",
      "num_iter : 549, loss = 0.03918132809955447\n",
      "num_iter : 550, loss = 0.03915023446088908\n",
      "num_iter : 551, loss = 0.03911968693644855\n",
      "num_iter : 552, loss = 0.03908967565318844\n",
      "num_iter : 553, loss = 0.03906019091657931\n",
      "num_iter : 554, loss = 0.039031223207378826\n",
      "num_iter : 555, loss = 0.039002763178462095\n",
      "num_iter : 556, loss = 0.03897480165170992\n",
      "num_iter : 557, loss = 0.038947329614952605\n",
      "num_iter : 558, loss = 0.038920338218969425\n",
      "num_iter : 559, loss = 0.038893818774542344\n",
      "num_iter : 560, loss = 0.03886776274956293\n",
      "num_iter : 561, loss = 0.03884216176619163\n",
      "num_iter : 562, loss = 0.038817007598068234\n",
      "num_iter : 563, loss = 0.03879229216757303\n",
      "num_iter : 564, loss = 0.03876800754313743\n",
      "num_iter : 565, loss = 0.038744145936603094\n",
      "num_iter : 566, loss = 0.038720699700628865\n",
      "num_iter : 567, loss = 0.03869766132614465\n",
      "num_iter : 568, loss = 0.038675023439851346\n",
      "num_iter : 569, loss = 0.03865277880176587\n",
      "num_iter : 570, loss = 0.03863092030281067\n",
      "num_iter : 571, loss = 0.03860944096244684\n",
      "num_iter : 572, loss = 0.038588333926349784\n",
      "num_iter : 573, loss = 0.03856759246412734\n",
      "num_iter : 574, loss = 0.03854720996707888\n",
      "num_iter : 575, loss = 0.03852717994599492\n",
      "num_iter : 576, loss = 0.038507496028996585\n",
      "num_iter : 577, loss = 0.03848815195941423\n",
      "num_iter : 578, loss = 0.038469141593704084\n",
      "num_iter : 579, loss = 0.038450458899403184\n",
      "num_iter : 580, loss = 0.03843209795312063\n",
      "num_iter : 581, loss = 0.03841405293856551\n",
      "num_iter : 582, loss = 0.038396318144610385\n",
      "num_iter : 583, loss = 0.03837888796338986\n",
      "num_iter : 584, loss = 0.03836175688843364\n",
      "num_iter : 585, loss = 0.03834491951283283\n",
      "num_iter : 586, loss = 0.038328370527440085\n",
      "num_iter : 587, loss = 0.038312104719101764\n",
      "num_iter : 588, loss = 0.03829611696892259\n",
      "num_iter : 589, loss = 0.0382804022505609\n",
      "num_iter : 590, loss = 0.03826495562855554\n",
      "num_iter : 591, loss = 0.0382497722566825\n",
      "num_iter : 592, loss = 0.03823484737634142\n",
      "num_iter : 593, loss = 0.03822017631497135\n",
      "num_iter : 594, loss = 0.038205754484495015\n",
      "num_iter : 595, loss = 0.038191577379791175\n",
      "num_iter : 596, loss = 0.038177640577194874\n",
      "num_iter : 597, loss = 0.038163939733024524\n",
      "num_iter : 598, loss = 0.03815047058213573\n",
      "num_iter : 599, loss = 0.03813722893650133\n",
      "num_iter : 600, loss = 0.03812421068381691\n",
      "num_iter : 601, loss = 0.03811141178613181\n",
      "num_iter : 602, loss = 0.038098828278504714\n",
      "num_iter : 603, loss = 0.0380864562676834\n",
      "num_iter : 604, loss = 0.03807429193080886\n",
      "num_iter : 605, loss = 0.038062331514142116\n",
      "num_iter : 606, loss = 0.038050571331814845\n",
      "num_iter : 607, loss = 0.03803900776460198\n",
      "num_iter : 608, loss = 0.038027637258716815\n",
      "num_iter : 609, loss = 0.03801645632462786\n",
      "num_iter : 610, loss = 0.038005461535896984\n",
      "num_iter : 611, loss = 0.03799464952803864\n",
      "num_iter : 612, loss = 0.03798401699739971\n",
      "num_iter : 613, loss = 0.03797356070005969\n",
      "num_iter : 614, loss = 0.03796327745075052\n",
      "num_iter : 615, loss = 0.037953164121796175\n",
      "num_iter : 616, loss = 0.037943217642071356\n",
      "num_iter : 617, loss = 0.03793343499597897\n",
      "num_iter : 618, loss = 0.037923813222446236\n",
      "num_iter : 619, loss = 0.03791434941393865\n",
      "num_iter : 620, loss = 0.03790504071549214\n",
      "num_iter : 621, loss = 0.03789588432376251\n",
      "num_iter : 622, loss = 0.03788687748609192\n",
      "num_iter : 623, loss = 0.03787801749959284\n",
      "num_iter : 624, loss = 0.03786930171024772\n",
      "num_iter : 625, loss = 0.03786072751202564\n",
      "num_iter : 626, loss = 0.03785229234601466\n",
      "num_iter : 627, loss = 0.03784399369956973\n",
      "num_iter : 628, loss = 0.037835829105476254\n",
      "num_iter : 629, loss = 0.03782779614112873\n",
      "num_iter : 630, loss = 0.03781989242772401\n",
      "num_iter : 631, loss = 0.0378121156294694\n",
      "num_iter : 632, loss = 0.03780446345280505\n",
      "num_iter : 633, loss = 0.037796933645640275\n",
      "num_iter : 634, loss = 0.03778952399660383\n",
      "num_iter : 635, loss = 0.037782232334307594\n",
      "num_iter : 636, loss = 0.03777505652662367\n",
      "num_iter : 637, loss = 0.037767994479974566\n",
      "num_iter : 638, loss = 0.037761044138636075\n",
      "num_iter : 639, loss = 0.037754203484053006\n",
      "num_iter : 640, loss = 0.03774747053416704\n",
      "num_iter : 641, loss = 0.03774084334275677\n",
      "num_iter : 642, loss = 0.03773431999878992\n",
      "num_iter : 643, loss = 0.03772789862578695\n",
      "num_iter : 644, loss = 0.03772157738119643\n",
      "num_iter : 645, loss = 0.03771535445578141\n",
      "num_iter : 646, loss = 0.03770922807301735\n",
      "num_iter : 647, loss = 0.037703196488500394\n",
      "num_iter : 648, loss = 0.03769725798936667\n",
      "num_iter : 649, loss = 0.037691410893722126\n",
      "num_iter : 650, loss = 0.037685653550082465\n",
      "num_iter : 651, loss = 0.037679984336823416\n",
      "num_iter : 652, loss = 0.03767440166164069\n",
      "num_iter : 653, loss = 0.037668903961019976\n",
      "num_iter : 654, loss = 0.03766348969971633\n",
      "num_iter : 655, loss = 0.03765815737024311\n",
      "num_iter : 656, loss = 0.03765290549237002\n",
      "num_iter : 657, loss = 0.03764773261263035\n",
      "num_iter : 658, loss = 0.03764263730383695\n",
      "num_iter : 659, loss = 0.03763761816460727\n",
      "num_iter : 660, loss = 0.03763267381889664\n",
      "num_iter : 661, loss = 0.03762780291554018\n",
      "num_iter : 662, loss = 0.037623004127803\n",
      "num_iter : 663, loss = 0.03761827615293844\n",
      "num_iter : 664, loss = 0.03761361771175431\n",
      "num_iter : 665, loss = 0.03760902754818714\n",
      "num_iter : 666, loss = 0.03760450442888385\n",
      "num_iter : 667, loss = 0.03760004714279119\n",
      "num_iter : 668, loss = 0.03759565450075254\n",
      "num_iter : 669, loss = 0.03759132533511205\n",
      "num_iter : 670, loss = 0.03758705849932582\n",
      "num_iter : 671, loss = 0.03758285286758017\n",
      "num_iter : 672, loss = 0.037578707334416865\n",
      "num_iter : 673, loss = 0.03757462081436511\n",
      "num_iter : 674, loss = 0.03757059224158011\n",
      "num_iter : 675, loss = 0.03756662056948813\n",
      "num_iter : 676, loss = 0.03756270477043833\n",
      "num_iter : 677, loss = 0.03755884383536025\n",
      "num_iter : 678, loss = 0.03755503677342821\n",
      "num_iter : 679, loss = 0.03755128261173126\n",
      "num_iter : 680, loss = 0.03754758039494919\n",
      "num_iter : 681, loss = 0.0375439291850347\n",
      "num_iter : 682, loss = 0.03754032806090089\n",
      "num_iter : 683, loss = 0.03753677611811482\n",
      "num_iter : 684, loss = 0.03753327246859602\n",
      "num_iter : 685, loss = 0.03752981624032131\n",
      "num_iter : 686, loss = 0.0375264065770341\n",
      "num_iter : 687, loss = 0.03752304263795944\n",
      "num_iter : 688, loss = 0.037519723597524146\n",
      "num_iter : 689, loss = 0.03751644864508191\n",
      "num_iter : 690, loss = 0.03751321698464336\n",
      "num_iter : 691, loss = 0.03751002783461111\n",
      "num_iter : 692, loss = 0.037506880427519596\n",
      "num_iter : 693, loss = 0.03750377400977935\n",
      "num_iter : 694, loss = 0.03750070784142638\n",
      "num_iter : 695, loss = 0.03749768119587564\n",
      "num_iter : 696, loss = 0.03749469335967915\n",
      "num_iter : 697, loss = 0.03749174363228856\n",
      "num_iter : 698, loss = 0.03748883132582178\n",
      "num_iter : 699, loss = 0.037485955764834086\n",
      "num_iter : 700, loss = 0.037483116286093104\n",
      "num_iter : 701, loss = 0.037480312238358175\n",
      "num_iter : 702, loss = 0.037477542982163485\n",
      "num_iter : 703, loss = 0.03747480788960497\n",
      "num_iter : 704, loss = 0.03747210634413162\n",
      "num_iter : 705, loss = 0.03746943774033985\n",
      "num_iter : 706, loss = 0.03746680148377224\n",
      "num_iter : 707, loss = 0.037464196990719456\n",
      "num_iter : 708, loss = 0.03746162368802592\n",
      "num_iter : 709, loss = 0.037459081012899124\n",
      "num_iter : 710, loss = 0.037456568412722095\n",
      "num_iter : 711, loss = 0.03745408534486946\n",
      "num_iter : 712, loss = 0.03745163127652687\n",
      "num_iter : 713, loss = 0.037449205684513515\n",
      "num_iter : 714, loss = 0.03744680805510802\n",
      "num_iter : 715, loss = 0.03744443788387731\n",
      "num_iter : 716, loss = 0.037442094675508804\n",
      "num_iter : 717, loss = 0.03743977794364539\n",
      "num_iter : 718, loss = 0.03743748721072356\n",
      "num_iter : 719, loss = 0.03743522200781443\n",
      "num_iter : 720, loss = 0.037432981874467586\n",
      "num_iter : 721, loss = 0.03743076635855776\n",
      "num_iter : 722, loss = 0.03742857501613443\n",
      "num_iter : 723, loss = 0.03742640741127387\n",
      "num_iter : 724, loss = 0.037424263115934074\n",
      "num_iter : 725, loss = 0.03742214170981226\n",
      "num_iter : 726, loss = 0.037420042780204936\n",
      "num_iter : 727, loss = 0.03741796592187055\n",
      "num_iter : 728, loss = 0.03741591073689442\n",
      "num_iter : 729, loss = 0.037413876834556443\n",
      "num_iter : 730, loss = 0.0374118638312009\n",
      "num_iter : 731, loss = 0.03740987135010871\n",
      "num_iter : 732, loss = 0.03740789902137215\n",
      "num_iter : 733, loss = 0.037405946481771486\n",
      "num_iter : 734, loss = 0.037404013374654294\n",
      "num_iter : 735, loss = 0.03740209934981652\n",
      "num_iter : 736, loss = 0.03740020406338598\n",
      "num_iter : 737, loss = 0.03739832717770783\n",
      "num_iter : 738, loss = 0.037396468361232206\n",
      "num_iter : 739, loss = 0.037394627288403835\n",
      "num_iter : 740, loss = 0.037392803639553614\n",
      "num_iter : 741, loss = 0.037390997100792155\n",
      "num_iter : 742, loss = 0.0373892073639054\n",
      "num_iter : 743, loss = 0.037387434126251895\n",
      "num_iter : 744, loss = 0.03738567709066206\n",
      "num_iter : 745, loss = 0.03738393596533931\n",
      "num_iter : 746, loss = 0.037382210463762854\n",
      "num_iter : 747, loss = 0.037380500304592265\n",
      "num_iter : 748, loss = 0.037378805211573926\n",
      "num_iter : 749, loss = 0.037377124913448974\n",
      "num_iter : 750, loss = 0.03737545914386296\n",
      "num_iter : 751, loss = 0.03737380764127737\n",
      "num_iter : 752, loss = 0.03737217014888226\n",
      "num_iter : 753, loss = 0.03737054641451095\n",
      "num_iter : 754, loss = 0.03736893619055609\n",
      "num_iter : 755, loss = 0.03736733923388708\n",
      "num_iter : 756, loss = 0.03736575530576927\n",
      "num_iter : 757, loss = 0.03736418417178436\n",
      "num_iter : 758, loss = 0.03736262560175247\n",
      "num_iter : 759, loss = 0.03736107936965537\n",
      "num_iter : 760, loss = 0.03735954525356146\n",
      "num_iter : 761, loss = 0.03735802303555158\n",
      "num_iter : 762, loss = 0.03735651250164671\n",
      "num_iter : 763, loss = 0.03735501344173658\n",
      "num_iter : 764, loss = 0.03735352564950986\n",
      "num_iter : 765, loss = 0.03735204892238532\n",
      "num_iter : 766, loss = 0.03735058306144447\n",
      "num_iter : 767, loss = 0.03734912787136534\n",
      "num_iter : 768, loss = 0.037347683160357414\n",
      "num_iter : 769, loss = 0.03734624874009785\n",
      "num_iter : 770, loss = 0.037344824425668645\n",
      "num_iter : 771, loss = 0.03734341003549527\n",
      "num_iter : 772, loss = 0.037342005391285994\n",
      "num_iter : 773, loss = 0.037340610317972756\n",
      "num_iter : 774, loss = 0.037339224643652635\n",
      "num_iter : 775, loss = 0.037337848199530885\n",
      "num_iter : 776, loss = 0.03733648081986451\n",
      "num_iter : 777, loss = 0.03733512234190724\n",
      "num_iter : 778, loss = 0.03733377260585525\n",
      "num_iter : 779, loss = 0.03733243145479402\n",
      "num_iter : 780, loss = 0.037331098734646025\n",
      "num_iter : 781, loss = 0.037329774294119586\n",
      "num_iter : 782, loss = 0.03732845798465828\n",
      "num_iter : 783, loss = 0.03732714966039174\n",
      "num_iter : 784, loss = 0.037325849178086776\n",
      "num_iter : 785, loss = 0.03732455639710007\n",
      "num_iter : 786, loss = 0.03732327117933097\n",
      "num_iter : 787, loss = 0.03732199338917587\n",
      "num_iter : 788, loss = 0.03732072289348274\n",
      "num_iter : 789, loss = 0.03731945956150704\n",
      "num_iter : 790, loss = 0.0373182032648681\n",
      "num_iter : 791, loss = 0.03731695387750628\n",
      "num_iter : 792, loss = 0.037315711275641227\n",
      "num_iter : 793, loss = 0.037314475337730496\n",
      "num_iter : 794, loss = 0.0373132459444291\n",
      "num_iter : 795, loss = 0.03731202297854984\n",
      "num_iter : 796, loss = 0.03731080632502425\n",
      "num_iter : 797, loss = 0.03730959587086432\n",
      "num_iter : 798, loss = 0.03730839150512487\n",
      "num_iter : 799, loss = 0.0373071931188665\n",
      "num_iter : 800, loss = 0.03730600060511962\n",
      "num_iter : 801, loss = 0.037304813858848436\n",
      "num_iter : 802, loss = 0.03730363277691631\n",
      "num_iter : 803, loss = 0.03730245725805131\n",
      "num_iter : 804, loss = 0.03730128720281242\n",
      "num_iter : 805, loss = 0.03730012251355658\n",
      "num_iter : 806, loss = 0.03729896309440604\n",
      "num_iter : 807, loss = 0.037297808851216536\n",
      "num_iter : 808, loss = 0.03729665969154593\n",
      "num_iter : 809, loss = 0.03729551552462334\n",
      "num_iter : 810, loss = 0.037294376261319116\n",
      "num_iter : 811, loss = 0.03729324181411498\n",
      "num_iter : 812, loss = 0.03729211209707498\n",
      "num_iter : 813, loss = 0.03729098702581688\n",
      "num_iter : 814, loss = 0.03728986651748401\n",
      "num_iter : 815, loss = 0.037288750490717754\n",
      "num_iter : 816, loss = 0.03728763886563046\n",
      "num_iter : 817, loss = 0.03728653156377874\n",
      "num_iter : 818, loss = 0.0372854285081375\n",
      "num_iter : 819, loss = 0.03728432962307423\n",
      "num_iter : 820, loss = 0.03728323483432381\n",
      "num_iter : 821, loss = 0.037282144068963755\n",
      "num_iter : 822, loss = 0.03728105725539006\n",
      "num_iter : 823, loss = 0.03727997432329328\n",
      "num_iter : 824, loss = 0.03727889520363511\n",
      "num_iter : 825, loss = 0.037277819828625415\n",
      "num_iter : 826, loss = 0.03727674813169966\n",
      "num_iter : 827, loss = 0.03727568004749684\n",
      "num_iter : 828, loss = 0.037274615511837535\n",
      "num_iter : 829, loss = 0.03727355446170272\n",
      "num_iter : 830, loss = 0.03727249683521266\n",
      "num_iter : 831, loss = 0.037271442571606465\n",
      "num_iter : 832, loss = 0.03727039161122164\n",
      "num_iter : 833, loss = 0.03726934389547441\n",
      "num_iter : 834, loss = 0.037268299366840195\n",
      "num_iter : 835, loss = 0.0372672579688343\n",
      "num_iter : 836, loss = 0.03726621964599338\n",
      "num_iter : 837, loss = 0.037265184343856686\n",
      "num_iter : 838, loss = 0.037264152008948136\n",
      "num_iter : 839, loss = 0.03726312258875846\n",
      "num_iter : 840, loss = 0.03726209603172772\n",
      "num_iter : 841, loss = 0.037261072287228074\n",
      "num_iter : 842, loss = 0.037260051305547055\n",
      "num_iter : 843, loss = 0.03725903303787089\n",
      "num_iter : 844, loss = 0.037258017436268324\n",
      "num_iter : 845, loss = 0.03725700445367472\n",
      "num_iter : 846, loss = 0.03725599404387624\n",
      "num_iter : 847, loss = 0.03725498616149455\n",
      "num_iter : 848, loss = 0.037253980761971735\n",
      "num_iter : 849, loss = 0.03725297780155542\n",
      "num_iter : 850, loss = 0.037251977237284205\n",
      "num_iter : 851, loss = 0.037250979026973455\n",
      "num_iter : 852, loss = 0.037249983129201025\n",
      "num_iter : 853, loss = 0.03724898950329384\n",
      "num_iter : 854, loss = 0.03724799810931396\n",
      "num_iter : 855, loss = 0.03724700890804552\n",
      "num_iter : 856, loss = 0.03724602186098172\n",
      "num_iter : 857, loss = 0.03724503693031178\n",
      "num_iter : 858, loss = 0.037244054078908496\n",
      "num_iter : 859, loss = 0.03724307327031595\n",
      "num_iter : 860, loss = 0.03724209446873721\n",
      "num_iter : 861, loss = 0.03724111763902256\n",
      "num_iter : 862, loss = 0.03724014274665767\n",
      "num_iter : 863, loss = 0.037239169757752245\n",
      "num_iter : 864, loss = 0.03723819863902854\n",
      "num_iter : 865, loss = 0.03723722935781057\n",
      "num_iter : 866, loss = 0.037236261882012976\n",
      "num_iter : 867, loss = 0.03723529618013046\n",
      "num_iter : 868, loss = 0.037234332221227275\n",
      "num_iter : 869, loss = 0.037233369974926954\n",
      "num_iter : 870, loss = 0.037232409411402156\n",
      "num_iter : 871, loss = 0.03723145050136477\n",
      "num_iter : 872, loss = 0.03723049321605614\n",
      "num_iter : 873, loss = 0.03722953752723751\n",
      "num_iter : 874, loss = 0.0372285834071806\n",
      "num_iter : 875, loss = 0.037227630828658445\n",
      "num_iter : 876, loss = 0.03722667976493624\n",
      "num_iter : 877, loss = 0.03722573018976247\n",
      "num_iter : 878, loss = 0.037224782077360186\n",
      "num_iter : 879, loss = 0.037223835402418484\n",
      "num_iter : 880, loss = 0.03722289014008395\n",
      "num_iter : 881, loss = 0.03722194626595249\n",
      "num_iter : 882, loss = 0.03722100375606113\n",
      "num_iter : 883, loss = 0.037220062586880105\n",
      "num_iter : 884, loss = 0.03721912273530502\n",
      "num_iter : 885, loss = 0.037218184178649125\n",
      "num_iter : 886, loss = 0.037217246894635785\n",
      "num_iter : 887, loss = 0.037216310861391065\n",
      "num_iter : 888, loss = 0.037215376057436404\n",
      "num_iter : 889, loss = 0.03721444246168157\n",
      "num_iter : 890, loss = 0.03721351005341751\n",
      "num_iter : 891, loss = 0.03721257881230959\n",
      "num_iter : 892, loss = 0.0372116487183907\n",
      "num_iter : 893, loss = 0.03721071975205469\n",
      "num_iter : 894, loss = 0.03720979189404984\n",
      "num_iter : 895, loss = 0.037208865125472376\n",
      "num_iter : 896, loss = 0.0372079394277603\n",
      "num_iter : 897, loss = 0.037207014782687106\n",
      "num_iter : 898, loss = 0.03720609117235579\n",
      "num_iter : 899, loss = 0.037205168579192816\n",
      "num_iter : 900, loss = 0.037204246985942324\n",
      "num_iter : 901, loss = 0.037203326375660345\n",
      "num_iter : 902, loss = 0.037202406731709256\n",
      "num_iter : 903, loss = 0.037201488037752105\n",
      "num_iter : 904, loss = 0.037200570277747244\n",
      "num_iter : 905, loss = 0.03719965343594306\n",
      "num_iter : 906, loss = 0.03719873749687255\n",
      "num_iter : 907, loss = 0.03719782244534836\n",
      "num_iter : 908, loss = 0.03719690826645763\n",
      "num_iter : 909, loss = 0.037195994945557025\n",
      "num_iter : 910, loss = 0.03719508246826796\n",
      "num_iter : 911, loss = 0.03719417082047169\n",
      "num_iter : 912, loss = 0.037193259988304646\n",
      "num_iter : 913, loss = 0.03719234995815392\n",
      "num_iter : 914, loss = 0.03719144071665258\n",
      "num_iter : 915, loss = 0.03719053225067526\n",
      "num_iter : 916, loss = 0.03718962454733393\n",
      "num_iter : 917, loss = 0.037188717593973505\n",
      "num_iter : 918, loss = 0.0371878113781675\n",
      "num_iter : 919, loss = 0.03718690588771413\n",
      "num_iter : 920, loss = 0.03718600111063215\n",
      "num_iter : 921, loss = 0.03718509703515686\n",
      "num_iter : 922, loss = 0.03718419364973609\n",
      "num_iter : 923, loss = 0.0371832909430266\n",
      "num_iter : 924, loss = 0.03718238890389004\n",
      "num_iter : 925, loss = 0.03718148752138954\n",
      "num_iter : 926, loss = 0.037180586784785676\n",
      "num_iter : 927, loss = 0.03717968668353325\n",
      "num_iter : 928, loss = 0.0371787872072776\n",
      "num_iter : 929, loss = 0.03717788834585127\n",
      "num_iter : 930, loss = 0.037176990089270494\n",
      "num_iter : 931, loss = 0.037176092427731934\n",
      "num_iter : 932, loss = 0.03717519535160944\n",
      "num_iter : 933, loss = 0.03717429885145087\n",
      "num_iter : 934, loss = 0.037173402917974795\n",
      "num_iter : 935, loss = 0.03717250754206761\n",
      "num_iter : 936, loss = 0.03717161271478037\n",
      "num_iter : 937, loss = 0.037170718427325895\n",
      "num_iter : 938, loss = 0.037169824671075734\n",
      "num_iter : 939, loss = 0.03716893143755739\n",
      "num_iter : 940, loss = 0.0371680387184515\n",
      "num_iter : 941, loss = 0.03716714650558901\n",
      "num_iter : 942, loss = 0.037166254790948504\n",
      "num_iter : 943, loss = 0.037165363566653495\n",
      "num_iter : 944, loss = 0.03716447282496982\n",
      "num_iter : 945, loss = 0.037163582558303127\n",
      "num_iter : 946, loss = 0.037162692759196245\n",
      "num_iter : 947, loss = 0.03716180342032677\n",
      "num_iter : 948, loss = 0.03716091453450457\n",
      "num_iter : 949, loss = 0.03716002609466951\n",
      "num_iter : 950, loss = 0.037159138093888995\n",
      "num_iter : 951, loss = 0.03715825052535561\n",
      "num_iter : 952, loss = 0.0371573633823851\n",
      "num_iter : 953, loss = 0.037156476658413845\n",
      "num_iter : 954, loss = 0.03715559034699687\n",
      "num_iter : 955, loss = 0.0371547044418057\n",
      "num_iter : 956, loss = 0.03715381893662611\n",
      "num_iter : 957, loss = 0.03715293382535619\n",
      "num_iter : 958, loss = 0.03715204910200428\n",
      "num_iter : 959, loss = 0.037151164760686986\n",
      "num_iter : 960, loss = 0.03715028079562715\n",
      "num_iter : 961, loss = 0.0371493972011521\n",
      "num_iter : 962, loss = 0.037148513971691485\n",
      "num_iter : 963, loss = 0.03714763110177575\n",
      "num_iter : 964, loss = 0.037146748586034034\n",
      "num_iter : 965, loss = 0.03714586641919255\n",
      "num_iter : 966, loss = 0.03714498459607285\n",
      "num_iter : 967, loss = 0.03714410311158987\n",
      "num_iter : 968, loss = 0.037143221960750566\n",
      "num_iter : 969, loss = 0.03714234113865201\n",
      "num_iter : 970, loss = 0.03714146064047984\n",
      "num_iter : 971, loss = 0.03714058046150668\n",
      "num_iter : 972, loss = 0.03713970059709055\n",
      "num_iter : 973, loss = 0.03713882104267332\n",
      "num_iter : 974, loss = 0.037137941793779175\n",
      "num_iter : 975, loss = 0.03713706284601314\n",
      "num_iter : 976, loss = 0.03713618419505966\n",
      "num_iter : 977, loss = 0.03713530583668107\n",
      "num_iter : 978, loss = 0.03713442776671628\n",
      "num_iter : 979, loss = 0.03713354998107938\n",
      "num_iter : 980, loss = 0.0371326724757582\n",
      "num_iter : 981, loss = 0.037131795246813044\n",
      "num_iter : 982, loss = 0.0371309182903754\n",
      "num_iter : 983, loss = 0.03713004160264657\n",
      "num_iter : 984, loss = 0.03712916517989649\n",
      "num_iter : 985, loss = 0.03712828901846247\n",
      "num_iter : 986, loss = 0.03712741311474793\n",
      "num_iter : 987, loss = 0.03712653746522126\n",
      "num_iter : 988, loss = 0.03712566206641458\n",
      "num_iter : 989, loss = 0.037124786914922665\n",
      "num_iter : 990, loss = 0.03712391200740181\n",
      "num_iter : 991, loss = 0.037123037340568626\n",
      "num_iter : 992, loss = 0.03712216291119904\n",
      "num_iter : 993, loss = 0.03712128871612715\n",
      "num_iter : 994, loss = 0.0371204147522443\n",
      "num_iter : 995, loss = 0.037119541016497865\n",
      "num_iter : 996, loss = 0.03711866750589037\n",
      "num_iter : 997, loss = 0.037117794217478496\n",
      "num_iter : 998, loss = 0.037116921148372\n",
      "num_iter : 999, loss = 0.0371160482957328\n"
     ]
    }
   ],
   "source": [
    "lr1 = ScratchLinearRegression(num_iter=1000, lr=0.01, random_state=42, no_bias=False, verbose=True)\n",
    "lr1.fit(X_train1, y_train1, X_test1, y_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-07-06T22:11:32.902327Z",
     "start_time": "2020-07-06T22:11:32.894541Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[210559.86649422 248639.43041109 118173.44345919 157378.55538966\n",
      " 208233.17083617  73699.59789074 260423.39238878 216663.45493251\n",
      " 102390.51415391 104176.72155135  83252.91985865 250115.2205128\n",
      " 271963.19802224  93385.72551275  77147.76543218 131294.70548297\n",
      " 106989.47551592 131012.76358512 109828.02542215 263504.92474829\n",
      " 186808.10519115 253496.89385857 157694.50241294 149771.82750138\n",
      "  77500.75108501 250913.88098224 116876.04578238 288906.13825578\n",
      " 110792.5567063   72137.34867131 140539.60955143 270227.90338157\n",
      " 308443.1350917  228770.161241   177221.49762943 293215.87073398\n",
      " 106296.69277156 164753.81152984  70637.21527387  91523.32941207\n",
      " 196386.09068718 246993.28147546 421394.88531676 146770.68617825\n",
      " 190651.07645506 156468.88499719 107227.63488285  75233.86774198\n",
      "  94028.65390889 160608.39121447  92695.83321265 250181.66011964\n",
      "  87553.10864247  96016.24283219 118785.88336118 146437.72631056\n",
      " 131177.40911723 135306.37556999 229068.76704138 290887.91368044\n",
      " 220044.16822167 110733.71916809 138875.8793215  260190.73473996\n",
      " 138963.14589983 122335.53602327  87909.14521265  75459.53079947\n",
      " 246730.90659708 105564.9001755   45471.77826748 174069.22681452\n",
      " 122220.31704918 366864.87538819 102675.24911596 199918.39880481\n",
      " 113655.02384287 178217.40407015 109971.38015538 299985.90060284\n",
      " 236976.20622135  86900.00325583 259807.63309263  79140.90725486\n",
      "  84118.04418133 257402.74577145  59614.16880944 265870.52349396\n",
      " 109436.07739743 178152.87555673  95507.38845382 151478.18870038\n",
      " 116768.8762705  222551.14725093 175483.44804472 227755.66104471\n",
      " 256725.93300711  78338.29753038 224353.86821107 111388.29876144\n",
      " 142808.36250229 158255.41767124 237853.61834438 114650.3598072\n",
      " 142223.20907327  82225.64518461 239681.85177999 293045.94534151\n",
      " 123645.67195648 260731.76211041 127036.34989357 197566.14557094\n",
      " 218460.66142392  82897.70504196 229800.22260272 242570.61311528\n",
      "  51494.01344689  81728.6234863  124223.39864081 220240.92756772\n",
      " 381413.13798267 103501.9711157  403709.47412788 161152.35419302\n",
      " 135718.80617947 274669.89512012 224451.50496154 143212.50110277\n",
      " 217728.2974924  311287.3537458  138929.45856856 225816.47678444\n",
      " 106741.77281187 103392.0235215  143562.39605315 283527.67161449\n",
      " 158787.59625984 214958.77130456 125805.69920577 315740.14260053\n",
      " 122738.07670268 138470.54900788 187690.05521843 138627.85165828\n",
      " 159834.35508155 242248.69226448 206899.54237995 139188.24053139\n",
      " 260190.73473996 150828.31950309 131599.55776179 230651.06541264\n",
      " 139838.58889722 329309.21434495 202587.2245265  275946.76360926\n",
      " 140889.62092859 197308.51921489  87333.46439551  81823.43315774\n",
      " 129662.17164939 132771.3560424  190950.55316167  64404.81172495\n",
      " 220331.19168048 180644.58262408 126543.29282275 138066.46205804\n",
      " 125678.15286044 223547.73805401 126607.48871209 145777.82821295\n",
      " 275274.33748017 134615.28610569 157067.22102677 219783.82400836\n",
      " 104524.53386371 312145.77724509 232081.71912995 354332.18135718\n",
      " 309009.83816135  78255.08068673 122838.91866278 228880.7179791\n",
      "  81248.34064043 232216.39535056  76136.86411692 172929.39466169\n",
      " 151156.5365362  182499.66944482 270580.62254274  91802.30386816\n",
      " 163908.30683911 215114.61338275 252915.96228647 120081.18836907\n",
      "  92545.92590479 200343.7257603  133643.28290914  99510.87046445\n",
      "  83557.04170575 225805.52727382 223019.36452311 301358.33902267\n",
      " 253087.05037698 153696.95691637 315358.99914287 172929.39466169\n",
      "  79955.61699384  87019.66486118 119777.01082822 278155.14761058\n",
      " 217444.56976017 108945.53881731  90402.77125982 286508.62197455\n",
      " 262228.68856275 244264.10176395 256310.75691839  78309.93299488\n",
      " 320480.96746129 185401.07770947 133008.84735185 129265.04051117\n",
      " 273438.70899251 169554.18937378 489970.46432745 174684.18947495\n",
      " 248909.81513761 268587.79389868 228339.53990553 242910.78837594\n",
      " 103973.13185739 267972.39606792 173905.28362691 244506.12217351\n",
      " 145141.88911185  98813.8798403  124454.62464191 287208.49064153\n",
      " 151928.8953216  136204.88644902 240162.81773747 179865.1054683\n",
      " 103055.49765698 136501.28147531 198974.12051    221435.72739806\n",
      " 230100.17290667  94559.36657469 131570.93164316  77983.33672651\n",
      " 132643.06338646  69602.32299071 104136.40842556 209302.10914071\n",
      " 283979.98687873  90064.99819432 112605.59278657 147164.69275677\n",
      "  87122.74372194 308927.64079706 225631.09761968 132736.03854058\n",
      " 354828.93329606 242108.30353311 240215.07038344 137397.7633039\n",
      " 109486.3334987  227233.62059986 355343.61174255 199329.94316958\n",
      " 287021.23227002 106191.43030458 287465.30705823 321885.29531917\n",
      " 172929.39466169 234074.80805897 166679.13134381 128368.8991125\n",
      " 238791.95817315 255297.44253785 242277.90162675 330888.07263471\n",
      " 148335.31414722 121056.83799577 162540.21575667 208409.35415185\n",
      " 207505.17770819 267029.01923398  83540.9827051  255889.88039221]\n"
     ]
    }
   ],
   "source": [
    "y_pred1_log = lr1.predict(X_test1)\n",
    "y_pred1 = np.exp(y_pred1_log)\n",
    "print(y_pred1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse1 = 38679524556.663124\n",
      "mse2 = 38679524556.663124\n"
     ]
    }
   ],
   "source": [
    "# 自作関数でMSEを計算\n",
    "mse1 = MSE(y_test1, y_pred1)\n",
    "print('mse1 = {}'.format(mse1))\n",
    "\n",
    "# scikit-learnライブラリによりMSEを計算\n",
    "mse2 = mean_squared_error(y_test1, y_pred1)\n",
    "print('mse2 = {}'.format(mse2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[210599.51095729 248519.69001308 118817.45400035 158101.10793161\n",
      " 208418.47432237  74393.88228346 260199.32647492 216663.54113432\n",
      " 103056.64359112 104902.72107463  83921.13521099 250001.36891919\n",
      " 271659.3600313   94303.79905708  77841.0192821  131952.31212059\n",
      " 107960.84817722 131648.91437817 110488.32804274 263320.68766419\n",
      " 187231.39901976 253488.96982891 158185.78364506 150323.47080071\n",
      "  78237.82177669 250985.95087464 117690.84714496 288735.95939678\n",
      " 111502.34475255  72832.09965781 141007.69259803 269948.69639933\n",
      " 308100.35664158 228724.40560498 177589.99904159 292832.66990794\n",
      " 106957.91188607 165268.97001464  71332.25249111  92321.86095025\n",
      " 196753.08648314 246908.72506261 420084.86441223 147261.48493399\n",
      " 190878.89314315 157106.50758789 107907.75318631  75927.06539611\n",
      "  94734.49445851 161086.22735472  93499.02303283 250121.58871444\n",
      "  88335.33700499  96790.3485085  119423.88044381 147111.91424967\n",
      " 131849.2453971  135898.73154511 228996.77404001 290502.87848728\n",
      " 220135.46754126 111395.1843299  139474.40674999 259996.08720155\n",
      " 139516.343934   122922.43335596  88644.8989242   76186.70702121\n",
      " 246661.01136938 106213.50915516  46131.12743512 174521.34658941\n",
      " 122867.01368445 366179.54952538 103515.35050741 200101.99593374\n",
      " 114221.08938001 178621.94182489 110619.89919238 299545.63502894\n",
      " 236915.35912019  87584.6502367  259629.10863394  79885.74476559\n",
      "  84857.93540516 257181.41071341  60255.09540914 265630.40116122\n",
      " 110143.52666672 178489.96147649  96217.30092679 152019.82985892\n",
      " 117456.27688369 222619.63579794 176018.2607642  227690.2080728\n",
      " 256494.46435806  79070.79847074 224463.00554376 112026.33698417\n",
      " 143405.09221027 158780.16469143 237788.43216246 115341.41377564\n",
      " 142688.94516789  82953.23884344 239659.43357616 292647.9278242\n",
      " 124199.05856601 260604.66166848 127637.08068346 197737.14236718\n",
      " 218570.51733689  83637.10806029 229898.63346802 242731.22222102\n",
      "  52182.04131029  82467.5428848  124864.43580855 220307.54771733\n",
      " 380466.6315036  104311.05416737 402513.43826978 161804.23083756\n",
      " 136318.40043948 274498.78056015 224414.49283361 143863.48879264\n",
      " 218065.24720222 310860.77557128 139747.04580568 225736.19114566\n",
      " 107490.85461966 104206.40303985 144041.78346363 283171.03035335\n",
      " 159440.94020967 215129.68557579 126461.0919949  315269.44260339\n",
      " 123325.00871296 139121.90871546 188172.27169791 139256.60752044\n",
      " 160339.5351826  242148.44446194 207045.07745255 139703.48037385\n",
      " 259996.08720155 151321.55564488 132164.27007939 230745.84869324\n",
      " 140451.92015768 328860.25687937 202661.98350211 275634.62492491\n",
      " 141525.57007665 197683.59718066  88067.10848111  82571.69557541\n",
      " 130276.93874887 133328.35010398 191373.9384735   65144.14288566\n",
      " 220803.19513072 180827.81234061 127102.74161425 138642.79140579\n",
      " 126292.47919262 223902.38726847 127187.56060418 146317.44209749\n",
      " 275040.48937152 135163.97547802 157627.69535461 219995.22098227\n",
      " 105308.27234007 311579.12147266 232233.18144884 353651.27825261\n",
      " 308763.98780402  78991.46988682 123425.85839138 228809.66372679\n",
      "  82002.92349275 232166.26111068  77067.22387231 173224.49551965\n",
      " 151714.95198628 182913.38831329 270270.15533494  92606.96010365\n",
      " 164478.16353699 215249.98272123 252691.94421534 120726.54471628\n",
      "  93373.77866346 200688.51387867 134286.97496394 100231.27246397\n",
      "  84413.29306126 225810.79652666 223001.3344159  301349.4965925\n",
      " 252916.80649243 154244.37714748 314975.83864417 173224.49551965\n",
      "  80682.42494879  87847.15227787 120435.19276513 277845.17508445\n",
      " 217605.74810785 109580.95873844  91196.78492916 286227.11746896\n",
      " 262022.9465657  244127.13407254 256150.88336229  79012.37456479\n",
      " 320119.39575116 185817.70675141 133659.69168968 129837.6368545\n",
      " 273096.7534509  170015.49647216 488049.82339764 175125.54055063\n",
      " 248842.52332291 268782.84991178 228308.23524804 242859.7143094\n",
      " 104619.40126789 267691.54030113 174442.66350907 244381.10679522\n",
      " 145650.60771687  99478.67746563 125170.2338928  286845.41809525\n",
      " 152494.775876   136745.14156055 240229.0600558  180275.37006239\n",
      " 103705.97696536 137100.64304985 199225.92369602 221676.91996451\n",
      " 230172.4001911   95225.45781813 132178.55458565  78765.31066683\n",
      " 133351.5391689   70375.74641049 104987.20048315 209744.16942703\n",
      " 283728.08954518  90725.01621657 113385.84229152 147742.85308436\n",
      "  87903.15467996 308899.4164031  225551.74501272 133386.82995392\n",
      " 354163.91315515 241995.67800927 240203.09855605 137959.5087894\n",
      " 110152.14761453 227183.04837939 354560.10970657 199904.71492971\n",
      " 286674.78057645 106945.02327982 287069.64552486 321289.51237999\n",
      " 173224.49551965 234028.2012268  167263.59395035 128976.60888369\n",
      " 238709.20019306 255087.72434223 242282.34388285 330197.25107349\n",
      " 148840.06129572 121650.1651428  163102.89151383 208458.61110876\n",
      " 207569.67478977 266767.98790774  84214.70448775 256203.07120106]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDRegressor\n",
    "lr2 = SGDRegressor(eta0=0.01, max_iter=1000, random_state=42)\n",
    "lr2.fit(X_train1, y_train1)\n",
    "y_pred2_log = lr2.predict(X_test1)\n",
    "y_pred2 = np.exp(y_pred2_log)\n",
    "print(y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse3 = 38722285402.45527\n",
      "mse4 = 38722285402.45527\n"
     ]
    }
   ],
   "source": [
    "# 自作関数でMSEを計算\n",
    "mse3 = MSE(y_test1, y_pred2)\n",
    "print('mse3 = {}'.format(mse3))\n",
    "\n",
    "# scikit-learnライブラリによりMSEを計算\n",
    "mse4 = mean_squared_error(y_test1, y_pred2)\n",
    "print('mse4 = {}'.format(mse4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】学習曲線のプロット"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "学習曲線を表示する関数を作成し、実行してください。  \n",
    "グラフを見て損失が適切に下がっているかどうか確認してください。  \n",
    "線形回帰クラスの雛形ではself.loss, self.val_lossに損失を記録しておくようになっているため、入力にはこれを利用してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.xlabel('iter')\n",
    "plt.ylabel('loss')\n",
    "plt.plot(lr1.loss, label='train_loss')\n",
    "plt.plot(lr1.val_loss, label='val_loss')\n",
    "plt.legend(loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([72.30585734, 70.16113475, 68.09050223, 66.09113233, 64.16031137])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1.loss[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([69.86273475, 67.72648143, 65.664583  , 63.67422088, 61.75268866])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr1.val_loss[:5]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
