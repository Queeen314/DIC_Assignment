{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】ブレンディングのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ブレンディング をスクラッチ実装し、単一モデルより精度があがる例を 最低3つ 示してください。  \n",
    "精度があがるとは、検証用データに対する平均二乗誤差（MSE）が小さくなることを指します。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データ読み込み\n",
    "df = pd.read_csv('train.csv')\n",
    "\n",
    "# 目的変数とクラスに分ける\n",
    "X = df[['GrLivArea', 'YearBuilt']].values\n",
    "y = df['SalePrice'].values\n",
    "\n",
    "# データ分割\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "# 標準化\n",
    "sc= StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_val_std = sc.transform(X_val)\n",
    "\n",
    "# 対数変換\n",
    "y_train_log = np.log(y_train)\n",
    "y_valid_log = np.log(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "weak_models = []\n",
    "df_mse = pd.DataFrame(columns=[\"Model\", \"MSE\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例1**  \n",
    "- 標準化\n",
    "- 対数変換なし\n",
    "- 線形回帰 + SVM + 決定木"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "MSE = 2.94e+09\n",
      "SVM\n",
      "MSE = 7.24e+09\n",
      "Tree\n",
      "MSE = 2.66e+09\n",
      "Blending by Mean\n",
      "MSE = 2.84e+09\n",
      "Blending by Weighted Mean\n",
      "MSE = 2.48e+09\n"
     ]
    }
   ],
   "source": [
    "# 線形回帰\n",
    "print(\"Linear Regression\")\n",
    "weak1 = LinearRegression()\n",
    "weak1.fit(X_train_std, y_train)\n",
    "y_pred1 = weak1.predict(X_val_std)\n",
    "mse1 = mean_squared_error(y_pred1, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse1))\n",
    "\n",
    "# SVM\n",
    "print(\"SVM\")\n",
    "C = 0.1\n",
    "k = \"rbf\"\n",
    "weak2 = SVR(kernel=k, C=C)\n",
    "weak2.fit(X_train_std, y_train)\n",
    "y_pred2 = weak2.predict(X_val_std)\n",
    "mse2 = mean_squared_error(y_pred2, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse2))\n",
    "\n",
    "# 決定木\n",
    "print(\"Tree\")\n",
    "max_depth = 10\n",
    "weak3 = DecisionTreeRegressor(max_depth=max_depth)\n",
    "weak3.fit(X_train_std, y_train)\n",
    "y_pred3 = weak3.predict(X_val_std)\n",
    "mse3 = mean_squared_error(y_pred3, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse3))\n",
    "\n",
    "# 平均で Blending\n",
    "print(\"Blending by Mean\")\n",
    "y_blend = (y_pred1 + y_pred2 + y_pred3)/3\n",
    "mse_blend = mean_squared_error(y_blend, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse_blend))\n",
    "\n",
    "# Blending：MSEの逆数で加重平均\n",
    "print(\"Blending by Weighted Mean\")\n",
    "reciprocal_mse = 1/mse1 + 1/mse2 + 1/mse3\n",
    "y_blend = (1/mse1*y_pred1 + 1/mse2*y_pred2 + 1/mse3*y_pred3)/reciprocal_mse\n",
    "mse_blend = mean_squared_error(y_blend, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse_blend))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例2**  \n",
    "- 標準化  \n",
    "- 対数変換  \n",
    "- 線形回帰 + SVM + 決定木"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Linear Regression\n",
      "MSE = 8.65e+09\n",
      "SVM\n",
      "MSE = 2.7e+09\n",
      "Tree\n",
      "MSE = 2.53e+09\n",
      "Blending by Mean\n",
      "MSE = 2.9e+09\n",
      "Blending by Weighted Mean\n",
      "MSE = 2.26e+09\n"
     ]
    }
   ],
   "source": [
    "# 線形回帰\n",
    "print(\"Linear Regression\")\n",
    "weak1 = LinearRegression()\n",
    "weak1.fit(X_train_std, y_train_log)\n",
    "y_pred1_log = weak1.predict(X_val_std)\n",
    "y_pred1 = np.exp(y_pred1_log)\n",
    "mse1 = mean_squared_error(y_pred1, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse1))\n",
    "\n",
    "# SVM\n",
    "print(\"SVM\")\n",
    "C = 0.1\n",
    "k = \"rbf\"\n",
    "weak2 = SVR(kernel=k, C=C)\n",
    "weak2.fit(X_train_std, y_train_log)\n",
    "y_pred2_log = weak2.predict(X_val_std)\n",
    "y_pred2 = np.exp(y_pred2_log)\n",
    "mse2 = mean_squared_error(y_pred2, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse2))\n",
    "\n",
    "# 決定木\n",
    "print(\"Tree\")\n",
    "max_depth = 10\n",
    "weak3 = DecisionTreeRegressor(max_depth=max_depth)\n",
    "weak3.fit(X_train_std, y_train_log)\n",
    "y_pred3_log = weak3.predict(X_val_std)\n",
    "y_pred3 = np.exp(y_pred3_log)\n",
    "mse3 = mean_squared_error(y_pred3, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse3))\n",
    "\n",
    "# Blending：平均\n",
    "print(\"Blending by Mean\")\n",
    "y_blend = (y_pred1 + y_pred2 + y_pred3)/3\n",
    "mse_blend = mean_squared_error(y_blend, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse_blend))\n",
    "\n",
    "# Blending：MSEの逆数で加重平均\n",
    "print(\"Blending by Weighted Mean\")\n",
    "reciprocal_mse = 1/mse1 + 1/mse2 + 1/mse3\n",
    "y_blend = (1/mse1*y_pred1 + 1/mse2*y_pred2 + 1/mse3*y_pred3)/reciprocal_mse\n",
    "mse_blend = mean_squared_error(y_blend, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse_blend))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**例3**  \n",
    "- 標準化なし\n",
    "- 対数変換なし\n",
    "- 深さの違う決定木を10個"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tree 1\n",
      "MSE = 4.95e+09\n",
      "Tree 2\n",
      "MSE = 3.71e+09\n",
      "Tree 3\n",
      "MSE = 2.7e+09\n",
      "Tree 4\n",
      "MSE = 2.69e+09\n",
      "Tree 5\n",
      "MSE = 2.17e+09\n",
      "Tree 6\n",
      "MSE = 2.19e+09\n",
      "Tree 7\n",
      "MSE = 2e+09\n",
      "Tree 8\n",
      "MSE = 2.09e+09\n",
      "Tree 9\n",
      "MSE = 2.27e+09\n",
      "Tree 10\n",
      "MSE = 2.49e+09\n",
      "Blending by Mean\n",
      "MSE = 2.04e+09\n",
      "Blending by Weighted Mean\n",
      "MSE = 1.96e+09\n"
     ]
    }
   ],
   "source": [
    "# 決定木10個\n",
    "n = 10\n",
    "mses = np.zeros(n)\n",
    "y_preds = np.zeros((y_val.shape[0], n))\n",
    "\n",
    "for i in range(1, n+1):\n",
    "    print(\"Tree\", i)\n",
    "    max_depth = i\n",
    "    weak = DecisionTreeRegressor(max_depth=max_depth)\n",
    "    weak.fit(X_train, y_train)\n",
    "    y_pred = weak.predict(X_val)\n",
    "    mse = mean_squared_error(y_pred, y_val)\n",
    "    print(\"MSE = {:.3}\".format(mse))    \n",
    "    y_preds[:, i-1] = y_pred\n",
    "    mses[i-1] = mse\n",
    "\n",
    "# Blending：平均\n",
    "print(\"Blending by Mean\")\n",
    "y_blend = np.mean(y_preds, axis=1)\n",
    "mse_blend = mean_squared_error(y_blend, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse_blend))\n",
    "\n",
    "# Blending：MSEの逆数で加重平均\n",
    "print(\"Blending by Weighted Mean\")\n",
    "reciprocal_mse = np.reciprocal(mses).reshape(1, -1)\n",
    "y_blend = np.sum((reciprocal_mse * y_preds) / np.sum(reciprocal_mse), axis=1)\n",
    "mse_blend = mean_squared_error(y_blend, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse_blend))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】バギングのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "バギング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====n_subsets:  1 =====\n",
      "MSE = 2.42e+09\n",
      "Bagging:\n",
      "MSE: 2.42e+09\n",
      "=====n_subsets:  2 =====\n",
      "MSE = 2.42e+09\n",
      "MSE = 2.22e+09\n",
      "Bagging:\n",
      "MSE: 2.16e+09\n",
      "=====n_subsets:  3 =====\n",
      "MSE = 2.37e+09\n",
      "MSE = 2.26e+09\n",
      "MSE = 3.13e+09\n",
      "Bagging:\n",
      "MSE: 2.1e+09\n",
      "=====n_subsets:  4 =====\n",
      "MSE = 2.41e+09\n",
      "MSE = 2.23e+09\n",
      "MSE = 3.13e+09\n",
      "MSE = 2.04e+09\n",
      "Bagging:\n",
      "MSE: 2e+09\n",
      "=====n_subsets:  5 =====\n",
      "MSE = 2.38e+09\n",
      "MSE = 2.23e+09\n",
      "MSE = 3.13e+09\n",
      "MSE = 2.04e+09\n",
      "MSE = 2.06e+09\n",
      "Bagging:\n",
      "MSE: 1.88e+09\n",
      "=====n_subsets:  6 =====\n",
      "MSE = 2.36e+09\n",
      "MSE = 2.23e+09\n",
      "MSE = 3.12e+09\n",
      "MSE = 2.03e+09\n",
      "MSE = 2.05e+09\n",
      "MSE = 2.05e+09\n",
      "Bagging:\n",
      "MSE: 1.8e+09\n",
      "=====n_subsets:  7 =====\n",
      "MSE = 2.37e+09\n",
      "MSE = 2.24e+09\n",
      "MSE = 3.13e+09\n",
      "MSE = 2.03e+09\n",
      "MSE = 2e+09\n",
      "MSE = 1.91e+09\n",
      "MSE = 2.19e+09\n",
      "Bagging:\n",
      "MSE: 1.7e+09\n",
      "=====n_subsets:  8 =====\n",
      "MSE = 2.36e+09\n",
      "MSE = 2.22e+09\n",
      "MSE = 3.12e+09\n",
      "MSE = 2.08e+09\n",
      "MSE = 2.05e+09\n",
      "MSE = 1.84e+09\n",
      "MSE = 2.13e+09\n",
      "MSE = 2.01e+09\n",
      "Bagging:\n",
      "MSE: 1.66e+09\n",
      "=====n_subsets:  9 =====\n",
      "MSE = 2.41e+09\n",
      "MSE = 2.24e+09\n",
      "MSE = 3.13e+09\n",
      "MSE = 2.05e+09\n",
      "MSE = 2e+09\n",
      "MSE = 1.86e+09\n",
      "MSE = 2.15e+09\n",
      "MSE = 2.01e+09\n",
      "MSE = 2.03e+09\n",
      "Bagging:\n",
      "MSE: 1.64e+09\n",
      "=====n_subsets:  10 =====\n",
      "MSE = 2.36e+09\n",
      "MSE = 2.25e+09\n",
      "MSE = 3.13e+09\n",
      "MSE = 2.09e+09\n",
      "MSE = 2.01e+09\n",
      "MSE = 1.93e+09\n",
      "MSE = 2.19e+09\n",
      "MSE = 1.95e+09\n",
      "MSE = 2.03e+09\n",
      "MSE = 3.35e+09\n",
      "Bagging:\n",
      "MSE: 1.71e+09\n"
     ]
    }
   ],
   "source": [
    "def bagging_tree(n_subsets):\n",
    "    print(\"=====n_subsets: \", n_subsets, \"=====\")\n",
    "    mses = np.zeros(n_subsets)\n",
    "    y_preds = np.zeros((y_val.shape[0], n_subsets))\n",
    "\n",
    "    for subset in range(n_subsets):\n",
    "        X_part, _, y_part, _ = train_test_split(X_train_std, y_train_log, train_size=0.8,\n",
    "                                                shuffle=True, random_state=subset)\n",
    "        tree = DecisionTreeRegressor(max_depth=8)\n",
    "        tree.fit(X_part, y_part)\n",
    "        y_pred_log = tree.predict(X_val_std)\n",
    "        y_pred = np.exp(y_pred_log)\n",
    "        mse = mean_squared_error(y_pred, y_val)\n",
    "        print(\"MSE = {:.3}\".format(mse))\n",
    "        y_preds[:, subset-1] = y_pred\n",
    "        mses[subset-1] = mse\n",
    "\n",
    "    print(\"Bagging:\")\n",
    "    y_bagging = np.mean(y_preds, axis=1)\n",
    "    mse_bagging = mean_squared_error(y_bagging, y_val)\n",
    "    print(\"MSE: {:.3}\".format(mse_bagging))\n",
    "    mse_min = np.min(mses)\n",
    "    return mse_bagging, mse_min\n",
    "\n",
    "n_bag = 10\n",
    "bag_mse = np.zeros((n_bag, 2))\n",
    "for i in range(1, n_bag+1):\n",
    "    bag_mse[i-1, :] = bagging_tree(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】スタッキングのスクラッチ実装"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "スタッキング をスクラッチ実装し、単一モデルより精度があがる例を 最低1つ 示してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Stage():\n",
    "    def __init__(self, n_folds, n_models, regressor):\n",
    "        self.n_folds = n_folds  # k\n",
    "        self.n_models = n_models  # m\n",
    "        # regressor（インスタンス）を　（k行 m列） だけ複製する\n",
    "        self.model = [[regressor for m in range(n_models)] for k in range(n_folds)]\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"スタッキングでアンサンブル学習\n",
    "        :parameters\n",
    "            X (2d-ndarray, (n_samples, n_features)): 訓練データ\n",
    "            y (1d-ndarray, (n_samples, )): 正解値\n",
    "        :returns\n",
    "            next_X (2d-ndarray, (n_samples, n_models)): ブレンドデータ（次のステージの学習に使う）\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        \n",
    "        # blendデータを格納する変数\n",
    "        next_X = np.zeros((n_samples, self.n_models))\n",
    "        \n",
    "        # mの方向にループ\n",
    "        for m in range(self.n_models):\n",
    "            # m番目のモデルを使って、学習\n",
    "            next_X[:, m] = self.fit_each_model(X, y, m)\n",
    "        return next_X\n",
    "    \n",
    "    def fit_each_model(self, X, y, m):\n",
    "        \"\"\"１つのモデルで、K分割しながら学習\n",
    "        :parameters\n",
    "            X (2d-ndarray, (n_samples, n_features)): 訓練データ\n",
    "            y (1d-ndarray, (n_sampels,)): 正解値\n",
    "            m (int): 何番目のモデルかというインデックス\n",
    "        :returns\n",
    "            y_blend (1d-ndarray, (n_samples,)): K分割それぞれをブレンドした予測値\n",
    "        \"\"\"\n",
    "        n_samples = X.shape[0]\n",
    "        # ランダムにインデックスを用意\n",
    "        random_index = np.random.permutation(np.arange(n_samples))\n",
    "        \n",
    "        # k分割した後のサイズを決定（データ数がkで割り切れない分はそれぞれに割り振る）\n",
    "        fold_sizes = np.full(self.n_folds, X.shape[0] // self.n_folds, dtype=np.int)\n",
    "        fold_sizes[:n_samples % self.n_folds] += 1\n",
    "        \n",
    "        # ブレンドした予測値を格納する配列\n",
    "        y_blend = np.zeros(n_samples)\n",
    "        \n",
    "        # k分割したもの、それぞれで学習していく\n",
    "        current = 0\n",
    "        k = 0\n",
    "        for fold_size in fold_sizes:\n",
    "            # indexのはじめと終わり\n",
    "            start, stop = current, current + fold_size\n",
    "            \n",
    "            # 予測に使うデータのindex\n",
    "            index = random_index[start:stop]\n",
    "            \n",
    "            # 学習に使うデータのindex\n",
    "            other_index = self.set_others(random_index, start, stop)\n",
    "            \n",
    "            # k行m列のモデルに学習させる\n",
    "            self.model[k][m].fit(X[other_index], y[other_index])\n",
    "            \n",
    "            # 予測\n",
    "            y_blend[index] = self.model[k][m].predict(X[index])\n",
    "            \n",
    "            # 次のインデックス\n",
    "            current = stop\n",
    "            k += 1\n",
    "        return y_blend\n",
    "    \n",
    "    def set_others(self, array, start, stop):\n",
    "        \"\"\"指定インデックスの初めと終わりに対して、指定されていないインデックスを返す関数\n",
    "        :parameters\n",
    "            array (1d-ndarray, (n_samples)): 乱数配列（インデックス）\n",
    "            start (int): 始まり\n",
    "            stop (int): 終わり\n",
    "        :returns\n",
    "            others (1d-ndarray, (??)): 乱数配列（start ~ stop以外のインデックス）\n",
    "        \"\"\"\n",
    "        if start == 0:\n",
    "            others = array[stop:]\n",
    "        elif stop == len(array):\n",
    "            others = array[:start]\n",
    "        else:\n",
    "            others = np.concatenate([array[:start], array[stop:]])\n",
    "        return others\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"予測\n",
    "        :parameters\n",
    "            X (2d-ndarray, (n_samples, n_features)): 本番データ\n",
    "        :returns\n",
    "            y_pred_mean (1d-ndarray, (n_samples,)): このステージでの予測値\n",
    "        \"\"\"\n",
    "        # k行 m列のモデルで予測した結果をデータ数分だけ格納する三次元配列: y_pred （n_samples, k, m)\n",
    "        y_pred = np.zeros((X.shape[0], self.n_folds, self.n_models))\n",
    "        \n",
    "        # k行 m列のモデルで、予測していく\n",
    "        for m in range(self.n_models):\n",
    "            for k in range(self.n_folds):\n",
    "                y_pred[:, k, m] = self.model[k][m].predict(X)\n",
    "        \n",
    "        # k軸方向に平均をとる >>> 結果は （n_samples, m） の二次元配列になる\n",
    "        y_pred_mean = np.mean(y_pred, axis=1)\n",
    "        return y_pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# モデル定義\n",
    "k0, m0, reg0 = 5, 10, DecisionTreeRegressor(max_depth=5)\n",
    "k1, m1, reg1 = 2, 1, LinearRegression()\n",
    "stage0 = Stage(k0, m0, reg0)\n",
    "stage1 = Stage(k1, m1, reg1)\n",
    "\n",
    "# 学習\n",
    "X_blend0 = stage0.fit(X_train_std, y_train_log)\n",
    "X_blend1 = stage1.fit(X_blend0, y_train_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE = 2.5e+09\n"
     ]
    }
   ],
   "source": [
    "# 推定\n",
    "y_pred0_log = stage0.predict(X_val_std)\n",
    "y_pred1_log = stage1.predict(y_pred0_log)\n",
    "\n",
    "# 評価\n",
    "y_pred = np.exp(y_pred1_log)\n",
    "mse_stack = mean_squared_error(y_pred, y_val)\n",
    "print(\"MSE = {:.3}\".format(mse_stack))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train : (1168, 2)\n",
      "X_blend0: (1168, 10)\n",
      "X_blend1: (1168, 8)\n",
      "X_blend2: (1168, 2)\n",
      "X_blend3: (1168, 1)\n",
      "\n",
      "X_val : (292, 2)\n",
      "y_pred0 : (292, 10)\n",
      "y_pred1 : (292, 8)\n",
      "y_pred2 : (292, 2)\n",
      "y_pred3 : (292, 1)\n",
      "\n",
      "MSE = 2.77e+09\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# モデル定義\n",
    "k0, m0, reg0 = 5, 10, RandomForestRegressor(n_estimators=10, max_depth=5)\n",
    "k1, m1, reg1 = 4, 8, SVR(kernel=\"rbf\", C=1, gamma=\"scale\")\n",
    "k2, m2, reg2 = 3, 2, DecisionTreeRegressor(max_depth=5)\n",
    "k3, m3, reg3 = 2, 1, LinearRegression()\n",
    "stage0 = Stage(k0, m0, reg0)\n",
    "stage1 = Stage(k1, m1, reg1)\n",
    "stage2 = Stage(k2, m2, reg2)\n",
    "stage3 = Stage(k3, m3, reg3)\n",
    "\n",
    "# 学習\n",
    "X_blend0 = stage0.fit(X_train_std, y_train_log)\n",
    "X_blend1 = stage1.fit(X_blend0, y_train_log)\n",
    "X_blend2 = stage2.fit(X_blend1, y_train_log)\n",
    "X_blend3 = stage3.fit(X_blend2, y_train_log)\n",
    "print(\"X_train :\", X_train_std.shape)\n",
    "print(\"X_blend0:\", X_blend0.shape)\n",
    "print(\"X_blend1:\", X_blend1.shape)\n",
    "print(\"X_blend2:\", X_blend2.shape)\n",
    "print(\"X_blend3:\", X_blend3.shape)\n",
    "print()\n",
    "\n",
    "# 推定\n",
    "y_pred0_log = stage0.predict(X_val_std)\n",
    "y_pred1_log = stage1.predict(y_pred0_log)\n",
    "y_pred2_log = stage2.predict(y_pred1_log)\n",
    "y_pred3_log = stage3.predict(y_pred2_log)\n",
    "print(\"X_val :\", X_val_std.shape)\n",
    "print(\"y_pred0 :\", y_pred0_log.shape)\n",
    "print(\"y_pred1 :\", y_pred1_log.shape)\n",
    "print(\"y_pred2 :\", y_pred2_log.shape)\n",
    "print(\"y_pred3 :\", y_pred3_log.shape)\n",
    "print()\n",
    "\n",
    "# 評価\n",
    "y_pred = np.exp(y_pred3_log)\n",
    "mse_stack = mean_squared_error(y_pred, y_val)\n",
    "\n",
    "print(\"MSE = {:.3}\".format(mse_stack))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
