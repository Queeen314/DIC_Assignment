{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 事前準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# 各種ライブラリのインポート\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.datasets import mnist\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n",
      "uint8\n"
     ]
    }
   ],
   "source": [
    "# MNISTデータ読み込み\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)\n",
    "print(X_train[0].dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 全てのピクセルを一列にするため、平滑化を行う\n",
    "X_train = X_train.reshape(-1, 784)\n",
    "X_test = X_test.reshape(-1, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 - 1.0\n",
      "0.0 - 1.0\n",
      "(60000,)\n",
      "(60000, 10)\n",
      "float64\n",
      "(48000, 1, 784)\n",
      "(12000, 1, 784)\n",
      "(48000, 10)\n",
      "(12000, 10)\n"
     ]
    }
   ],
   "source": [
    "#前処理 画像は0~255で表されるが、機械学習をする上では0~1のfloat型で扱う\n",
    "X_train = X_train.astype(np.float)\n",
    "X_test = X_test.astype(np.float)\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.min(), \"-\", X_train.max())\n",
    "print(X_test.min(), \"-\", X_test.max())\n",
    "\n",
    "# チャネル軸を増やす\n",
    "X_train = X_train[:, np.newaxis, :]\n",
    "X_test = X_test[:, np.newaxis, :]\n",
    "\n",
    "# 正解ラベル（0から9の整数）をone-hot表現に変換する\n",
    "enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "y_test_one_hot = enc.fit_transform(y_test[:, np.newaxis])\n",
    "print(y_train.shape)\n",
    "print(y_train_one_hot.shape)\n",
    "print(y_train_one_hot.dtype)\n",
    "\n",
    "# データを分割する\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=0)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(y_train.shape)\n",
    "print(y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】チャンネル数を1に限定した1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここでは パディング は考えず、ストライド も1に固定します。  \n",
    "また、複数のデータを同時に処理することも考えなくて良く、バッチサイズは1のみに対応してください。  \n",
    "この部分の拡張はアドバンス課題とします。  \n",
    "フォワードプロパゲーションの数式は以下のようになります。  \n",
    "$a_i$ : 出力される配列のi番目の値  \n",
    "$F$ : フィルタのサイズ  \n",
    "$x_{(i+s)}$ : 入力の配列の(i+s)番目の値  \n",
    "$w_s$ : 重みの配列のs番目の値  \n",
    "$b$ : バイアス項  \n",
    "全てスカラーです。  \n",
    "次に更新式です。ここがAdaGradなどに置き換えられる点は全結合層と同様です。  \n",
    "$\\alpha$ : 学習率  \n",
    "$\\frac{\\partial L}{\\partial w_s}$ : $w_s$ に関する損失 $L$ の勾配  \n",
    "$\\frac{\\partial L}{\\partial b}$ : $b$ に関する損失 $L$ の勾配  \n",
    "勾配 $\\frac{\\partial L}{\\partial w_s}$ や $\\frac{\\partial L}{\\partial b}$ を求めるためのバックプロパゲーションの数式が以下です。  \n",
    "$\\frac{\\partial L}{\\partial a_i}$ : 勾配の配列のi番目の値  \n",
    "$N_{out}$ : 出力のサイズ  \n",
    "前の層に流す誤差の数式は以下です。  \n",
    "$\\frac{\\partial L}{\\partial x_j}$ : 前の層に流す誤差の配列のj番目の値  \n",
    "ただし、 $j-s<0$ または $j-s>N_{out}-1$ のとき $\\frac{\\partial L}{\\partial a_{(j-s)}} =0$ です。  \n",
    "全結合層との大きな違いは、重みが複数の特徴量に対して共有されていることです。この場合は共有されている分の誤差を全て足すことで勾配を求めます。  \n",
    "計算グラフ上での分岐はバックプロパゲーションの際に誤差の足し算をすれば良いことになります。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer:\n",
    "    \"\"\"Xavierの初期値\"\"\"\n",
    "    def W(self, Nin, F):\n",
    "        \"\"\"重みの初期化\n",
    "        :parameters\n",
    "            Nin (int): 入力のサイズ\n",
    "            F (int): フィルターサイズ\n",
    "        :returns\n",
    "            W (1d-array, (F,)): ランダムに発生させた重み\n",
    "        \"\"\"\n",
    "        return 1.0 / np.sqrt(Nin) * np.random.randn(F)\n",
    "    \n",
    "    def B(self, Nin):\n",
    "        \"\"\"バイアスの初期化\n",
    "        :parameters\n",
    "            Nin (int): 入力のサイズ\n",
    "        :returns\n",
    "            B (1d-array, (1,)): ランダムに発生させたバイアス\n",
    "        \"\"\"\n",
    "        return 1.0 / np.sqrt(Nin) * np.random.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"確率的勾配降下法\n",
    "    :parameters\n",
    "        lr (float): 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"ある層の重みやバイアスを更新する\n",
    "        :parameters\n",
    "            layer (instance): 更新前の層（FC）のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW\n",
    "        layer.B -= self.lr * layer.dB\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConv1d:\n",
    "    \"\"\"シンプルな１次元畳み込み（データ１次元、チャンネル数１、パディングなし、ストライド１）\n",
    "    :parameters\n",
    "        n_input (int): 入力サイズ（特徴量の数）\n",
    "        filter_size (int): 重みのサイズ（正方形と仮定）\n",
    "        stride (int): ストライド(今回は1)\n",
    "        padding (int): パディング（今回はなし）\n",
    "        initializer (class instance): 初期化方法のインスタンス\n",
    "        optimizer (class instance): 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_input, filter_size, stride=1, padding=0, initializer=XavierInitializer(), optimizer=SGD()):\n",
    "        self.optimizer = optimizer\n",
    "        # 入出力の形状などを取得\n",
    "        self.F = filter_size\n",
    "        self.Nin = n_input\n",
    "        self.S = stride\n",
    "        self.P = padding\n",
    "        self.Nout = self.calc_output_size()  # 出力形状\n",
    "        # 重みとバイアスの初期化\n",
    "        self.W = initializer.W(self.Nin, self.F)\n",
    "        self.B = initializer.B(self.Nin)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"順伝播\n",
    "        :parameters\n",
    "            X (1d-array, (Nin,)): 入力\n",
    "        :returns\n",
    "            A (1d-array, (Nout,)): 出力\n",
    "        \"\"\"\n",
    "        A = np.zeros((self.Nout,))\n",
    "        # 要素ごとに１つずつずらしながら、アダマール積して合計する\n",
    "        for i in range(self.Nout):\n",
    "            A[i] = np.sum(X[i:i+self.F] * self.W) + self.B\n",
    "        \n",
    "        # Xをbackward用に保持\n",
    "        self.X = X\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"逆伝播\n",
    "        :parameters\n",
    "            dA (1d-array, (Nout,)): 出力に関する勾配\n",
    "        :returns\n",
    "            dX (1d-array, (Nin,)): 入力に関する勾配\n",
    "        \"\"\"\n",
    "        # dB\n",
    "        self.dB = np.sum(dA, axis=0, keepdims=True)\n",
    "        \n",
    "        # dW （dAを固定して、xをずらしていく）\n",
    "        self.dW = np.zeros(self.F)\n",
    "        for i in range(self.F):\n",
    "            self.dW[i] = np.matmul(self.X[i:i+self.Nout].T, dA)  # X.Tは転置されないけど、後のため。\n",
    "        \n",
    "        # dX （wを１つずつずらした（Nout, Nin）行列を作成して、dAとの行列積を計算する）\n",
    "        tile_W = np.zeros((self.Nout, self.Nin))\n",
    "        for i in range(self.Nout):\n",
    "            tile_W[i, i:i+self.F] = self.W  # １つずつずらしながら、代入（空いているところはゼロ）\n",
    "        dX = np.matmul(dA, tile_W)  # 行列積\n",
    "\n",
    "        # 勾配をもとに重みとバイアスを更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dX\n",
    "\n",
    "    def calc_output_size(self):\n",
    "        return int((self.Nin + 2*self.P - self.F) / self.S + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題2】1次元畳み込み後の出力サイズの計算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "畳み込みを行うと特徴量の数が変化します。どのように変化するかは以下の数式から求められます。  \n",
    "パディングやストライドも含めています。この計算を行う関数を作成してください。  \n",
    "$N_{out}$ : 出力のサイズ（特徴量の数）  \n",
    "$N_{in}$ : 入力のサイズ（特徴量の数）  \n",
    "$P$ : ある方向へのパディングの数  \n",
    "$F$ : フィルタのサイズ  \n",
    "$S$ : ストライドのサイズ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_output_size(self):\n",
    "        return int((self.Nin + 2*self.P - self.F) / self.S + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】小さな配列での1次元畳み込み層の実験"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "次に示す小さな配列でフォワードプロパゲーションとバックプロパゲーションが正しく行えているか確認してください。  \n",
    "入力x、重みw、バイアスbを次のようにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nin = 4\n",
      "F = 3\n",
      "S = 1\n",
      "P = 0\n",
      "Nout = 2\n",
      "ランダムに初期値生成した場合の重みとバイアス\n",
      "W [-0.31278988 -0.40323115 -0.23836034]\n",
      "B [-0.00025004]\n",
      "重みとバイアスはこっちに変更\n",
      "W [3. 5. 7.]\n",
      "B [1.]\n",
      "=====forward\n",
      "A [35. 50.]\n",
      "=====backward\n",
      "dW [ 50.  80. 110.]\n",
      "dB [30.]\n",
      "dX [ 30. 110. 170. 140.]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([1, 2, 3, 4], dtype=float)\n",
    "w = np.array([3, 5, 7], dtype=float)\n",
    "b = np.array([1], dtype=float)\n",
    "\n",
    "cnn1 = SimpleConv1d(n_input=len(x), filter_size=len(w))\n",
    "print(\"Nin =\", cnn1.Nin)\n",
    "print(\"F =\", cnn1.F)\n",
    "print(\"S =\", cnn1.S)\n",
    "print(\"P =\", cnn1.P)\n",
    "print(\"Nout =\", cnn1.Nout)\n",
    "print(\"ランダムに初期値生成した場合の重みとバイアス\")\n",
    "print(\"W\", cnn1.W)\n",
    "print(\"B\", cnn1.B)\n",
    "print(\"重みとバイアスはこっちに変更\")\n",
    "cnn1.W = w\n",
    "cnn1.B = b\n",
    "print(\"W\", cnn1.W)\n",
    "print(\"B\", cnn1.B)\n",
    "\n",
    "# forward\n",
    "print(\"=====forward\")\n",
    "a = cnn1.forward(x)\n",
    "print(\"A\", a)\n",
    "\n",
    "# lossがこうだとすると、、、\n",
    "da = np.array([10, 20], dtype=float)\n",
    "\n",
    "# backward\n",
    "print(\"=====backward\")\n",
    "dx = cnn1.backward(da)\n",
    "print(\"dW\", cnn1.dW)\n",
    "print(\"dB\", cnn1.dB)\n",
    "print(\"dX\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】チャンネル数を限定しない1次元畳み込み層クラスの作成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "チャンネル数を1に限定しない1次元畳み込み層のクラスConv1dを作成してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class XavierInitializer2:\n",
    "    \"\"\"Xavierの初期値\"\"\"\n",
    "    def W(self, Nin, F, Cin, Cout):\n",
    "        \"\"\"重みの初期化\n",
    "        :parameters\n",
    "            Nin (int): 入力のサイズ\n",
    "            F (int): フィルターサイズ\n",
    "            Cin (int): 入力チャンネル数\n",
    "            Cout (int): 出力チャンネル数\n",
    "        :returns\n",
    "            W (3d-array, (Cout, Cin, F)): ランダムに発生させた重み\n",
    "        \"\"\"\n",
    "        return 1.0 / np.sqrt(Nin) * np.random.randn(Cout, Cin, F)\n",
    "    \n",
    "    def B(self, Nin, Cout):\n",
    "        \"\"\"バイアスの初期化\n",
    "        :parameters\n",
    "            Nin (int): 入力のサイズ\n",
    "            Cout (int): 出力チャンネル数\n",
    "        :returns\n",
    "            B (1d-array, (Cout,)): ランダムに発生させたバイアス\n",
    "        \"\"\"\n",
    "        return 1.0 / np.sqrt(Nin) * np.random.randn(Cout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"チャンネル数を限定しない１次元畳み込み（データ１次元、パディングなし、ストライド１）\n",
    "    :parameters\n",
    "        Nin (int): 入力サイズ（特徴量の数）\n",
    "        Cin (int): 入力チャンネル数（色の次元の数？）\n",
    "        Cout (int): 出力チャンネル数（フィルター枚数）\n",
    "        F (int): フィルター（重み）のサイズ（正方形と仮定）\n",
    "        S (int): ストライド(今回は1)\n",
    "        P (int): パディング（今回はなし）\n",
    "        initializer (class instance): 初期化方法のインスタンス\n",
    "        optimizer (class instance): 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, Nin, F, Cin, Cout, S=1, P=0, initializer=XavierInitializer2(), optimizer=SGD()):\n",
    "        self.optimizer = optimizer\n",
    "        # 入出力の形状などを取得\n",
    "        self.Nin = Nin\n",
    "        self.F = F\n",
    "        self.Cin = Cin\n",
    "        self.Cout = Cout\n",
    "        self.S = S\n",
    "        self.P = P\n",
    "        self.Nout = self.calc_output_size()  # 出力形状\n",
    "        # 重みとバイアスの初期化\n",
    "        self.W = initializer.W(self.Nin, self.F, self.Cin, self.Cout)\n",
    "        self.B = initializer.B(self.Nin, self.Cout)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"順伝播\n",
    "        :parameters\n",
    "            X (2d-array, (Cin, Nin)): 入力\n",
    "        :returns\n",
    "            A (2d-array, (Cout, Nout)): 出力\n",
    "        \"\"\"\n",
    "        # 出力格納用\n",
    "        A = np.zeros((self.Cout, self.Nout))\n",
    "        \n",
    "        # 要素ごとに１つずつずらしながら、アダマール積して合計する\n",
    "        for i in range(self.Nout):\n",
    "            # 畳み込みでの、X*W + Bを計算する\n",
    "            # X(Cin, Nin)のうち、スライスでサイズFだけ取得する。 --> X(Cin, F)\n",
    "            # X(Cin, F)と、W(Cout, Cin, F)のアダマール積を計算 --> \n",
    "            # このとき、XがCout方向にブロードキャストされるので、XW(Cout, Cin, F)になる。\n",
    "            # (Cin, F)方向に合計するので、np.sum(axis=(1, 2))\n",
    "            # その結果、np.sum(XW)は(Cout,)となる。これとB(Cout,)との和をとる。\n",
    "            A[:, i] = np.sum(X[:, i:i+self.F] * self.W, axis=(1, 2)) + self.B\n",
    "\n",
    "        # Xをbackward用に保持\n",
    "        self.X = X\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"逆伝播\n",
    "        :parameters\n",
    "            dA (2d-array, (Cout, Nout)): 出力の勾配\n",
    "        :returns\n",
    "            dX (2d-array, (Cin, Nin)): 入力の勾配\n",
    "        :returns\n",
    "        \"\"\"\n",
    "        # dB(Cout,)\n",
    "        self.dB = np.sum(dA, axis=1)\n",
    "        \n",
    "        # dW(Cout, Cin, F)\n",
    "        self.dW = np.zeros((self.Cout, self.Cin, self.F))\n",
    "        for i in range(self.F):\n",
    "            # X(Cin, Nin) x dA(Cout, Nout)を計算して、dW(Cout, Cin, F)にしたい。\n",
    "            # スライスして、X(Cin, Nout) x dA.T(Nout, Cout) = XdA(Cin, Cout)\n",
    "            # これが、F回計算される。\n",
    "            #print(\"X[]dA\", np.matmul(self.X[:, i:i+self.Nout], dA.T).shape)\n",
    "            self.dW[:, :, i] = np.matmul(self.X[:, i:i+self.Nout], dA.T).T\n",
    "        \n",
    "        # dX(Cin, Nin)\n",
    "        # W(Cout, Cin, F)\n",
    "        # dA(Cout, Nout)・ tile_W(Cin, Nout, Nin) = dAW(Cin, Cout, Nin)\n",
    "        # np.sum(dAW, axis=1).T = dX(Cin, Nin)\n",
    "        tile_W = np.zeros((self.Cin, self.Cout, self.Nout, self.Nin))\n",
    "        for i in range(self.Nout):\n",
    "            tile_W[:, :, i, i:i+self.F] = self.W.transpose(1, 0, 2)  # W(Cout, Cin, F) --> W(Cin, Cout, F)\n",
    "        dX = np.sum(np.matmul(dA[:, np.newaxis, :], tile_W), axis=(1, 2))  # 行列積\n",
    "        \n",
    "\n",
    "        # 勾配をもとに重みとバイアスを更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dX\n",
    "\n",
    "    def calc_output_size(self):\n",
    "        return int((self.Nin + 2*self.P - self.F) / self.S + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nin = 4\n",
      "Cin = 2\n",
      "Cout = 3\n",
      "F = 3\n",
      "S = 1\n",
      "P = 0\n",
      "Nout = 2\n",
      "ランダムに初期値生成した場合の重みとバイアス\n",
      "W [[[-7.10145978e-01 -9.36557113e-01  2.42480738e-01]\n",
      "  [-3.06398261e-01  1.98884294e-01 -4.73332275e-01]]\n",
      "\n",
      " [[-1.68247262e-01 -5.87483433e-01  4.11674844e-01]\n",
      "  [-9.86415483e-04  5.61295847e-02 -6.41281042e-01]]\n",
      "\n",
      " [[ 4.89812762e-01  1.32009216e-01  5.74506544e-01]\n",
      "  [-1.94824386e-01  5.17991821e-01 -1.01128735e+00]]]\n",
      "B [ 1.06731223 -0.63646687 -0.29228512]\n",
      "重みとバイアスはこっちに変更\n",
      "W [[[1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n",
      "B [1. 2. 3.]\n",
      "=====forward\n",
      "A [[16. 22.]\n",
      " [17. 23.]\n",
      " [18. 24.]]\n",
      "=====backward\n",
      "dW [[[ 50.  80. 110.]\n",
      "  [ 80. 110. 140.]]\n",
      "\n",
      " [[110. 180. 250.]\n",
      "  [180. 250. 320.]]\n",
      "\n",
      " [[170. 280. 390.]\n",
      "  [280. 390. 500.]]]\n",
      "dB [ 30.  70. 110.]\n",
      "dX [[ 90. 210. 210. 120.]\n",
      " [ 90. 210. 210. 120.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]], dtype=float)\n",
    "w = np.ones((3, 2, 3), dtype=float)\n",
    "b = np.array([1, 2, 3], dtype=float)\n",
    "\n",
    "\n",
    "cnn = Conv1d(Nin=x.shape[1], Cin=x.shape[0], Cout=w.shape[0], F=w.shape[2])\n",
    "print(\"Nin =\", cnn.Nin)\n",
    "print(\"Cin =\", cnn.Cin)\n",
    "print(\"Cout =\", cnn.Cout)\n",
    "print(\"F =\", cnn.F)\n",
    "print(\"S =\", cnn.S)\n",
    "print(\"P =\", cnn.P)\n",
    "print(\"Nout =\", cnn.Nout)\n",
    "print(\"ランダムに初期値生成した場合の重みとバイアス\")\n",
    "print(\"W\", cnn.W)\n",
    "print(\"B\", cnn.B)\n",
    "print(\"重みとバイアスはこっちに変更\")\n",
    "cnn.W = w\n",
    "cnn.B = b\n",
    "print(\"W\", cnn.W)\n",
    "print(\"B\", cnn.B)\n",
    "\n",
    "# forward\n",
    "print(\"=====forward\")\n",
    "a = cnn.forward(x)\n",
    "print(\"A\", a)\n",
    "\n",
    "\n",
    "# lossがこうだとすると、、、dxはこうなるはず（dx_true）\n",
    "da = np.array([[10, 20], [30, 40], [50, 60]], dtype=float)\n",
    "dx_true = np.array([[90, 210, 210, 120], [90, 210, 210, 120]], dtype=float)\n",
    "\n",
    "# backward\n",
    "print(\"=====backward\")\n",
    "dx = cnn.backward(da)\n",
    "print(\"dW\", cnn.dW)\n",
    "print(\"dB\", cnn.dB)\n",
    "print(\"dX\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】パディングの実装"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dPad:\n",
    "    \"\"\"チャンネル数を限定しない１次元畳み込み（データ１次元、パディングなし、ストライド１）\n",
    "    :parameters\n",
    "        Nin (int): 入力サイズ（特徴量の数）\n",
    "        Cin (int): 入力チャンネル数（色の次元の数？）\n",
    "        Cout (int): 出力チャンネル数（フィルター枚数）\n",
    "        F (int): フィルター（重み）のサイズ（正方形と仮定）\n",
    "        S (int): ストライド(今回は1)\n",
    "        P (int): パディング（今回はなし）\n",
    "        initializer (class instance): 初期化方法のインスタンス\n",
    "        optimizer (class instance): 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, Cout, F, S=1, P=0, pad_method=\"constant\", initializer=XavierInitializer2(), optimizer=SGD()):\n",
    "        # 重み初期化方法・最適化手法\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        # フィルター形状・畳み込みの方法（stride, padding）などを取得\n",
    "        self.Cout = Cout\n",
    "        self.F = F\n",
    "        self.S = S\n",
    "        self.P = P\n",
    "        self.pad_method = pad_method\n",
    "        # 重みとバイアス\n",
    "        self.W = None\n",
    "        self.B = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"順伝播\n",
    "        :parameters\n",
    "            X (2d-array, (Cin, Nin)): 入力\n",
    "        :returns\n",
    "            A (2d-array, (Cout, Nout)): 出力\n",
    "        \"\"\"\n",
    "        # 形状取得\n",
    "        # Cout, F, S, Pはインスタンス化した時に取得済み\n",
    "        self.Cin = X.shape[0]  # 入力チャンネル数\n",
    "        self.Nin = X.shape[1]  # 入力特徴量数\n",
    "        self.Nout = self.calc_output_size()  # 出力サイズ（出力特徴量数）\n",
    "        print(\"Nout\", self.Nout)\n",
    "        \n",
    "        # パディング\n",
    "        if self.P > 0:\n",
    "            self.X = np.pad(X, [(0, 0), (self.P, self.P)], self.pad_method)\n",
    "            print(\"padded X\\n\", self.X)\n",
    "        else:\n",
    "            self.X = X.copy()\n",
    "\n",
    "        # 重みとバイアスの初期化\n",
    "        #self.W = self.initializer.W(self.Nin, self.F, self.Cin, self.Cout)\n",
    "        #self.B = self.initializer.B(self.Nin, self.Cout)\n",
    "        \n",
    "        # 出力格納用\n",
    "        A = np.zeros((self.Cout, self.Nout))\n",
    "        \n",
    "        # 要素ごとに１つずつずらしながら、アダマール積して合計する\n",
    "        for i in range(self.Nout):\n",
    "            # 畳み込み： X*W + Bを計算する\n",
    "            # X(Cin, Nin)のうち、スライスでサイズFだけ取得する。 --> X(Cin, F)\n",
    "            # X(Cin, F)と、W(Cout, Cin, F)のアダマール積を計算 --> \n",
    "            # このとき、XがCout方向にブロードキャストされるので、XW(Cout, Cin, F)になる。\n",
    "            # (Cin, F)方向に合計するので、np.sum(axis=(1, 2))\n",
    "            # その結果、np.sum(XW)は(Cout,)となる。これとB(Cout,)との和をとる。\n",
    "            A[:, i] = np.sum(self.X[:, i:i+self.F] * self.W, axis=(1, 2)) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"逆伝播\n",
    "        :parameters\n",
    "            dA (2d-array, (Cout, Nout)): 出力の勾配\n",
    "        :returns\n",
    "            dX (2d-array, (Cin, Nin)): 入力の勾配\n",
    "        :returns\n",
    "        \"\"\"\n",
    "        # dB(Cout,)\n",
    "        self.dB = np.sum(dA, axis=1)\n",
    "        \n",
    "        # dW(Cout, Cin, F)\n",
    "        self.dW = np.zeros((self.Cout, self.Cin, self.F))\n",
    "        for i in range(self.F):\n",
    "            # X(Cin, Nin) x dA(Cout, Nout)を計算して、dW(Cout, Cin, F)にしたい。\n",
    "            # スライスして、X(Cin, Nout) x dA.T(Nout, Cout) = XdA(Cin, Cout)\n",
    "            # これが、F回計算される。\n",
    "            #print(self.X[:, i:i+self.Nout].shape)\n",
    "            #print(dA.T.shape)\n",
    "            self.dW[:, :, i] = np.matmul(self.X[:, i:i+self.Nout], dA.T).T\n",
    "        \n",
    "        # dX(Cin, Nin)を求める部分\n",
    "        # 最初に、W(Cout, Cin, F)からtile_W(Cin, Cout, Nout, Nin)を作る\n",
    "        # 一つずつずらしながら、Wをタイルする。\n",
    "        tile_W = np.zeros((self.Cin, self.Cout, self.Nout, self.Nin + 2*self.P))\n",
    "        for i in range(self.Nout):\n",
    "            tile_W[:, :, i, i:i+self.F] = self.W.transpose(1, 0, 2)  # W(Cout, Cin, F) --> W(Cin, Cout, F)\n",
    "        # dA(Cout, Nout)のまま、行列計算を行うと、\n",
    "        # dA(Cout, Nout) ・ tile_W(Cin, Cout, Nout, Nin) = dAW(Cin, Cout, Cout, Nin)になるので、\n",
    "        # dA(Cout, 1, Nout)に変形してから\n",
    "        # dA(Cout, 1, Nout) ・ tile_W(Cin, Cout, Nout, Nin) = dAW(Cin, Cout, 1, Nin)とする。\n",
    "        # np.sum(dAW, axis=(1, 2))で、dX(Cin, Nin)となる。\n",
    "        dX = np.sum(np.matmul(dA[:, np.newaxis, :], tile_W), axis=(1, 2))\n",
    "        \n",
    "        # パディングした部分を削除する\n",
    "        if self.P > 0:\n",
    "            dX = dX[:, self.P:-self.P]\n",
    "        \n",
    "        # 勾配をもとに重みとバイアスを更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dX\n",
    "\n",
    "    def calc_output_size(self):\n",
    "        return int((self.Nin + 2*self.P - self.F) / self.S + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cout = 3\n",
      "F = 3\n",
      "S = 1\n",
      "P = 2\n",
      "重みとバイアスはこっちに変更\n",
      "W [[[1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n",
      "B [1. 2. 3.]\n",
      "=====forward\n",
      "Nout 6\n",
      "padded X\n",
      " [[0. 0. 1. 2. 3. 4. 0. 0.]\n",
      " [0. 0. 2. 3. 4. 5. 0. 0.]]\n",
      "A [[ 4.  9. 16. 22. 17. 10.]\n",
      " [ 5. 10. 17. 23. 18. 11.]\n",
      " [ 6. 11. 18. 24. 19. 12.]]\n",
      "=====backward\n",
      "dW [[[100. 100. 100.]\n",
      "  [140. 140. 140.]]\n",
      "\n",
      " [[320. 280. 320.]\n",
      "  [440. 400. 440.]]\n",
      "\n",
      " [[180. 220. 180.]\n",
      "  [260. 300. 260.]]]\n",
      "dB [ 60. 180. 120.]\n",
      "dX [[180. 180. 180. 180.]\n",
      " [180. 180. 180. 180.]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[1, 2, 3, 4], [2, 3, 4, 5]], dtype=float)\n",
    "w = np.ones((3, 2, 3), dtype=float)\n",
    "b = np.array([1, 2, 3], dtype=float)\n",
    "\n",
    "\n",
    "#cnn = Conv1d(Nin=x.shape[1], Cin=x.shape[0], Cout=w.shape[0], F=w.shape[2])\n",
    "n_pad = 2\n",
    "cnn = Conv1dPad(Cout=w.shape[0], F=w.shape[2], S=1, P=n_pad)\n",
    "print(\"Cout =\", cnn.Cout)\n",
    "print(\"F =\", cnn.F)\n",
    "print(\"S =\", cnn.S)\n",
    "print(\"P =\", cnn.P)\n",
    "print(\"重みとバイアスはこっちに変更\")\n",
    "cnn.W = w\n",
    "cnn.B = b\n",
    "print(\"W\", cnn.W)\n",
    "print(\"B\", cnn.B)\n",
    "\n",
    "# forward\n",
    "print(\"=====forward\")\n",
    "a = cnn.forward(x)\n",
    "print(\"A\", a)\n",
    "\n",
    "\n",
    "# lossがこうだとすると、、、dxはこうなるはず（dx_true）\n",
    "if n_pad == 0:\n",
    "    da = np.array([[10, 10], [20, 40], [30, 10]], dtype=float)\n",
    "    dx_true = np.array([[90, 210, 210, 120], [90, 210, 210, 120]], dtype=float)\n",
    "elif n_pad == 1:\n",
    "    da = np.array([[10, 10, 10, 10],\n",
    "                   [20, 40, 20, 40],\n",
    "                   [30, 10, 30, 10]], dtype=float)\n",
    "elif n_pad == 2:\n",
    "    da = np.array([[10, 10, 10, 10, 10, 10],\n",
    "                   [20, 40, 20, 40, 20, 40],\n",
    "                   [30, 10, 30, 10, 30, 10]], dtype=float)\n",
    "\n",
    "# backward\n",
    "print(\"=====backward\")\n",
    "dx = cnn.backward(da)\n",
    "print(\"dW\", cnn.dW)\n",
    "print(\"dB\", cnn.dB)\n",
    "print(\"dX\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】ミニバッチへの対応"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ここまでの課題はバッチサイズ1で良いとしてきました。しかし、実際は全結合層同様にミニバッチ学習が行われます。  \n",
    "Conv1dクラスを複数のデータが同時に計算できるように変更してください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"Heの初期値\"\"\"\n",
    "    def W(self, Nin, F, Cin, Cout):\n",
    "        \"\"\"重みの初期化\n",
    "        :parameters\n",
    "            Nin (int): 入力のサイズ\n",
    "            F (int): フィルターサイズ\n",
    "            Cin (int): 入力チャンネル数\n",
    "            Cout (int): 出力チャンネル数\n",
    "        :returns\n",
    "            W (3d-array, (Cout, Cin, F)): ランダムに発生させた重み\n",
    "        \"\"\"\n",
    "        return np.sqrt(2.0 / (Cin*Nin)) * np.random.randn(Cout, Cin, F)\n",
    "    \n",
    "    def B(self, Nin, Cin, Cout):\n",
    "        \"\"\"バイアスの初期化\n",
    "        :parameters\n",
    "            Nin (int): 入力のサイズ\n",
    "            Cout (int): 出力チャンネル数\n",
    "        :returns\n",
    "            B (1d-array, (Cout,)): ランダムに発生させたバイアス\n",
    "        \"\"\"\n",
    "        return np.sqrt(2.0 / (Cin*Nin)) * np.random.randn(Cout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD:\n",
    "    \"\"\"確率的勾配降下法\n",
    "    :parameters\n",
    "        lr (float): 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"ある層の重みやバイアスを更新する\n",
    "        :parameters\n",
    "            layer (instance): 更新前の層（FC）のインスタンス\n",
    "        \"\"\"\n",
    "        layer.W -= self.lr * layer.dW / layer.Ndata\n",
    "        layer.B -= self.lr * layer.dB / layer.Ndata\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1dBatch:\n",
    "    \"\"\"チャンネル数を限定しない１次元畳み込み（データ１次元、パディングなし、ストライド１）\n",
    "    :parameters\n",
    "        Nin (int): 入力サイズ（特徴量の数）\n",
    "        Cin (int): 入力チャンネル数（色の次元の数？）\n",
    "        Cout (int): 出力チャンネル数（フィルター枚数）\n",
    "        F (int): フィルター（重み）のサイズ（正方形と仮定）\n",
    "        S (int): ストライド(今回は1)\n",
    "        P (int): パディング（今回はなし）\n",
    "        initializer (class instance): 初期化方法のインスタンス\n",
    "        optimizer (class instance): 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, Cout, F, S=1, P=0, pad_method=\"constant\", initializer=HeInitializer(), optimizer=SGD()):\n",
    "        # 重み初期化方法・最適化手法\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        # フィルター形状・畳み込みの方法（stride, padding）などを取得\n",
    "        self.Cout = Cout\n",
    "        self.F = F\n",
    "        self.S = S\n",
    "        self.P = P\n",
    "        self.pad_method = pad_method\n",
    "        # 重みとバイアス\n",
    "        self.W = None\n",
    "        self.B = None\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"順伝播\n",
    "        :parameters\n",
    "            X (3d-array, (Ndata, Cin, Nin)): 入力\n",
    "        :returns\n",
    "            A (3d-array, (Ndata, Cout, Nout)): 出力\n",
    "        \"\"\"\n",
    "        # データ数・形状等の取得\n",
    "        # Cout, F, S, Pはインスタンス化した時に取得済み\n",
    "        self.Ndata = X.shape[0]  # データ数\n",
    "        self.Cin = X.shape[1]  # 入力チャンネル数\n",
    "        self.Nin = X.shape[2]  # 入力特徴量数\n",
    "        self.Nout = self.calc_output_size()  # 出力サイズ（出力特徴量数）\n",
    "        print(\"Nout\", self.Nout)\n",
    "        \n",
    "        # パディング\n",
    "        if self.P > 0:\n",
    "            self.X = np.pad(X, [(0, 0), (0, 0), (self.P, self.P)], self.pad_method)\n",
    "            print(\"padded X\\n\", self.X)\n",
    "        else:\n",
    "            self.X = X.copy()\n",
    "\n",
    "        # 重みとバイアスの初期化（テスト時はコメントアウトする）\n",
    "        #self.W = self.initializer.W(self.Nin, self.F, self.Cin, self.Cout)\n",
    "        #self.B = self.initializer.B(self.Nin, self.Cout)\n",
    "        \n",
    "        # 出力格納用\n",
    "        A = np.zeros((self.Ndata, self.Cout, self.Nout))\n",
    "\n",
    "        # 要素ごとに１つずつずらしながら、アダマール積して合計する\n",
    "        for i in range(self.Nout):\n",
    "            # 畳み込み： X*W + Bを計算する\n",
    "            # X(Ndata, Cin, Nin)のうち、スライスでサイズFだけ取得する。 --> X(Ndata, Cin, F)\n",
    "            # X(Ndata, Cin, F)と、W(Cout, Cin, F)のアダマール積を計算するが、\n",
    "            # このとき、XをCout方向にブロードキャストしたいので、newaxisで X(Ndata, 1, Cin, F)とする。\n",
    "            # これで、XW(Ndata, Cout, Cin, F)になる。\n",
    "            XW = self.X[:, np.newaxis, :, i:i+self.F] * self.W\n",
    "            \n",
    "            # XWを(Cin, F)方向に合計するので、np.sum(XW, axis=(1, 2))\n",
    "            # その結果、np.sum(XW)は(Cout,)となる。これとB(Cout,)との和をとる。\n",
    "            A[:, :, i] = np.sum(XW, axis=(1, 2)) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"逆伝播\n",
    "        :parameters\n",
    "            dA (3d-array, (Ndata, Cout, Nout)): 出力の勾配\n",
    "        :returns\n",
    "            dX (3d-array, (Ndata, Cin, Nin)): 入力の勾配\n",
    "        :returns\n",
    "        \"\"\"\n",
    "        # dB(Cout,)\n",
    "        self.dB = np.sum(dA, axis=(0, 2))\n",
    "        \n",
    "        # dW(Cout, Cin, F)\n",
    "        self.dW = np.zeros((self.Cout, self.Cin, self.F))\n",
    "        for i in range(self.F):\n",
    "            # X(Ndata, Cin, Nin) x dA(Ndata, Cout, Nout)を計算して、dW(Cout, Cin, F)にしたい。\n",
    "            # スライスして、X[](Ndata, Cin, Nout) x dA(Ndata, Nout, Cout) = XdA(Ndata, Cin, Cout)\n",
    "            XdA = np.matmul(self.X[:, :, i:i+self.Nout], dA.transpose(0, 2, 1))\n",
    "            \n",
    "            # これをNdata方向に合計すると、sumXdA(Cin, Cout)となる。\n",
    "            sumXdA = np.sum(XdA, axis=0)\n",
    "            \n",
    "            # dW(Cout, Cin, F)に格納するために、XdA(Cin, Cout)を転置して、(Cout, Cin)にする。\n",
    "            self.dW[:, :, i] = sumXdA.T\n",
    "        \n",
    "        # dX(Ndata, Cin, Nin)を求める部分\n",
    "        # 最初に、W(Cout, Cin, F)からtile_W(Cin, Cout, Nout, Nin)を作る\n",
    "        # 一つずつずらしながら、Wをタイルする。\n",
    "        tile_W = np.zeros((self.Cin, self.Cout, self.Nout, self.Nin + 2*self.P))\n",
    "        for i in range(self.Nout):\n",
    "            tile_W[:, :, i, i:i+self.F] = self.W.transpose(1, 0, 2)  # W(Cout, Cin, F) --> W(Cin, Cout, F)\n",
    "\n",
    "        # dA(Ndata, Cout, Nout)のまま、行列計算を行うと、\n",
    "        # dA(Ndata, Cout, Nout) ・ tile_W(Cin, Cout, Nout, Nin) = dAW(Ndata, Cin, Cout, Cout, Nin)になるので、\n",
    "        # dA(Ndata, 1, Cout, 1, Nout)に変形してから\n",
    "        # dA(Ndata, 1, Cout, 1, Nout) ・ tile_W(Cin, Cout, Nout, Nin) = dAW(Ndata, Cin, Cout, 1, Nin)とする。\n",
    "        dAW = np.matmul(dA[:, np.newaxis, :, np.newaxis, :], tile_W)  # (Ndata, Cin, Cout, 1, Nin)\n",
    "        \n",
    "        # (Cout, 1)方向で合計をとると、np.sum(dAW, axis=(2, 3))\n",
    "        # その結果、dX(Ndata, Cin, Nin)となる。\n",
    "        dX = np.sum(dAW, axis=(2, 3))\n",
    "        \n",
    "        # パディングした部分を削除する\n",
    "        if self.P > 0:\n",
    "            dX = dX[:, :, self.P:-self.P]\n",
    "        \n",
    "        # 勾配をもとに重みとバイアスを更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dX\n",
    "\n",
    "    def calc_output_size(self):\n",
    "        return int((self.Nin + 2*self.P - self.F) / self.S + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 3)\n",
      "(5, 1, 3, 1, 6)\n",
      "(2, 3, 6, 8)\n",
      "(5, 2, 3, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "# X(Ndata, Cin, Nout) x dA(Ndata, Nout, Cout)\n",
    "x = np.ones((5, 2, 6))\n",
    "da = np.ones((5, 6, 3))\n",
    "c = np.matmul(x, da)\n",
    "print(c.shape)\n",
    "\n",
    "# dA(Ndata, Cout, Nout), tile_W(Cin, Cout, Nout, Nin)\n",
    "da = np.ones((5, 3, 6))\n",
    "tw = np.ones((2, 3, 6, 8))\n",
    "print(da[:, np.newaxis, :, np.newaxis, :].shape)\n",
    "print(tw.shape)\n",
    "dx = np.matmul(da[:, np.newaxis, :, np.newaxis, :], tw)\n",
    "print(dx.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 2, 4)\n",
      "Cout = 3\n",
      "F = 3\n",
      "S = 1\n",
      "P = 2\n",
      "重みとバイアスはこっちに変更\n",
      "W [[[1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n",
      "B [1. 2. 3.]\n",
      "=====forward\n",
      "Nout 6\n",
      "padded X\n",
      " [[[0. 0. 1. 2. 3. 4. 0. 0.]\n",
      "  [0. 0. 2. 3. 4. 5. 0. 0.]]\n",
      "\n",
      " [[0. 0. 2. 3. 4. 5. 0. 0.]\n",
      "  [0. 0. 3. 4. 5. 6. 0. 0.]]\n",
      "\n",
      " [[0. 0. 3. 4. 5. 6. 0. 0.]\n",
      "  [0. 0. 4. 5. 6. 7. 0. 0.]]\n",
      "\n",
      " [[0. 0. 4. 5. 6. 7. 0. 0.]\n",
      "  [0. 0. 5. 6. 7. 8. 0. 0.]]\n",
      "\n",
      " [[0. 0. 5. 6. 7. 8. 0. 0.]\n",
      "  [0. 0. 6. 7. 8. 9. 0. 0.]]]\n",
      "A [[[ 1.  1. 10. 16. 22. 28.]\n",
      "  [ 2. 11. 17. 23. 29.  2.]\n",
      "  [12. 18. 24. 30.  3.  3.]]\n",
      "\n",
      " [[ 1.  1. 16. 22. 28. 34.]\n",
      "  [ 2. 17. 23. 29. 35.  2.]\n",
      "  [18. 24. 30. 36.  3.  3.]]\n",
      "\n",
      " [[ 1.  1. 22. 28. 34. 40.]\n",
      "  [ 2. 23. 29. 35. 41.  2.]\n",
      "  [24. 30. 36. 42.  3.  3.]]\n",
      "\n",
      " [[ 1.  1. 28. 34. 40. 46.]\n",
      "  [ 2. 29. 35. 41. 47.  2.]\n",
      "  [30. 36. 42. 48.  3.  3.]]\n",
      "\n",
      " [[ 1.  1. 34. 40. 46. 52.]\n",
      "  [ 2. 35. 41. 47. 53.  2.]\n",
      "  [36. 42. 48. 54.  3.  3.]]]\n",
      "dA (5, 3, 6)\n",
      "=====backward\n",
      "dW [[[ 900.  900.  900.]\n",
      "  [1100. 1100. 1100.]]\n",
      "\n",
      " [[2800. 2600. 2800.]\n",
      "  [3400. 3200. 3400.]]\n",
      "\n",
      " [[1700. 1900. 1700.]\n",
      "  [2100. 2300. 2100.]]]\n",
      "dB [300. 900. 600.]\n",
      "dX [[[180. 180. 180. 180.]\n",
      "  [180. 180. 180. 180.]]\n",
      "\n",
      " [[180. 180. 180. 180.]\n",
      "  [180. 180. 180. 180.]]\n",
      "\n",
      " [[180. 180. 180. 180.]\n",
      "  [180. 180. 180. 180.]]\n",
      "\n",
      " [[180. 180. 180. 180.]\n",
      "  [180. 180. 180. 180.]]\n",
      "\n",
      " [[180. 180. 180. 180.]\n",
      "  [180. 180. 180. 180.]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.array([[[1, 2, 3, 4], [2, 3, 4, 5]],\n",
    "              [[2, 3, 4, 5], [3, 4, 5, 6]],\n",
    "              [[3, 4, 5, 6], [4, 5, 6, 7]],\n",
    "              [[4, 5, 6, 7], [5, 6, 7, 8]],\n",
    "              [[5, 6, 7, 8], [6, 7, 8, 9]],\n",
    "             ], dtype=float)\n",
    "print(x.shape)\n",
    "w = np.ones((3, 2, 3), dtype=float)\n",
    "b = np.array([1, 2, 3], dtype=float)\n",
    "\n",
    "#cnn = Conv1d(Nin=x.shape[1], Cin=x.shape[0], Cout=w.shape[0], F=w.shape[2])\n",
    "n_pad = 2\n",
    "cnn = Conv1dBatch(Cout=w.shape[0], F=w.shape[2], S=1, P=n_pad)\n",
    "print(\"Cout =\", cnn.Cout)\n",
    "print(\"F =\", cnn.F)\n",
    "print(\"S =\", cnn.S)\n",
    "print(\"P =\", cnn.P)\n",
    "print(\"重みとバイアスはこっちに変更\")\n",
    "cnn.W = w\n",
    "cnn.B = b\n",
    "print(\"W\", cnn.W)\n",
    "print(\"B\", cnn.B)\n",
    "\n",
    "# forward\n",
    "print(\"=====forward\")\n",
    "a = cnn.forward(x)\n",
    "print(\"A\", a)\n",
    "\n",
    "\n",
    "# lossがこうだとすると、、、dxはこうなるはず（dx_true）\n",
    "if n_pad == 0:\n",
    "    da = np.array([[[10, 10], [20, 40], [30, 10]]], dtype=float)\n",
    "    dx_true = np.array([[90, 210, 210, 120], [90, 210, 210, 120]], dtype=float)\n",
    "elif n_pad == 1:\n",
    "    da = np.array([[10, 10, 10, 10],\n",
    "                   [20, 40, 20, 40],\n",
    "                   [30, 10, 30, 10]], dtype=float)\n",
    "elif n_pad == 2:\n",
    "    da = np.array([[[10, 10, 10, 10, 10, 10], [20, 40, 20, 40, 20, 40], [30, 10, 30, 10, 30, 10]],\n",
    "                   [[10, 10, 10, 10, 10, 10], [20, 40, 20, 40, 20, 40], [30, 10, 30, 10, 30, 10]],\n",
    "                   [[10, 10, 10, 10, 10, 10], [20, 40, 20, 40, 20, 40], [30, 10, 30, 10, 30, 10]],\n",
    "                   [[10, 10, 10, 10, 10, 10], [20, 40, 20, 40, 20, 40], [30, 10, 30, 10, 30, 10]],\n",
    "                   [[10, 10, 10, 10, 10, 10], [20, 40, 20, 40, 20, 40], [30, 10, 30, 10, 30, 10]],\n",
    "                  ], dtype=float)\n",
    "\n",
    "print(\"dA\", da.shape)\n",
    "# backward\n",
    "print(\"=====backward\")\n",
    "dx = cnn.backward(da)\n",
    "print(\"dW\", cnn.dW)\n",
    "print(\"dB\", cnn.dB)\n",
    "print(\"dX\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題7】任意のストライド数"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ストライドは1限定の実装をしてきましたが、任意のストライド数に対応できるようにしてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializerForConv1d:\n",
    "    \"\"\"Heの初期値\"\"\"\n",
    "    def W(self, Nin, F, Cin, Cout):\n",
    "        \"\"\"重みの初期化\n",
    "        :parameters\n",
    "            Nin (int): 入力のサイズ\n",
    "            F (int): フィルターサイズ\n",
    "            Cin (int): 入力チャンネル数\n",
    "            Cout (int): 出力チャンネル数\n",
    "        :returns\n",
    "            W (3d-array, (Cout, Cin, F)): ランダムに発生させた重み\n",
    "        \"\"\"\n",
    "        return np.sqrt(2.0 / (Cin*Nin)) * np.random.randn(Cout, Cin, F)\n",
    "    \n",
    "    def B(self, Nin, Cin, Cout):\n",
    "        \"\"\"バイアスの初期化\n",
    "        :parameters\n",
    "            Nin (int): 入力のサイズ\n",
    "            Cout (int): 出力チャンネル数\n",
    "        :returns\n",
    "            B (1d-array, (Cout,)): ランダムに発生させたバイアス\n",
    "        \"\"\"\n",
    "        return np.sqrt(2.0 / (Cin*Nin)) * np.random.randn(Cout)\n",
    "\n",
    "class HeInitializerForAffine:\n",
    "    \"\"\"Heの初期値\"\"\"\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"重みの初期化\n",
    "        :parameters\n",
    "            n_nodes1 (int): 前の層のノード数\n",
    "            n_nodes2 (int): 後の層のノード数\n",
    "        :returns\n",
    "            W (2d-array, (n_nodes1, n_nodes2)): ランダムに発生させた重み\n",
    "        \"\"\"\n",
    "        return np.sqrt(2.0 / n_nodes1) * np.random.randn(n_nodes1, n_nodes2)\n",
    "    \n",
    "    def B(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"バイアスの初期化\n",
    "        :parameters\n",
    "            n_nodes2 (int): 後の層のノード数\n",
    "        :returns\n",
    "            B (1d-array, (n_nodes2, )): ランダムに発生させたバイアス\n",
    "        \"\"\"\n",
    "        return np.sqrt(2.0 / n_nodes1) * np.random.randn(1, n_nodes2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv1d:\n",
    "    \"\"\"チャンネル数を限定しない１次元畳み込み（データ１次元、パディングなし、ストライド１）\n",
    "    :parameters\n",
    "        Nin (int): 入力サイズ（特徴量の数）\n",
    "        Cin (int): 入力チャンネル数（色の次元の数？）\n",
    "        Cout (int): 出力チャンネル数（フィルター枚数）\n",
    "        F (int): フィルター（重み）のサイズ（正方形と仮定）\n",
    "        S (int): ストライド(今回は1)\n",
    "        P (int): パディング（今回はなし）\n",
    "        initializer (class instance): 初期化方法のインスタンス\n",
    "        optimizer (class instance): 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, Cout, F, S=1, P=0, pad_method=\"constant\", initializer=HeInitializerForConv1d(), optimizer=SGD()):\n",
    "        # 重み初期化方法・最適化手法\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        # フィルター形状・畳み込みの方法（stride, padding）などを取得\n",
    "        self.Cout = Cout\n",
    "        self.F = F\n",
    "        self.S = S\n",
    "        self.P = P\n",
    "        self.pad_method = pad_method\n",
    "        # 重みとバイアス\n",
    "        self.W = None\n",
    "        self.B = None\n",
    "        self.is_first_forward = True\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"順伝播\n",
    "        :parameters\n",
    "            X (3d-array, (Ndata, Cin, Nin)): 入力\n",
    "        :returns\n",
    "            A (3d-array, (Ndata, Cout, Nout)): 出力\n",
    "        \"\"\"\n",
    "        # データ数・形状等の取得\n",
    "        # Cout, F, S, Pはインスタンス化した時に取得済み\n",
    "        self.Ndata = X.shape[0]  # データ数\n",
    "        self.Cin = X.shape[1]  # 入力チャンネル数\n",
    "        self.Nin = X.shape[2]  # 入力特徴量数\n",
    "        self.Nout = self.calc_output_size()  # 出力サイズ（出力特徴量数）\n",
    "        \n",
    "        # パディング\n",
    "        self.X = np.pad(X, [(0, 0), (0, 0), (self.P, self.P)], self.pad_method)\n",
    "\n",
    "        # １回目のforwardでは、重みとバイアスを初期化する（テスト時はコメントアウトする）\n",
    "        if self.is_first_forward:\n",
    "            self.W = self.initializer.W(self.Nin, self.F, self.Cin, self.Cout)\n",
    "            self.B = self.initializer.B(self.Nin, self.Cin, self.Cout)\n",
    "            self.is_first_forward = False\n",
    "        \n",
    "        # 出力格納用\n",
    "        A = np.zeros((self.Ndata, self.Cout, self.Nout))\n",
    "\n",
    "        # 要素ごとにSずつずらしながら、アダマール積して合計する\n",
    "        for i in range(self.Nout):\n",
    "            # 畳み込み： X*W + Bを計算する\n",
    "            # X(Ndata, Cin, Nin)のうち、スライスでサイズFだけ取得する。 --> X(Ndata, Cin, F)\n",
    "            # X(Ndata, Cin, F)と、W(Cout, Cin, F)のアダマール積を計算するが、\n",
    "            # このとき、Xを、WのCout方向にブロードキャストしたいので、newaxisで X(Ndata, 1, Cin, F)とする。\n",
    "            # これで、XW(Ndata, Cout, Cin, F)になる。\n",
    "            p = i * self.S\n",
    "            XW = self.X[:, np.newaxis, :, p:p+self.F] * self.W\n",
    "            \n",
    "            # XWを(Cin, F)方向に合計する\n",
    "            # その結果、np.sum(XW)は(Ndata, Cout)となる。\n",
    "            # これとB(Cout,)との和をとると、ブロードキャストされて、(Ndata, Cout)となる。\n",
    "            A[:, :, i] = np.sum(XW, axis=(2, 3)) + self.B\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"逆伝播\n",
    "        :parameters\n",
    "            dA (3d-array, (Ndata, Cout, Nout)): 出力の勾配\n",
    "        :returns\n",
    "            dX (3d-array, (Ndata, Cin, Nin)): 入力の勾配\n",
    "        :returns\n",
    "        \"\"\"\n",
    "        # dB(Cout,)\n",
    "        self.dB = np.sum(dA, axis=(0, 2))\n",
    "        \n",
    "        # dW(Cout, Cin, F)\n",
    "        self.dW = np.zeros((self.Cout, self.Cin, self.F))\n",
    "        for i in range(self.F):\n",
    "            # X(Ndata, Cin, Nin) x dA(Ndata, Cout, Nout)を計算して、dW(Cout, Cin, F)にしたい。\n",
    "            # スライスして、X[](Ndata, Cin, Nout) x dA(Ndata, Nout, Cout) = XdA(Ndata, Cin, Cout)\n",
    "            XdA = np.matmul(self.X[:, :, i:i+self.S*self.Nout:self.S], dA.transpose(0, 2, 1))\n",
    "            \n",
    "            # これをNdata方向に合計すると、sumXdA(Cin, Cout)となる。\n",
    "            sumXdA = np.sum(XdA, axis=0)\n",
    "            \n",
    "            # dW(Cout, Cin, F)に格納するために、XdA(Cin, Cout)を転置して、(Cout, Cin)にする。\n",
    "            self.dW[:, :, i] = sumXdA.T\n",
    "        \n",
    "        # dX(Ndata, Cin, Nin)を求める部分\n",
    "        # 最初に、W(Cout, Cin, F)からtile_W(Cin, Cout, Nout, Nin)を作る\n",
    "        # 一つずつずらしながら、Wをタイルする。\n",
    "        tile_W = np.zeros((self.Cin, self.Cout, self.Nout, self.Nin + 2*self.P))\n",
    "        for i in range(self.Nout):\n",
    "            p = i * self.S\n",
    "            tile_W[:, :, i, p:p+self.F] = self.W.transpose(1, 0, 2)  # W(Cout, Cin, F) --> W(Cin, Cout, F)\n",
    "\n",
    "        # dA(Ndata, Cout, Nout) を tile_W(Cin, Cout, Nout, Nin+2P) と積和する\n",
    "        # dA     (Ndata,   1, Cout, Nout,      1)\n",
    "        # tile_W (    1, Cin, Cout, Nout, Nin+2P)\n",
    "        # アダマール積すると、\n",
    "        # dAW    (Ndata, Cin, Cout, Nout, Nin+2P)\n",
    "        dAW = dA[:, np.newaxis, :, :, np.newaxis] * tile_W[np.newaxis, ...]\n",
    "        \n",
    "        # dAW(Ndata, Cin, Cout, Nout, Nin+2P)のうち、(Cout, Nout)方向で合計をとる\n",
    "        # その結果、dX(Ndata, Cin, Nin+2P)となる。\n",
    "        dX = np.sum(dAW, axis=(2, 3))\n",
    "        \n",
    "        # パディングした部分を削除して\n",
    "        # dX(Ndata, Cin, Nin+2P) --> dX(Ndata, Cin, Nin)とする。\n",
    "        if self.P > 0:\n",
    "            dX = dX[:, :, self.P:-self.P]\n",
    "        \n",
    "        # 勾配をもとに重みとバイアスを更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dX\n",
    "\n",
    "    def calc_output_size(self):\n",
    "        return (self.Nin + 2*self.P - self.F) // self.S + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (5, 2, 7)\n",
      "Cout = 3\n",
      "F = 3\n",
      "S = 2\n",
      "P = 1\n",
      "重みとバイアスはこっちに変更\n",
      "W [[[1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]]\n",
      "\n",
      " [[1. 1. 1.]\n",
      "  [1. 1. 1.]]]\n",
      "B [1. 2. 3.]\n",
      "=====forward\n",
      "A (5, 3, 4)\n",
      "dA (5, 3, 4)\n",
      "=====backward\n",
      "dW [[[150. 200. 150.]\n",
      "  [150. 200. 150.]]\n",
      "\n",
      " [[500. 600. 400.]\n",
      "  [500. 600. 400.]]\n",
      "\n",
      " [[250. 400. 350.]\n",
      "  [250. 400. 350.]]]\n",
      "dB [200. 600. 400.]\n",
      "dX [[[  2.76169381   6.79854078  -2.72256609  -5.60055129   2.76169381\n",
      "     6.79854078  -2.72256609]\n",
      "  [-14.320249    -3.99400275  -9.34516671  -7.75242074 -14.320249\n",
      "    -3.99400275  -9.34516671]]\n",
      "\n",
      " [[  2.76169381   6.79854078  -2.72256609  -5.60055129   2.76169381\n",
      "     6.79854078  -2.72256609]\n",
      "  [-14.320249    -3.99400275  -9.34516671  -7.75242074 -14.320249\n",
      "    -3.99400275  -9.34516671]]\n",
      "\n",
      " [[  2.76169381   6.79854078  -2.72256609  -5.60055129   2.76169381\n",
      "     6.79854078  -2.72256609]\n",
      "  [-14.320249    -3.99400275  -9.34516671  -7.75242074 -14.320249\n",
      "    -3.99400275  -9.34516671]]\n",
      "\n",
      " [[  2.76169381   6.79854078  -2.72256609  -5.60055129   2.76169381\n",
      "     6.79854078  -2.72256609]\n",
      "  [-14.320249    -3.99400275  -9.34516671  -7.75242074 -14.320249\n",
      "    -3.99400275  -9.34516671]]\n",
      "\n",
      " [[  2.76169381   6.79854078  -2.72256609  -5.60055129   2.76169381\n",
      "     6.79854078  -2.72256609]\n",
      "  [-14.320249    -3.99400275  -9.34516671  -7.75242074 -14.320249\n",
      "    -3.99400275  -9.34516671]]]\n"
     ]
    }
   ],
   "source": [
    "x = np.ones((5, 2, 7))\n",
    "\"\"\"\n",
    "x = np.array([[[1, 2, 3, 4], [2, 3, 4, 5]],\n",
    "              [[2, 3, 4, 5], [3, 4, 5, 6]],\n",
    "              [[3, 4, 5, 6], [4, 5, 6, 7]],\n",
    "              [[4, 5, 6, 7], [5, 6, 7, 8]],\n",
    "              [[5, 6, 7, 8], [6, 7, 8, 9]],\n",
    "             ], dtype=float)\n",
    "\"\"\"\n",
    "print(\"X\", x.shape)\n",
    "w = np.ones((3, 2, 3), dtype=float)\n",
    "b = np.array([1, 2, 3], dtype=float)\n",
    "\n",
    "n_pad = 1\n",
    "n_stride = 2\n",
    "cnn = Conv1d(Cout=w.shape[0], F=w.shape[2], S=n_stride, P=n_pad)\n",
    "print(\"Cout =\", cnn.Cout)\n",
    "print(\"F =\", cnn.F)\n",
    "print(\"S =\", cnn.S)\n",
    "print(\"P =\", cnn.P)\n",
    "print(\"重みとバイアスはこっちに変更\")\n",
    "cnn.W = w\n",
    "cnn.B = b\n",
    "print(\"W\", cnn.W)\n",
    "print(\"B\", cnn.B)\n",
    "\n",
    "# forward\n",
    "print(\"=====forward\")\n",
    "a = cnn.forward(x)\n",
    "print(\"A\", a.shape)\n",
    "\n",
    "# da\n",
    "da = np.array([[[10, 10, 10, 10], [20, 40, 20, 40], [30, 10, 30, 10]],\n",
    "               [[10, 10, 10, 10], [20, 40, 20, 40], [30, 10, 30, 10]],\n",
    "               [[10, 10, 10, 10], [20, 40, 20, 40], [30, 10, 30, 10]],\n",
    "               [[10, 10, 10, 10], [20, 40, 20, 40], [30, 10, 30, 10]],\n",
    "               [[10, 10, 10, 10], [20, 40, 20, 40], [30, 10, 30, 10]],\n",
    "              ], dtype=float)\n",
    "print(\"dA\", da.shape)\n",
    "# backward\n",
    "print(\"=====backward\")\n",
    "dx = cnn.backward(da)\n",
    "print(\"dW\", cnn.dW)\n",
    "print(\"dB\", cnn.dB)\n",
    "print(\"dX\", dx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題8】学習と推定"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "これまで使ってきたニューラルネットワークの全結合層の一部をConv1dに置き換えてMNISTを学習・推定し、Accuracyを計算してください。  \n",
    "出力層だけは全結合層をそのまま使ってください。ただし、チャンネルが複数ある状態では全結合層への入力は行えません。  \n",
    "その段階でのチャンネルは1になるようにするか、 平滑化 を行なってください。  \n",
    "画像に対しての1次元畳み込みは実用上は行わないことのため、精度は問いません。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GetMiniBatch:\n",
    "    \"\"\"ミニバッチを取得するイテレータ\n",
    "    :parameters\n",
    "        X (2d-array, (n_samples, n_features)): 訓練データ\n",
    "        y (1d-array, (n_samples, 1)): 正解値\n",
    "        batch_size (int): バッチサイズ\n",
    "        seed (int): 乱数のシード\n",
    "    \"\"\"\n",
    "    def __init__(self, X, y, batch_size=20, seed=0):\n",
    "        self.batch_size = batch_size\n",
    "        np.random.seed(seed)\n",
    "        shuffle_index = np.random.permutation(np.arange(X.shape[0]))\n",
    "        self._X = X[shuffle_index]\n",
    "        self._y = y[shuffle_index]\n",
    "        self._stop = np.ceil(X.shape[0]/self.batch_size).astype(np.int)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self._stop\n",
    "    \n",
    "    def __getitem__(self, item):\n",
    "        p0 = item*self.batch_size\n",
    "        p1 = item*self.batch_size + self.batch_size\n",
    "        return self._X[p0:p1], self._y[p0:p1]\n",
    "    \n",
    "    def __iter__(self):\n",
    "        self._counter = 0\n",
    "        return self\n",
    "    \n",
    "    def __next__(self):\n",
    "        if self._counter >= self._stop:\n",
    "            raise StopIteration()\n",
    "        p0 = self._counter*self.batch_size\n",
    "        p1 = self._counter*self.batch_size + self.batch_size\n",
    "        self._counter += 1\n",
    "        return self._X[p0:p1], self._y[p0:p1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReLU:\n",
    "    \"\"\"rectified linear unit関数（活性化関数）\"\"\"\n",
    "    def forward(self, A):\n",
    "        self.is_positive = A > 0\n",
    "        Z = self.is_positive * A\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ):\n",
    "        if dZ.shape == self.is_positive.shape:\n",
    "            dA = self.is_positive * dZ\n",
    "        else:\n",
    "            dA = self.is_positive * dZ[:, np.newaxis, :]\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Softmax:\n",
    "    \"\"\"softmax関数\n",
    "    backwardは交差エントロピー誤差関数と合わせて計算する\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        y_pred = np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
    "        # backward用にy_predを保持\n",
    "        self.y_pred = y_pred\n",
    "        return y_pred\n",
    "    \n",
    "    def backward(self, y_true):\n",
    "        dA = self.y_pred - y_true\n",
    "        return dA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FC:\n",
    "    \"\"\"ノード数n_nodes1からn_nodes2への全結合層\n",
    "    :parameters\n",
    "        n_node1 (int): 前層のノード数\n",
    "        n_node2 (int): 後層のノード数\n",
    "        initializer (class instance): 初期化方法のインスタンス\n",
    "        optimizer (class instance): 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_nodes2, initializer=HeInitializerForAffine(), optimizer=SGD()):\n",
    "        self.initializer = initializer\n",
    "        self.optimizer = optimizer\n",
    "        self.n_nodes2 = n_nodes2\n",
    "        self.is_first_forward = True\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"順伝播\n",
    "        :parameters\n",
    "            X (2d-array, (batch_size, n_nodes1)): 入力\n",
    "        :returns\n",
    "            A (2d-array, (batch_size, n_nodes2)): 出力\n",
    "        \"\"\"\n",
    "        # チャネル数が残っているときは削除する\n",
    "        if X.ndim > 2:\n",
    "            X = np.squeeze(X, 1)\n",
    "\n",
    "        # 形状取得\n",
    "        n_nodes1 = X.shape[1]\n",
    "        n_nodes2 = self.n_nodes2\n",
    "        \n",
    "        # 初めてforwardを実行するときは、重みとバイアスを初期化\n",
    "        if self.is_first_forward:\n",
    "            self.W = self.initializer.W(n_nodes1, n_nodes2)\n",
    "            self.B = self.initializer.B(n_nodes1, n_nodes2)\n",
    "            self.is_first_forward = False\n",
    "\n",
    "        # 線形結合\n",
    "        A = np.matmul(X, self.W) + self.B\n",
    "        \n",
    "        # backward用にXを保持\n",
    "        self.X = X\n",
    "        self.Ndata = X.shape[0]\n",
    "        return A\n",
    "    \n",
    "    def backward(self, dA):\n",
    "        \"\"\"逆伝播\n",
    "        :parameters\n",
    "            dA (2d-array, (batch_size, n_nodes2)): 後層から戻ってきた勾配\n",
    "        :returns\n",
    "            dZ (2d-array, (batch_size, n_nodes1)): 前層に送る勾配\n",
    "        \"\"\"\n",
    "        self.dB = np.sum(dA, axis=0, keepdims=True)\n",
    "        self.dW = np.matmul(self.X.T, dA)\n",
    "        dX = np.matmul(dA, self.W.T)\n",
    "        # 勾配をもとに重みとバイアスを更新\n",
    "        self = self.optimizer.update(self)\n",
    "        return dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN1dClassifier():\n",
    "    \"\"\"Convolution Neural Network Classifier\n",
    "    :attributes\n",
    "        layers (list of instances): 各層のインスタンスを並べたリスト\n",
    "        n_epoch (int): 繰り返すエポック数\n",
    "        batch_size (int): ミニバッチのデータ数\n",
    "        plot_interval (int): 損失関数を記録する間隔\n",
    "        epoch_interval (int): 何epochごとにprintするか\n",
    "        loss (list): 損失関数の推移（訓練データ）\n",
    "        lossval (list): 損失関数の推移（検証データ）\n",
    "        verbose (bool): 学習過程を出力する場合はTrue\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, n_epoch=10, batch_size=20, plot_interval=1, verbose=True, epoch_interval=1):\n",
    "        self.layers = layers\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.plot_interval = plot_interval\n",
    "        self.epoch_interval = epoch_interval\n",
    "        self.verbose = verbose\n",
    "        self.loss = []\n",
    "        self.loss_val = []\n",
    "    \n",
    "    def fit(self, X, y, X_val=None, y_val=None):\n",
    "        \"\"\"ニューラルネットワーク分類器を学習する。\n",
    "        :parameters\n",
    "            X (2d-array, (n_samples, n_features)): 訓練データの特徴量\n",
    "            y (2d-array, (n_samples, n_classes)): 訓練データの正解値（One-hot）\n",
    "            X_val (2d-array, (n_samples, n_features)): 検証データの特徴量\n",
    "            y_val (2d-array, (n_samples, n_classes)): 検証データの正解値（One-hot）\n",
    "            optimizer (instance): 最適化手法のインスタンス\n",
    "        \"\"\"\n",
    "        # データチェック\n",
    "        self.n_features = X.shape[1]\n",
    "        self.n_output = y.shape[1]\n",
    "\n",
    "        # 学習\n",
    "        for epoch in range(self.n_epoch):\n",
    "            # エポックごとに乱数を変えて、ミニバッチを取得する\n",
    "            get_mini_batch = GetMiniBatch(X, y, batch_size=self.batch_size, seed=epoch)\n",
    "            \n",
    "            # ミニバッチごとに学習をすすめる。\n",
    "            for mini_X_train, mini_y_train in get_mini_batch:\n",
    "                # forward\n",
    "                data = mini_X_train\n",
    "                for layer in self.layers:\n",
    "                    data = layer.forward(data)\n",
    "\n",
    "                # backward\n",
    "                grad = mini_y_train\n",
    "                for layer in reversed(self.layers):\n",
    "                    grad = layer.backward(grad)\n",
    "                \n",
    "            # 損失（交差エントロピー誤差）を計算して保持\n",
    "            y_pred = self.predict_proba(X)  # 分類カテゴリーごとの確率を計算\n",
    "            L = self.cross_entropy_error(y_pred, y)\n",
    "            self.loss.append(L)\n",
    "            # 検証データありの場合はそちらの損失も保持\n",
    "            if X_val is not None and y_val is not None:\n",
    "                y_pred_val = self.predict_proba(X_val)  # 分類カテゴリーごとの確率を計算\n",
    "                L_val = self.cross_entropy_error(y_pred_val, y_val)\n",
    "                self.loss_val.append(L_val)\n",
    "\n",
    "            # 学習過程の出力（エポックごと）\n",
    "            if self.verbose and (epoch % self.epoch_interval == 0):\n",
    "                str_loss = \"Epoch\" + str(epoch).rjust(4)\n",
    "                str_loss += \"\\tLoss {:.5f}\".format(self.loss[-1])\n",
    "                if len(self.loss_val) != 0:\n",
    "                    str_loss += \"\\tLoss_valid {:.5f}\".format(self.loss_val[-1])\n",
    "                print(str_loss)\n",
    "\n",
    "    def cross_entropy_error(self, y_pred, y_true):\n",
    "        \"\"\"交差エントロピー誤差の計算\n",
    "        :parameters\n",
    "            y_true (2d-array, (n_samples, n_classes)): クラスの正解ラベル（one-hotなので0 or 1）\n",
    "            y_pred (2d-array, (n_samples, n_classes)): クラス分類の予測確率（yと同じ形状）\n",
    "        :returns\n",
    "            cee (float): 交差エントロピー誤差\n",
    "        \"\"\"\n",
    "        cee = -1/y_pred.shape[0] * np.sum(y_true * np.log(y_pred))\n",
    "        return cee\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"ニューラルネットワーク分類器を使い推定する。推定結果は確率のOne-hot表現。\n",
    "        :parameters\n",
    "            X (2d-array, (n_samples, n_features)): サンプルデータ\n",
    "        :returns\n",
    "            y_pred (2d-array, (n_samples, n_classes)): 推定確率\n",
    "        \"\"\"\n",
    "        # forward\n",
    "        data = X\n",
    "        for layer in self.layers:\n",
    "            data = layer.forward(data)\n",
    "        y_pred = data\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"ニューラルネットワーク分類器を使い推定する。\n",
    "        :parameters\n",
    "            X (2d-array, (n_samples, n_features)): サンプル\n",
    "        :returns\n",
    "            label (1d-array, (n_samples,)): 推定ラベル\n",
    "        \"\"\"\n",
    "        probability = self.predict_proba(X)\n",
    "        label = np.argmax(probability, axis=1)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_accuracy(nn):\n",
    "    print(\"Validation Data\")\n",
    "    y_pred = nn.predict(X_val)\n",
    "    y_true = np.argmax(y_val, axis=1)\n",
    "    print(y_pred.shape)\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    print(\"accuracy = {:.5f}\".format(acc))\n",
    "\n",
    "    print()\n",
    "    print(\"Test Data\")\n",
    "    y_pred_test = nn.predict(X_test)\n",
    "    y_true_test = y_test\n",
    "    print(y_pred_test.shape)\n",
    "    acc_test = accuracy_score(y_true_test, y_pred_test)\n",
    "    print(\"accuracy = {:.5f}\".format(acc_test))\n",
    "\n",
    "def draw_loss(nn):\n",
    "    fig, ax = plt.subplots(figsize=(5, 5))\n",
    "    x1 = np.arange(len(nn.loss))\n",
    "    y1 = nn.loss\n",
    "    x2 = np.arange(len(nn.loss_val))\n",
    "    y2 = nn.loss_val\n",
    "\n",
    "    ax.plot(x1, y1, label=\"train\", alpha=0.7)\n",
    "    ax.plot(x2, y2, label=\"valid\", alpha=0.7)\n",
    "\n",
    "    ax.set_xlabel(\"Epoch\", fontsize=20)\n",
    "    ax.set_ylabel(\"Loss\", fontsize=20)\n",
    "    ax.set_yscale(\"log\")\n",
    "    ax.set_xlim(-1, nn.n_epoch + 1)\n",
    "    ax.set_ylim(1e-4, 5)\n",
    "    ax.legend(frameon=False)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0\tLoss 0.80269\tLoss_valid 0.79677\n",
      "Epoch   3\tLoss 0.27751\tLoss_valid 0.28471\n",
      "Epoch   6\tLoss 0.19473\tLoss_valid 0.23311\n",
      "Epoch   9\tLoss 0.13292\tLoss_valid 0.19507\n",
      "Epoch  12\tLoss 0.09640\tLoss_valid 0.17913\n",
      "Epoch  15\tLoss 0.06896\tLoss_valid 0.16928\n",
      "Epoch  18\tLoss 0.05416\tLoss_valid 0.16956\n",
      "Epoch  21\tLoss 0.03492\tLoss_valid 0.16098\n",
      "Epoch  24\tLoss 0.02433\tLoss_valid 0.16518\n",
      "Epoch  27\tLoss 0.01758\tLoss_valid 0.16639\n",
      "CPU times: user 54min 20s, sys: 5min 35s, total: 59min 56s\n",
      "Wall time: 15min 13s\n",
      "Validation Data\n",
      "(12000,)\n",
      "accuracy = 0.95567\n",
      "\n",
      "Test Data\n",
      "(10000,)\n",
      "accuracy = 0.95870\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "layers = [\n",
    "    Conv1d(1, 5),\n",
    "    ReLU(),\n",
    "    FC(100),\n",
    "    ReLU(),\n",
    "    FC(y_train.shape[1]),\n",
    "    Softmax(),\n",
    "]\n",
    "\n",
    "cnn = CNN1dClassifier(layers, n_epoch=30, epoch_interval=3)\n",
    "%time cnn.fit(X_train[:10000], y_train[:10000], X_val[:5000], y_val[:5000])\n",
    "calc_accuracy(cnn)\n",
    "draw_loss(cnn)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
