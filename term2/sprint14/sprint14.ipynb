{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題1】公式チュートリアルモデルを分担して実行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFLowの公式チュートリアルモデルを分担して実行してください。  \n",
    "以下の中から1人ひとつ選び実行し、その結果を簡単に発表してください。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "選んだチュートリアル:'画像:データの拡張'  \n",
    "https://www.tensorflow.org/tutorials/images/data_augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題3】Iris（2値分類）をKerasで学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowによるIrisデータセットに対する2値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from keras.datasets import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\t (64, 4) float64\n",
      "X_valid\t (16, 4) float64\n",
      "X_test\t (20, 4)\n",
      "y_train\t (64, 1)\n",
      "y_valid\t (16, 1)\n",
      "y_test\t (20, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_iris():\n",
    "    # データセットの読み込み\n",
    "    dataset_path = \"Iris.csv\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    # データフレームから条件抽出\n",
    "    df = df[(df[\"Species\"]==\"Iris-versicolor\")|(df[\"Species\"]==\"Iris-virginica\")]\n",
    "    y = df[\"Species\"]\n",
    "    X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "    # ラベルを数値に変換\n",
    "    y[y==\"Iris-versicolor\"] = 0\n",
    "    y[y==\"Iris-virginica\"] = 1\n",
    "    y = y.astype(np.int)[:, np.newaxis]\n",
    "    # trainとtestに分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    # さらにtrainとvalに分割\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = get_iris()\n",
    "print(\"X_train\\t\", X_train.shape, X_train.dtype)\n",
    "print(\"X_valid\\t\", X_valid.shape, X_valid.dtype)\n",
    "print(\"X_test\\t\", X_test.shape)\n",
    "print(\"y_train\\t\", y_train.shape)\n",
    "print(\"y_valid\\t\", y_valid.shape)\n",
    "print(\"y_test\\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model Summary >>>>>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                250       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 1,551\n",
      "Trainable params: 1,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Compile >>>>>\n",
      "WARNING:tensorflow:From /opt/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      ">>>>> Complete\n",
      "Fit >>>>>\n",
      "fit time: 2.5811073780059814\n",
      ">>>>> Complete\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from keras import backend as K\n",
    "K.clear_session()\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\", input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(25, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"sigmoid\"),\n",
    "])\n",
    "print(\"Model Summary >>>>>\")\n",
    "model.summary()\n",
    "print(\"Compile >>>>>\")\n",
    "model.compile(loss=\"binary_crossentropy\",\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "print(\">>>>> Complete\")\n",
    "\n",
    "print(\"Fit >>>>>\")\n",
    "st = time.time()\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=10,\n",
    "                    epochs=100,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "ed = time.time()\n",
    "print(\"fit time:\", ed - st)\n",
    "print(\">>>>> Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Train=====\n",
      "Loss: 0.05796097032725811\n",
      "Accuracy: 0.984375\n",
      "\n",
      "=====Validation=====\n",
      "Loss: 0.0674087405204773\n",
      "Accuracy: 0.9375\n",
      "\n",
      "=====Test=====\n",
      "Test loss: 0.20061512291431427\n",
      "Test accuracy: 0.95\n"
     ]
    }
   ],
   "source": [
    "print(\"=====Train=====\")\n",
    "score_train = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Loss:\", score_train[0])\n",
    "print(\"Accuracy:\", score_train[1])\n",
    "\n",
    "print(\"\\n=====Validation=====\")\n",
    "score_valid = model.evaluate(X_valid, y_valid, verbose=False)\n",
    "print(\"Loss:\", score_valid[0])\n",
    "print(\"Accuracy:\", score_valid[1])\n",
    "\n",
    "print(\"\\n=====Test=====\")\n",
    "score_test = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Test loss:\", score_test[0])\n",
    "print(\"Test accuracy:\", score_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題4】Iris（多値分類）をKerasで学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowによるIrisデータセットに対する3値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train\t (96, 4)\n",
      "y_train\t (96, 3)\n",
      "X_valid\t (24, 4)\n",
      "y_valid\t (24, 3)\n",
      "X_test\t (30, 4)\n",
      "y_test\t (30, 3)\n"
     ]
    }
   ],
   "source": [
    "def get_all_iris():\n",
    "    # データセットの読み込み\n",
    "    dataset_path = \"Iris.csv\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    # データフレームから条件抽出\n",
    "    y = df[\"Species\"]\n",
    "    X = df.loc[:, [\"SepalLengthCm\", \"SepalWidthCm\", \"PetalLengthCm\", \"PetalWidthCm\"]]\n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "\n",
    "    # One-hot Encoding\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    y = enc.fit_transform(y[:, np.newaxis])\n",
    "\n",
    "    # trainとtestに分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    # さらにtrainとvalに分割\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "    return X_train, y_train, X_valid, y_valid, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_valid, y_valid, X_test, y_test = get_all_iris()\n",
    "print(\"X_train\\t\", X_train.shape)\n",
    "print(\"y_train\\t\", y_train.shape)\n",
    "print(\"X_valid\\t\", X_valid.shape)\n",
    "print(\"y_valid\\t\", y_valid.shape)\n",
    "print(\"X_test\\t\", X_test.shape)\n",
    "print(\"y_test\\t\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary >>>>>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                250       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 78        \n",
      "=================================================================\n",
      "Total params: 1,603\n",
      "Trainable params: 1,603\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Compile >>>>>\n",
      ">>>>> Complete\n",
      "Fit >>>>>\n",
      "fit time: 3.472994804382324\n",
      ">>>>> Complete\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\", input_shape=(4,)),\n",
    "    tf.keras.layers.Dense(25, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(3, activation=\"softmax\"),\n",
    "])\n",
    "print(\"Model Summary >>>>>\")\n",
    "model.summary()\n",
    "print(\"Compile >>>>>\")\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "print(\">>>>> Complete\")\n",
    "\n",
    "print(\"Fit >>>>>\")\n",
    "st = time.time()\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=10,\n",
    "                    epochs=100,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "ed = time.time()\n",
    "print(\"fit time:\", ed - st)\n",
    "print(\">>>>> Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Train=====\n",
      "Loss: 0.032585891584555306\n",
      "Accuracy: 0.9791667\n",
      "\n",
      "=====Validation=====\n",
      "Loss: 0.2864319384098053\n",
      "Accuracy: 0.9166667\n",
      "\n",
      "=====Test=====\n",
      "Test loss: 0.017435209825634956\n",
      "Test accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(\"=====Train=====\")\n",
    "score_train = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Loss:\", score_train[0])\n",
    "print(\"Accuracy:\", score_train[1])\n",
    "\n",
    "print(\"\\n=====Validation=====\")\n",
    "score_valid = model.evaluate(X_valid, y_valid, verbose=False)\n",
    "print(\"Loss:\", score_valid[0])\n",
    "print(\"Accuracy:\", score_valid[1])\n",
    "\n",
    "print(\"\\n=====Test=====\")\n",
    "score_test = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Test loss:\", score_test[0])\n",
    "print(\"Test accuracy:\", score_test[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題5】House PricesをKerasで学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowによるHouse Pricesデータセットに対する回帰をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std_X_train\t (934, 2)\n",
      "log_y_train\t (934, 1)\n",
      "std_X_valid\t (234, 2)\n",
      "log_y_valid\t (234, 1)\n",
      "std_X_test\t (292, 2)\n",
      "log_y_test\t (292, 1)\n"
     ]
    }
   ],
   "source": [
    "def get_house_price():\n",
    "    # データセットの読み込み\n",
    "    dataset_path = \"train.csv\"\n",
    "    df = pd.read_csv(dataset_path)\n",
    "    # データフレームから条件抽出\n",
    "    y = df[\"SalePrice\"]\n",
    "    X = df.loc[:, [\"GrLivArea\", \"YearBuilt\"]]\n",
    "    y = np.array(y)\n",
    "    X = np.array(X)\n",
    "    # trainとtestに分割\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "    # さらにtrainとvalに分割\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.2, random_state=0)\n",
    "\n",
    "    # 標準化\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(X_train)\n",
    "    std_X_train = scaler.transform(X_train)\n",
    "    std_X_valid = scaler.transform(X_valid)\n",
    "    std_X_test = scaler.transform(X_test)\n",
    "    # 対数変換\n",
    "    log_y_train = np.log(y_train)[:, np.newaxis]\n",
    "    log_y_valid = np.log(y_valid)[:, np.newaxis]\n",
    "    log_y_test = np.log(y_test)[:, np.newaxis]\n",
    "    return std_X_train, log_y_train, std_X_valid, log_y_valid, std_X_test, log_y_test\n",
    "\n",
    "std_X_train, log_y_train, std_X_valid, log_y_valid, std_X_test, log_y_test = get_house_price()\n",
    "print(\"std_X_train\\t\", std_X_train.shape)\n",
    "print(\"log_y_train\\t\", log_y_train.shape)\n",
    "print(\"std_X_valid\\t\", std_X_valid.shape)\n",
    "print(\"log_y_valid\\t\", log_y_valid.shape)\n",
    "print(\"std_X_test\\t\", std_X_test.shape)\n",
    "print(\"log_y_test\\t\", log_y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary >>>>>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 50)                150       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 25)                1275      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 26        \n",
      "=================================================================\n",
      "Total params: 1,451\n",
      "Trainable params: 1,451\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Compile >>>>>\n",
      ">>>>> Complete\n",
      "Fit >>>>>\n",
      "fit time: 15.889699459075928\n",
      ">>>>> Complete\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(50, activation=\"relu\", input_shape=(2,)),\n",
    "    tf.keras.layers.Dense(25, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(1, activation=\"linear\"),\n",
    "])\n",
    "print(\"Model Summary >>>>>\")\n",
    "model.summary()\n",
    "print(\"Compile >>>>>\")\n",
    "model.compile(loss=\"mean_squared_error\",\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01))\n",
    "print(\">>>>> Complete\")\n",
    "\n",
    "print(\"Fit >>>>>\")\n",
    "st = time.time()\n",
    "history = model.fit(std_X_train, log_y_train,\n",
    "                    batch_size=10,\n",
    "                    epochs=100,\n",
    "                    verbose=False,\n",
    "                    validation_data=(std_X_valid, log_y_valid))\n",
    "ed = time.time()\n",
    "print(\"fit time:\", ed - st)\n",
    "print(\">>>>> Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Train=====\n",
      "Loss: 0.07396267387992055\n",
      "\n",
      "=====Validation=====\n",
      "Loss: 0.07770700889647517\n",
      "\n",
      "=====Test=====\n",
      "Test loss: 0.06840348692789469\n"
     ]
    }
   ],
   "source": [
    "print(\"=====Train=====\")\n",
    "score_train = model.evaluate(std_X_train, log_y_train, verbose=False)\n",
    "print(\"Loss:\", score_train)\n",
    "\n",
    "print(\"\\n=====Validation=====\")\n",
    "score_valid = model.evaluate(std_X_valid, log_y_valid, verbose=False)\n",
    "print(\"Loss:\", score_valid)\n",
    "\n",
    "print(\"\\n=====Test=====\")\n",
    "score_test = model.evaluate(std_X_test, log_y_test, verbose=False)\n",
    "print(\"Test loss:\", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 【問題6】MNISTをKerasで学習"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TensorFlowによるMNISTデータセットによる画像の多値分類をKerasに書き換えてください。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (48000, 784)\n",
      "X_valid (12000, 784)\n",
      "X_test (10000, 784)\n",
      "y_train (48000, 10)\n",
      "y_valid (12000, 10)\n",
      "y_test (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "def get_mnist():\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "    # 平滑化\n",
    "    X_train = X_train.reshape(-1, 784)\n",
    "    X_test = X_test.reshape(-1, 784)\n",
    "\n",
    "    # 前処理\n",
    "    X_train = X_train.astype(np.float)\n",
    "    X_test = X_test.astype(np.float)\n",
    "    X_train /= 255\n",
    "    X_test /= 255\n",
    "\n",
    "    # チャネル軸を増やす\n",
    "    X_train = X_train[:, :]\n",
    "    X_test = X_test[:, :]\n",
    "\n",
    "    # One-hot Encoding\n",
    "    enc = OneHotEncoder(handle_unknown=\"ignore\", sparse=False)\n",
    "    y_train_one_hot = enc.fit_transform(y_train[:, np.newaxis])\n",
    "    y_test_one_hot = enc.fit_transform(y_test[:, np.newaxis])\n",
    "\n",
    "    # データ分割\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train_one_hot, test_size=0.2, random_state=0)\n",
    "    return X_train, X_valid, X_test, y_train, y_valid, y_test_one_hot\n",
    "\n",
    "X_train, X_valid, X_test, y_train, y_valid, y_test = get_mnist()\n",
    "print(\"X_train\", X_train.shape)\n",
    "print(\"X_valid\", X_valid.shape)\n",
    "print(\"X_test\", X_test.shape)\n",
    "print(\"y_train\", y_train.shape)\n",
    "print(\"y_valid\", y_valid.shape)\n",
    "print(\"y_test\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Summary >>>>>\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 400)               314000    \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 200)               80200     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 10)                2010      \n",
      "=================================================================\n",
      "Total params: 396,210\n",
      "Trainable params: 396,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Compile >>>>>\n",
      ">>>>> Complete\n",
      "Fit >>>>>\n",
      "fit time: 91.4604184627533\n",
      ">>>>> Complete\n"
     ]
    }
   ],
   "source": [
    "K.clear_session()\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Dense(400, activation=\"relu\", input_shape=(784,)),\n",
    "    tf.keras.layers.Dense(200, activation=\"relu\"),\n",
    "    tf.keras.layers.Dense(10, activation=\"softmax\"),\n",
    "])\n",
    "print(\"Model Summary >>>>>\")\n",
    "model.summary()\n",
    "\n",
    "print(\"Compile >>>>>\")\n",
    "model.compile(loss=\"categorical_crossentropy\",\n",
    "              optimizer=tf.train.AdamOptimizer(learning_rate=0.01),\n",
    "              metrics=[\"accuracy\"])\n",
    "print(\">>>>> Complete\")\n",
    "\n",
    "print(\"Fit >>>>>\")\n",
    "st = time.time()\n",
    "history = model.fit(X_train, y_train,\n",
    "                    batch_size=50,\n",
    "                    epochs=20,\n",
    "                    verbose=False,\n",
    "                    validation_data=(X_valid, y_valid))\n",
    "ed = time.time()\n",
    "print(\"fit time:\", ed - st)\n",
    "print(\">>>>> Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====Train=====\n",
      "Loss: 0.08406770292452748\n",
      "Accuracy: 0.9820625\n",
      "\n",
      "=====Validation=====\n",
      "Loss: 0.3098243919020588\n",
      "Accuracy: 0.962\n",
      "\n",
      "=====Test=====\n",
      "Loss: 0.27004300401863773\n",
      "Accuracy: 0.9635\n"
     ]
    }
   ],
   "source": [
    "print(\"=====Train=====\")\n",
    "score_train = model.evaluate(X_train, y_train, verbose=False)\n",
    "print(\"Loss:\", score_train[0])\n",
    "print(\"Accuracy:\", score_train[1])\n",
    "\n",
    "print(\"\\n=====Validation=====\")\n",
    "score_valid = model.evaluate(X_valid, y_valid, verbose=False)\n",
    "print(\"Loss:\", score_valid[0])\n",
    "print(\"Accuracy:\", score_valid[1])\n",
    "\n",
    "print(\"\\n=====Test=====\")\n",
    "score_test = model.evaluate(X_test, y_test, verbose=False)\n",
    "print(\"Loss:\", score_test[0])\n",
    "print(\"Accuracy:\", score_test[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
