{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sprint22 深層学習スクラッチ リカレントニューラルネットワーク"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題１】SimpleRNNのフォワードプロパゲーション実装\n",
    "SimpleRNNのクラスSimpleRNNを作成してください。基本構造はFCクラスと同じになります。\n",
    "\n",
    "\n",
    "フォワードプロパゲーションの数式は以下のようになります。ndarrayのshapeがどうなるかを併記しています。\n",
    "\n",
    "\n",
    "バッチサイズをbatch_size、入力の特徴量数をn_features、RNNのノード数をn_nodesとして表記します。活性化関数はtanhとして進めますが、これまでのニューラルネットワーク同様にReLUなどに置き換えられます。\n",
    "\n",
    "$$\n",
    "a_t = x_{t}\\cdot W_{x} + h_{t-1}\\cdot W_{h} + B\\\\\n",
    "$$\n",
    "\n",
    "$$h_t = tanh(a_t)$$\n",
    "\n",
    "$a_t$ : 時刻tの活性化関数を通す前の状態 (batch_size, n_nodes)  \n",
    "$h_t$ : 時刻tの状態・出力 (batch_size, n_nodes)  \n",
    "$x_{t}$ : 時刻tの入力 (batch_size, n_features)  \n",
    "$W_{x}$ : 入力に対する重み (n_features, n_nodes)  \n",
    "$h_{t-1}$ : 時刻t-1の状態（前の時刻から伝わる順伝播） (batch_size, n_nodes)  \n",
    "$W_{h}$ : 状態に対する重み。 (n_nodes, n_nodes)  \n",
    "$B$ : バイアス項 (n_nodes,)  \n",
    "\n",
    "初期状態 h_{0} は全て0とすることが多いですが、任意の値を与えることも可能です。\n",
    "\n",
    "\n",
    "上記の処理を系列数n_sequences回繰り返すことになります。RNN全体への入力 x は(batch_size, n_sequences, n_features)のような配列で渡されることになり、そこから各時刻の配列を取り出していきます。\n",
    "\n",
    "\n",
    "分類問題であれば、それぞれの時刻のhに対して全結合層とソフトマックス関数（またはシグモイド関数）を使用します。タスクによっては最後の時刻のhだけを使用することもあります。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題２】小さな配列でのフォワードプロパゲーションの実験\n",
    "小さな配列でフォワードプロパゲーションを考えてみます。\n",
    "\n",
    "\n",
    "入力x、初期状態h、重みw_xとw_h、バイアスbを次のようにします。\n",
    "\n",
    "\n",
    "ここで配列xの軸はバッチサイズ、系列数、特徴量数の順番です。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 活性化関数クラス"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    \"\"\"tanh関数（活性化関数）\"\"\"\n",
    "    def forward(self, A):\n",
    "        Z = np.tanh(A)\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, dZ, A):\n",
    "        dA = dZ * (1 - np.tanh(A)**2)\n",
    "        return dA\n",
    "\n",
    "class Softmax:\n",
    "    \"\"\"softmax関数\n",
    "    backwardは交差エントロピー誤差関数と合わせて計算する\n",
    "    \"\"\"\n",
    "    def forward(self, A):\n",
    "        Z = np.exp(A) / np.sum(np.exp(A), axis=1, keepdims=True)\n",
    "        # backward用にZを保持\n",
    "        self.Z = Z\n",
    "        return Z\n",
    "    \n",
    "    def backward(self, y):\n",
    "        dA = self.Z - y\n",
    "        return dA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeInitializer:\n",
    "    \"\"\"Heの初期値\"\"\"\n",
    "    def W(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"重みの初期化\n",
    "        :parameters\n",
    "            n_nodes1 (int): 前の層のノード数\n",
    "            n_nodes2 (int): 後の層のノード数\n",
    "        :returns\n",
    "            W (2d-array, (n_nodes1, n_nodes2)): ランダムに発生させた重み\n",
    "        \"\"\"\n",
    "        return np.sqrt(2.0 / n_nodes1) * np.random.randn(n_nodes1, n_nodes2)\n",
    "    \n",
    "    def B(self, n_nodes1, n_nodes2):\n",
    "        \"\"\"バイアスの初期化\n",
    "        :parameters\n",
    "            n_nodes2 (int): 後の層のノード数\n",
    "        :returns\n",
    "            B (1d-array, (n_nodes2, )): ランダムに発生させたバイアス\n",
    "        \"\"\"\n",
    "        return np.sqrt(2.0 / n_nodes1) * np.random.randn(1, n_nodes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    \"\"\"Adative Gradient法\n",
    "    :parameters\n",
    "        lr (float): 学習率\n",
    "    \"\"\"\n",
    "    def __init__(self, lr=0.01):\n",
    "        self.lr = lr\n",
    "        self.H_Wx = 0\n",
    "        self.H_Wh = 0\n",
    "        self.H_B = 0\n",
    "    \n",
    "    def update(self, layer):\n",
    "        \"\"\"ある層の重みやバイアスを更新する\n",
    "        :parameters\n",
    "            layer (instance): 更新前の層（FC）のインスタンス\n",
    "        \"\"\"\n",
    "        # 行列Hを求める\n",
    "        self.H_Wx += layer.dWx * layer.dWx\n",
    "        self.H_Wh += layer.dWh * layer.dWh\n",
    "        self.H_B += layer.dB * layer.dB\n",
    "        # 重み・バイアスの更新\n",
    "        layer.Wx -= self.lr * layer.dWx / (np.sqrt(self.H_Wx) + 1e-8) / layer.N\n",
    "        layer.Wh -= self.lr * layer.dWh / (np.sqrt(self.H_Wh) + 1e-8) / layer.N\n",
    "        layer.B -= self.lr * layer.dB / (np.sqrt(self.H_B) + 1e-8) / layer.N\n",
    "        return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array([[[1, 2], [2, 3], [3, 4]]])/100 # (batch_size, n_sequences, n_features)\n",
    "w_x = np.array([[1, 3, 5, 7], [3, 5, 7, 8]])/100 # (n_features, n_nodes)\n",
    "w_h = np.array([[1, 3, 5, 7], [2, 4, 6, 8], [3, 5, 7, 8], [4, 6, 8, 10]])/100 # (n_nodes, n_nodes)\n",
    "batch_size = x.shape[0] # 1\n",
    "n_sequences = x.shape[1] # 3\n",
    "n_features = x.shape[2] # 2\n",
    "n_nodes = w_x.shape[1] # 4\n",
    "h0 = np.zeros((batch_size, n_nodes)) # (batch_size, n_nodes)\n",
    "b = np.array([1, 1, 1, 1]) # (n_nodes,)\n",
    "h_true = np.array([[0.79494228, 0.81839002, 0.83939649, 0.85584174]]) # (batch_size, n_nodes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRNN:\n",
    "    \"\"\"ノード数n_nodes1からn_nodes2への全結合層\n",
    "    :parameters\n",
    "        n_node1 (int): 前層のノード数\n",
    "        n_node2 (int): 後層のノード数\n",
    "        initializer (class instance): 初期化方法のインスタンス\n",
    "        optimizer (class instance): 最適化手法のインスタンス\n",
    "    \"\"\"\n",
    "    def __init__(self, n_features, n_nodes, initializer=None, optimizer=AdaGrad(), activation=Tanh()):\n",
    "        self.optimizer = optimizer\n",
    "        self.activation = activation\n",
    "        self.n_features = n_features\n",
    "        self.n_nodes = n_nodes\n",
    "        # 重みとバイアスの初期化\n",
    "        if initializer is None:\n",
    "            self.Wx = w_x  # shape(n_features, n_nodes)\n",
    "            self.Wh = w_h  # shape(n_nodes, n_nodes)\n",
    "            self.B = b.astype(np.float)\n",
    "        else:\n",
    "            self.Wx = initializer.W(n_features, n_nodes)\n",
    "            self.Wh = initializer.W(n_nodes, n_nodes)\n",
    "            self.B = initializer.B(n_features, n_nodes)\n",
    "        \n",
    "    def forward(self, X, pre_H):\n",
    "        \"\"\"順伝播\n",
    "        :parameters\n",
    "            X (3d-array, (batch_size, n_sequences, n_features)): 入力\n",
    "            pre_H (2d-array, (batch_size, n_nodes)): 前時点のH （活性化前の線形結合）\n",
    "        :returns\n",
    "            H (2d-array, (batch_size, n_features)): 出力\n",
    "        \"\"\"\n",
    "        \n",
    "        # backward用に保持する変数（最初に１度だけ）\n",
    "        if not hasattr(self, \"X\"):\n",
    "            self.X = X\n",
    "            self.N = X.shape[0]  # Backward内のoptimizer.updateで、平均を計算するとき使用する\n",
    "            self.n_sequences = X.shape[1]\n",
    "            self.A = np.zeros((self.n_sequences, self.n_nodes))\n",
    "            self.H = np.zeros((self.n_sequences, self.n_nodes))\n",
    "        \n",
    "        # 系列方向に再帰的にforwardしていく\n",
    "        print(\"X.shape :\", X.shape)\n",
    "        if X.shape[1] > 0:\n",
    "            # 線形結合\n",
    "            A = np.matmul(X[:, 0, :], self.Wx) + np.matmul(pre_H, self.Wh) + self.B\n",
    "            cur_H = self.activation.forward(A)\n",
    "            \n",
    "            # backward用に保存\n",
    "            self.A[self.n_sequences - X.shape[1], :] = A\n",
    "            self.H[self.n_sequences - X.shape[1], :] = cur_H\n",
    "            \n",
    "            # 再帰関数（Xからは今回の系列を除く）\n",
    "            H = self.forward(X[:, 1:, :], cur_H)\n",
    "        else:\n",
    "            H = pre_H\n",
    "        return H\n",
    "    \n",
    "    def backward(self, dH, s=1):\n",
    "        \"\"\"逆伝播\n",
    "        :parameters\n",
    "            dH (2d-array, (batch_size, n_features)): 後層から戻ってきた勾配\n",
    "            s (int): 後ろから何層目かを表すインデックス\n",
    "        :attributes\n",
    "            cur_dH (2d-array, (batch_size, n_nodes1)): 勾配\n",
    "        :returns\n",
    "            dX (3d-array, (batch_size, n_sequences, n_features)) : 入力の勾配\n",
    "        \"\"\"\n",
    "        # 勾配を足し合わせていくための格納用変数を用意\n",
    "        if not hasattr(self, \"dB\"):\n",
    "            self.dB = np.zeros(self.B.shape)\n",
    "            self.dWx = np.zeros(self.Wx.shape)\n",
    "            self.dWh = np.zeros(self.Wh.shape)\n",
    "            self.dX = np.zeros(self.X.shape)\n",
    "            \n",
    "        print(\"s\", s, self.X.shape[1])\n",
    "        cur_dH = np.zeros_like(dH[0, :])\n",
    "        while s <= self.X.shape[1]:\n",
    "            # tanhのbackward\n",
    "            dA = self.activation.backward(dH[-s, :] + cur_dH, self.A[-s, :])\n",
    "            # 勾配を計算して、これまでの勾配と合計する\n",
    "            self.dB += dA\n",
    "            self.dWx += np.matmul(self.X[:, -s, :].T, dA[np.newaxis, :])\n",
    "            self.dWh += np.matmul(self.H[-s, :][np.newaxis, :].T, dA[np.newaxis, :])\n",
    "            self.dX[:, -s, :] = np.matmul(dA, self.Wx.T)\n",
    "            print(\"dX:\\n\", self.dX)\n",
    "            \n",
    "            # 一つ前に渡すdHを求める\n",
    "            cur_dH = np.matmul(dA, self.Wh.T)\n",
    "            \n",
    "            # 時間をさかのぼる。\n",
    "            s += 1\n",
    "            \n",
    "        # 勾配をもとに重みとバイアスを更新\n",
    "        self = self.optimizer.update(self)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScratchRNNClassifier():\n",
    "    \"\"\"リカレントニューラルネットワーク分類器\n",
    "    :attributes\n",
    "        layers (list of instances): 各層のインスタンスを並べたリスト\n",
    "        n_epoch (int): 繰り返すエポック数\n",
    "        batch_size (int): ミニバッチのデータ数\n",
    "        plot_interval (int): 損失関数を記録する間隔\n",
    "        epoch_interval (int): 何epochごとにprintするか\n",
    "        loss (list): 損失関数の推移（訓練データ）\n",
    "        lossval (list): 損失関数の推移（検証データ）\n",
    "        verbose (bool): 学習過程を出力する場合はTrue\n",
    "    \"\"\"\n",
    "    def __init__(self, layers, n_epoch=10, batch_size=20, plot_interval=1, verbose=True, epoch_interval=1):\n",
    "        self.layers = layers\n",
    "        self.n_epoch = n_epoch\n",
    "        self.batch_size = batch_size\n",
    "        self.plot_interval = plot_interval\n",
    "        self.epoch_interval = epoch_interval\n",
    "        self.verbose = verbose\n",
    "        self.loss = []\n",
    "        self.loss_val = []\n",
    "\n",
    "    def predict_proba(self, X, H0):\n",
    "        \"\"\"ニューラルネットワーク分類器を使い推定する。推定結果は確率のOne-hot表現。\n",
    "        :parameters\n",
    "            X (3d-array, (n_samples, n_sequences, n_features)): 入力\n",
    "            pre_H (2d-array, (n_samples, n_nodes)): 前時点のH （活性化前の線形結合）\n",
    "        :returns\n",
    "            y (2d-array, (n_samples, n_classes)): 推定確率\n",
    "        \"\"\"\n",
    "        # forwardの先が recuurentなので複雑にしない。\n",
    "        y_pred = self.layers.forward(X, H0)\n",
    "        return y_pred\n",
    "    \n",
    "    def predict(self, X, H0):\n",
    "        \"\"\"ニューラルネットワーク分類器を使い推定する。\n",
    "        :parameters\n",
    "            X (2d-array, (n_samples, n_features)): サンプル\n",
    "        :returns\n",
    "            label (1d-array, (n_samples,)): 推定ラベル\n",
    "        \"\"\"\n",
    "        probability = self.predict_proba(X, H0)\n",
    "        label = np.argmax(probability, axis=1)\n",
    "        return label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape : (1, 3, 2)\n",
      "X.shape : (1, 2, 2)\n",
      "X.shape : (1, 1, 2)\n",
      "X.shape : (1, 0, 2)\n",
      "Pred [[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "True [[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "\n",
      "X.shape : (1, 3, 2)\n",
      "X.shape : (1, 2, 2)\n",
      "X.shape : (1, 1, 2)\n",
      "X.shape : (1, 0, 2)\n",
      "forward output: [[0.79494228 0.81839002 0.83939649 0.85584174]]\n",
      "\n",
      "X.shape : (1, 3, 2)\n",
      "X.shape : (1, 2, 2)\n",
      "X.shape : (1, 1, 2)\n",
      "X.shape : (1, 0, 2)\n",
      "predicted label: [3]\n"
     ]
    }
   ],
   "source": [
    "layer = SimpleRNN(n_features, n_nodes)\n",
    "print(\"Pred\", layer.forward(x, h0))\n",
    "print(\"True\", h_true)\n",
    "print()\n",
    "rnn_classifier = ScratchRNNClassifier(layer)\n",
    "print(\"forward output:\", rnn_classifier.predict_proba(x, h0))\n",
    "print()\n",
    "\n",
    "print(\"predicted label:\", rnn_classifier.predict(x, h0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 【問題３】バックプロパゲーションの実装\n",
    "バックプロパゲーションを実装してください。\n",
    "\n",
    "\n",
    "RNNの内部は全結合層を組み合わせた形になっているので、更新式は全結合層などと同様です。\n",
    "\n",
    "$$\n",
    "W_x^{\\prime} = W_x - \\alpha \\frac{\\partial L}{\\partial W_x} \\\\\n",
    "W_h^{\\prime} = W_h - \\alpha \\frac{\\partial L}{\\partial W_h} \\\\\n",
    "B^{\\prime} = B - \\alpha \\frac{\\partial L}{\\partial B}\n",
    "$$\n",
    "$\\alpha$ : 学習率\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_x}$ : $W_x$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial W_h}$ : $W_h$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "$\\frac{\\partial L}{\\partial B}$ : $B$ に関する損失 $L$ の勾配\n",
    "\n",
    "\n",
    "勾配を求めるためのバックプロパゲーションの数式が以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial h_t}{\\partial a_t} = \\frac{\\partial L}{\\partial h_t} ×(1-tanh^2(a_t))\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial B} = \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_x} = x_{t}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial W_h} = h_{t-1}^{T}\\cdot \\frac{\\partial h_t}{\\partial a_t}\n",
    "$$\n",
    "\n",
    "*$\\frac{\\partial L}{\\partial h_t}$ は前の時刻からの状態の誤差と出力の誤差の合計です。hは順伝播時に出力と次の層に伝わる状態双方に使われているからです。\n",
    "\n",
    "\n",
    "前の時刻や層に流す誤差の数式は以下です。\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial h_{t-1}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{h}^{T}\n",
    "$$\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_{t}} = \\frac{\\partial h_t}{\\partial a_t}\\cdot W_{x}^{T}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dH [[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n",
      "s 1 3\n",
      "dX:\n",
      " [[[0.         0.        ]\n",
      "  [0.         0.        ]\n",
      "  [0.04708592 0.06963563]]]\n",
      "dX:\n",
      " [[[0.         0.        ]\n",
      "  [0.05199753 0.07646683]\n",
      "  [0.04708592 0.06963563]]]\n",
      "dX:\n",
      " [[[0.07237975 0.10373134]\n",
      "  [0.05199753 0.07646683]\n",
      "  [0.04708592 0.06963563]]]\n",
      "===== Result =====\n",
      "dX:\n",
      " [[[0.07237975 0.10373134]\n",
      "  [0.05199753 0.07646683]\n",
      "  [0.04708592 0.06963563]]]\n",
      "\n",
      "dWx:\n",
      " [[0.02325421 0.02151871 0.01988135 0.01863078]\n",
      " [0.03524769 0.03286036 0.03059886 0.0288961 ]]\n",
      "\n",
      "dWh:\n",
      " [[0.9377582  0.88585433 0.83619035 0.80008613]\n",
      " [0.95506827 0.90155997 0.85038445 0.81309911]\n",
      " [0.97065743 0.91570602 0.86317077 0.82482314]\n",
      " [0.98291729 0.9268302  0.87322478 0.83404108]]\n",
      "\n",
      "dB:\n",
      " [1.19934775 1.13416539 1.07175132 1.02653129]\n"
     ]
    }
   ],
   "source": [
    "dh = np.ones((3, 4))\n",
    "print(\"dH\", dh)\n",
    "layer.backward(dh)\n",
    "print(\"===== Result =====\")\n",
    "print(\"dX:\\n\", layer.dX)\n",
    "print(\"\\ndWx:\\n\", layer.dWx)\n",
    "print(\"\\ndWh:\\n\", layer.dWh)\n",
    "print(\"\\ndB:\\n\", layer.dB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py37",
   "language": "python",
   "name": "py37"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
